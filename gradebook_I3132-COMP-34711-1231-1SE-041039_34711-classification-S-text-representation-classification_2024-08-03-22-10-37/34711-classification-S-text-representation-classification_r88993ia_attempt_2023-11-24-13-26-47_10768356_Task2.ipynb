{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "F-MKAhvZ2k71"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import unicodedata\n",
        "import re\n",
        "import string\n",
        "import time\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords, words"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('words')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z80QkbDZOEsO",
        "outputId": "6aa5c337-b4ae-4aac-8fa1-66c4c36f293f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_dataset_path = r\"./data/Training-dataset.csv\"\n",
        "validation_dataset_path = r\"./data/Task-2-validation-dataset.csv\"\n",
        "test_dataset_path = r\"./data/Task-2-test-dataset1.csv\""
      ],
      "metadata": {
        "id": "uBbWVOPFWsvy"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text pre-processing for both models:"
      ],
      "metadata": {
        "id": "ywKvoF49W6Yc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialise the lemmatizer and the words dictionary\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "english_vocab = set(words.words()) - stop_words\n",
        "\n",
        "def remove_accents(data):\n",
        "  \"\"\" Removes accents from a word. \"\"\"\n",
        "  return ''.join(x for x in unicodedata.normalize('NFKD', data) if x in string.ascii_letters or x == \" \")\n",
        "\n",
        "def process_text(text):\n",
        "  \"\"\" A function for tokenization and lemmatization. \"\"\"\n",
        "  # Remove punctuation\n",
        "  # replace dashes with ' ' and replace everything else with a ''.\n",
        "  pattern = re.compile('[%s]' % re.escape(string.punctuation.replace('-', '')))\n",
        "  text = re.sub(pattern, '', text.replace('-', ' '))\n",
        "\n",
        "  # Tokenize\n",
        "  tokens = word_tokenize(text)\n",
        "\n",
        "  # Remove extra whitespace, turn to lowercase, remove accents, lemmatize\n",
        "  tokens = [lemmatizer.lemmatize(remove_accents(token.strip().lower())) for token in tokens]\n",
        "\n",
        "  # Filter OOV words + stopwords\n",
        "  tokens = [token for token in tokens if token in english_vocab]\n",
        "\n",
        "  return tokens"
      ],
      "metadata": {
        "id": "-c-s9GLvOGjZ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "training_df = pd.read_csv(training_dataset_path)\n",
        "\n",
        "# Pre-process the data\n",
        "training_df['text'] = training_df['title'] + ' ' + training_df['plot_synopsis']\n",
        "training_df['text'] = training_df['text'].apply(process_text)\n",
        "training_df['text'] = training_df['text'].apply(' '.join)"
      ],
      "metadata": {
        "id": "deInBxr6cUeM"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model 1: a) A traditional classification method - NaÃ¯ve Bayes classifier**"
      ],
      "metadata": {
        "id": "NIKyFkozb4Nr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_bayes_model(input_path, output_path):\n",
        "  \"\"\" Evaluates the model on a validation/test set.\n",
        "\n",
        "      It does the following:\n",
        "        1- Reads the input file\n",
        "        2- Preprocesses the data.\n",
        "        3- Classify the data.\n",
        "        4- Store the results in the specified path.\n",
        "  \"\"\"\n",
        "\n",
        "  # Load and preprocess the evaluation/test data\n",
        "  validation_df = pd.read_csv(input_path)\n",
        "\n",
        "  validation_df['text'] = validation_df['title'] + ' ' + validation_df['plot_synopsis']\n",
        "  validation_df['text'] = validation_df['text'].apply(process_text)\n",
        "  validation_df['text'] = validation_df['text'].apply(' '.join)\n",
        "\n",
        "  # Use the trained Bayes classifier to classify the new data\n",
        "  x = vectorizer.transform(validation_df['text'])\n",
        "  y_pred = classifier.predict(x)\n",
        "\n",
        "  # Store the output in a csv file\n",
        "  output_df = pd.DataFrame(y_pred, columns=['comedy', 'cult', 'flashback', 'historical', 'murder', 'revenge', 'romantic', 'scifi', 'violence'])\n",
        "  output_df = pd.concat([validation_df['ID'], output_df], axis=1)\n",
        "  output_df.to_csv(output_path, index=False, header=False)"
      ],
      "metadata": {
        "id": "6JCYWiQ9d2ME"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = training_df[['comedy', 'cult', 'flashback', 'historical', 'murder', 'revenge', 'romantic', 'scifi', 'violence']]\n",
        "\n",
        "# ==================== Record the start time ====================\n",
        "start_time = time.time()\n",
        "\n",
        "# Initialise the term-document-matrix using term count (any term appearing in less than 10 documents will be excluded)\n",
        "vectorizer = CountVectorizer(min_df=10)\n",
        "term_doc_matrix = vectorizer.fit_transform(training_df['text'])\n",
        "\n",
        "# Create and fit the naive Bayes classifier\n",
        "# MultinomialNB() is wrapped by MultiOutputClassifier() to allow multi-label classification\n",
        "classifier = MultiOutputClassifier(MultinomialNB())\n",
        "classifier.fit(term_doc_matrix, labels)\n",
        "\n",
        "# ==================== Record the end time ====================\n",
        "end_time = time.time()\n",
        "\n",
        "# Calculate and print the elapsed time\n",
        "elapsed_time = end_time - start_time\n",
        "print(f\"Time taken: {elapsed_time} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9n0-3GNWcRPJ",
        "outputId": "f0043d5b-0d9e-48d8-f19f-9182113da272"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time taken: 2.2311453819274902 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Validation dataset:"
      ],
      "metadata": {
        "id": "BkhqooC-aZYR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Record the start time\n",
        "start_time = time.time()\n",
        "\n",
        "# Run the model on the validation dataset\n",
        "evaluate_bayes_model(validation_dataset_path, '10768356-Task2-method-a-validation.csv')\n",
        "\n",
        "# Record the end time\n",
        "end_time = time.time()\n",
        "\n",
        "# Calculate and print the elapsed time\n",
        "elapsed_time = end_time - start_time\n",
        "print(f\"Time taken: {elapsed_time} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GeZV3OBdehP1",
        "outputId": "05745eda-f233-457d-d37d-7082ea75f80b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time taken: 9.850985765457153 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test dataset:"
      ],
      "metadata": {
        "id": "p6Yo-Vs1adn_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Record the start time\n",
        "start_time = time.time()\n",
        "\n",
        "# Run the model on the test dataset\n",
        "evaluate_bayes_model(test_dataset_path, '10768356-Task2-method-a.csv')\n",
        "\n",
        "# Record the end time\n",
        "end_time = time.time()\n",
        "\n",
        "# Calculate and print the elapsed time\n",
        "elapsed_time = end_time - start_time\n",
        "print(f\"Time taken: {elapsed_time} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bGgqYZBZacLD",
        "outputId": "7f7f5613-9640-4f9b-91b8-1e900494fcd9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time taken: 10.004181146621704 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model 2: b) A traditional deep learning method with Bi-LSTM**"
      ],
      "metadata": {
        "id": "ZZqaihOxXHlU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_LSTM_model(input_path, output_path):\n",
        "  \"\"\" Evaluates the model on a validation/test set.\n",
        "\n",
        "      It does the following:\n",
        "        1- Reads the input file\n",
        "        2- Preprocesses the data.\n",
        "        3- Create sequences for the input tokens.\n",
        "        4- Classify the data.\n",
        "        5- Store the results in the specified path.\n",
        "  \"\"\"\n",
        "\n",
        "  validation_df = pd.read_csv(input_path)\n",
        "\n",
        "  validation_df['text'] = validation_df['title'] + ' ' + validation_df['plot_synopsis']\n",
        "  validation_df['text'] = validation_df['text'].apply(process_text)\n",
        "  validation_df['text'] = validation_df['text'].apply(' '.join)\n",
        "\n",
        "  sequences_val = tokenizer.texts_to_sequences(validation_df['text'])\n",
        "  x = pad_sequences(sequences_val)\n",
        "\n",
        "  y_pred = model.predict(x)\n",
        "  y_pred = [[1 if j>=0.5 else 0 for j in i] for i in y_pred]\n",
        "\n",
        "  output_df = pd.DataFrame(y_pred, columns=['comedy', 'cult', 'flashback', 'historical', 'murder', 'revenge', 'romantic', 'scifi', 'violence'])\n",
        "  output_df = pd.concat([validation_df['ID'], output_df], axis=1)\n",
        "  output_df.to_csv(output_path, index=False, header=False)"
      ],
      "metadata": {
        "id": "OH1Q2xLKdrNv"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = training_df[['comedy', 'cult', 'flashback', 'historical', 'murder', 'revenge', 'romantic', 'scifi', 'violence']].values\n",
        "\n",
        "# ==================== Record the start time ====================\n",
        "start_time = time.time()\n",
        "\n",
        "# Tokenize and pad sequences\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(training_df['text'])\n",
        "sequences_train = tokenizer.texts_to_sequences(training_df['text'])\n",
        "x_train = pad_sequences(sequences_train)\n",
        "\n",
        "\n",
        "# Define Adam optimiser with 0.001 learning rate\n",
        "optimizer = Adam(lr=0.001)\n",
        "\n",
        "# Define a callback to stop the training if the loss keeps dropping for 2 consecutive epochs\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=2)\n",
        "\n",
        "# Build the neural network:\n",
        "# Set the embedding dimension for the word embeddings\n",
        "embedding_dim = 150\n",
        "\n",
        "# Create a Sequential model\n",
        "model = Sequential()\n",
        "\n",
        "# Add an Embedding layer to the model\n",
        "# - len(tokenizer.word_index): Number of unique words in the vocabulary (input dimension)\n",
        "# - embedding_dim: Dimensionality of the dense embedding\n",
        "# - trainable=True: Allow the embedding weights to be updated during training\n",
        "model.add(Embedding(len(tokenizer.word_index), embedding_dim, trainable=True))\n",
        "\n",
        "# Add a Bidirectional LSTM layer to the model\n",
        "# - 200: Number of LSTM units in each direction (400 units in total)\n",
        "model.add(Bidirectional(LSTM(200)))\n",
        "\n",
        "# Add a Dense layer to the model for the output\n",
        "# - 9: Number of output units, corresponding to the 9 classes (activation='sigmoid' for multi-label classification)\n",
        "model.add(Dense(9, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(x_train,                                  # Training input data\n",
        "                    y_train,                                  # Training target data (labels)\n",
        "                    epochs=10,                                # Number of training epochs\n",
        "                    batch_size=32,                            # Mini-batch size\n",
        "                    validation_split=0.1,                     # Fraction of training data used for validation\n",
        "                    callbacks=[early_stopping])               # Callback for early stopping\n",
        "\n",
        "\n",
        "# ==================== Record the end time ====================\n",
        "end_time = time.time()\n",
        "\n",
        "# Calculate and print the elapsed time\n",
        "elapsed_time = end_time - start_time\n",
        "print(f\"Time taken: {elapsed_time} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SlRB2nNJOOGS",
        "outputId": "39103869-500e-4fb3-a677-028e32a6a298"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "233/233 [==============================] - 87s 336ms/step - loss: 0.4613 - accuracy: 0.2639 - val_loss: 0.4280 - val_accuracy: 0.2748\n",
            "Epoch 2/10\n",
            "233/233 [==============================] - 73s 315ms/step - loss: 0.4171 - accuracy: 0.3160 - val_loss: 0.4231 - val_accuracy: 0.2978\n",
            "Epoch 3/10\n",
            "233/233 [==============================] - 66s 284ms/step - loss: 0.3578 - accuracy: 0.4056 - val_loss: 0.4409 - val_accuracy: 0.2954\n",
            "Epoch 4/10\n",
            "233/233 [==============================] - 62s 267ms/step - loss: 0.2887 - accuracy: 0.5055 - val_loss: 0.4896 - val_accuracy: 0.2893\n",
            "Time taken: 295.84762740135193 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Validation dataset:"
      ],
      "metadata": {
        "id": "-MbDCLnKavd-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Record the start time\n",
        "start_time = time.time()\n",
        "\n",
        "# Evaluate the model\n",
        "evaluate_LSTM_model(validation_dataset_path, '10768356-Task2-method-b-validation.csv')\n",
        "\n",
        "# Record the end time\n",
        "end_time = time.time()\n",
        "\n",
        "# Calculate and print the elapsed time\n",
        "elapsed_time = end_time - start_time\n",
        "print(f\"Time taken: {elapsed_time} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TsgiO_SJxZ9G",
        "outputId": "2a1ad502-00b4-4dcd-ae69-0b2d653f6ac6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "38/38 [==============================] - 4s 69ms/step\n",
            "Time taken: 15.363977909088135 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test dataset:"
      ],
      "metadata": {
        "id": "yyusxM1xaymf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Record the start time\n",
        "start_time = time.time()\n",
        "\n",
        "# Run the model on the test dataset\n",
        "evaluate_LSTM_model(test_dataset_path, '10768356-Task2-method-b.csv')\n",
        "\n",
        "# Record the end time\n",
        "end_time = time.time()\n",
        "\n",
        "# Calculate and print the elapsed time\n",
        "elapsed_time = end_time - start_time\n",
        "print(f\"Time taken: {elapsed_time} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MjK8hea3aqSf",
        "outputId": "36b7fa33-a748-4400-97a0-15b6f3a9762b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "38/38 [==============================] - 3s 72ms/step\n",
            "Time taken: 15.44764232635498 seconds\n"
          ]
        }
      ]
    }
  ]
}