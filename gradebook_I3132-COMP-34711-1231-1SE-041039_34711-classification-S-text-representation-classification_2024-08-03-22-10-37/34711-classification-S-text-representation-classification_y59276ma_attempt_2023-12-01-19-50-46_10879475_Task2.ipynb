{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "I6OQY98Ow2z2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b397e0cd-5cbd-43e9-8a5e-ddbebae1b2f9"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "DmUmqwhWtHcQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5bbeeaf-0c69-4764-d60a-3880c70d5fd4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ],
      "source": [
        "import nltk,re,time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import LinearSVC,SVC\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "import csv\n",
        "import tensorflow as tf\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Bidirectional, LSTM,Dense, Dropout\n",
        "\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"punkt\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Only use is in seeing the accuracy f1meausre,recall,precision etc during the epochs of bilstm\n",
        "from keras import backend as K\n",
        "\n",
        "def recall_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
      ],
      "metadata": {
        "id": "XJIAy_maW2Up"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#CONSTANTS\n",
        "OUTPUT_TARGETS = ['comedy', 'cult', 'flashback','historical', 'murder', 'revenge', 'romantic', 'scifi', 'violence']\n",
        "VOCAB_SIZE = 5000\n",
        "EPOCHS = 7\n",
        "BATCH_SIZE = 32\n",
        "EMBEDDING_DIM = 200\n",
        "\n",
        "# Input Data\n",
        "TRAINING_PATH = './data/Training-dataset.csv'\n",
        "VALID_DATA_PATH = './data/Task-2-validation-dataset.csv'\n",
        "TEST_DATA_PATH = './data/Task-2-test-dataset2.csv'\n",
        "\n",
        "#Output data\n",
        "TASK_A_VALID_OUTPUT_PATH = './data/10879475-Task2-method-a-validation.csv'\n",
        "TASK_A_TEST_OUTPUT_PATH = './data/10879475-Task2-method-a.csv'\n",
        "\n",
        "TASK_B_VALID_OUTPUT_PATH = './data/10879475-Task2-method-b-validation.csv'\n",
        "TASK_B_TEST_OUTPUT_PATH = './data/10879475-Task2-method-b.csv'"
      ],
      "metadata": {
        "id": "yaaybl4eYKAg"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create SVM class\n",
        "class SVM:\n",
        "  def __init_(self):\n",
        "    '''SVM class intiaiser. Creates 2 elements: training corpus and validation corpus that are empty to start'''\n",
        "    self.training_corpus = []\n",
        "    self.validation_corpus = []\n",
        "    self.testing_corpus = []\n",
        "\n",
        "  def read_and_format_input_file(self, path,test=False):\n",
        "    '''Reads input csv file at the {path}, modifies the read file slightly and combines the text in plot_synopsis and title column of the read file together into a new Column called Combined_TitlePlot\n",
        "    Gets rid of plot_synopsis and title column after combining the 2 columsn together.\n",
        "    Returns the read csv file at the path after the slight processing and formatting'''\n",
        "    #Read input file and combine text in plot synosis and title column\n",
        "    corpus = pd.read_csv(path)\n",
        "    corpus['Combined_TitlePlot'] = corpus['title'] + \". \" + corpus['plot_synopsis']\n",
        "    corpus.drop(labels=['title','plot_synopsis'],axis=1,inplace=True)#Drop title and plot_synopsis column\n",
        "    #Rearrange new columns\n",
        "    if not test:\n",
        "      corpus = corpus[['ID', 'Combined_TitlePlot', 'comedy', 'cult', 'flashback',\n",
        "            'historical', 'murder', 'revenge', 'romantic', 'scifi', 'violence']]\n",
        "    else:\n",
        "      corpus = corpus[['ID', 'Combined_TitlePlot']]\n",
        "    return corpus\n",
        "\n",
        "  def clean_document(self,document):\n",
        "    '''Cleans the text in the input document.\n",
        "    https://towardsdatascience.com/multi-label-text-classification-with-scikit-learn-30714b7819c5'''\n",
        "    #Normalise document to lower case\n",
        "    document = document.lower()\n",
        "    #Deal with common apostrophe phrases\n",
        "    document = re.sub(r\"what's\", \"what is \", document)\n",
        "    document = re.sub(r\"\\'s\", \" \", document)\n",
        "    document = re.sub(r\"\\'ve\", \" have \", document)\n",
        "    document = re.sub(r\"can't\", \"can not \", document)\n",
        "    document = re.sub(r\"n't\", \" not \", document)\n",
        "    document = re.sub(r\"i'm\", \"i am \", document)\n",
        "    document = re.sub(r\"\\'re\", \" are \", document)\n",
        "    document = re.sub(r\"\\'d\", \" would \", document)\n",
        "    document = re.sub(r\"\\'ll\", \" will \", document)\n",
        "    document = re.sub(r\"\\'scuse\", \" excuse \", document)\n",
        "    #Replace text with space for any regular expression pattern that matches any non-alphanumeric character (equivalent to [^a-zA-Z0-9_])\n",
        "    document = re.sub('\\W', ' ', document)\n",
        "    #Replace sequences of a more than single space with a single space\n",
        "    document = re.sub('\\s+', ' ', document)\n",
        "    #Remove leading or trailing spaces from the document text\n",
        "    document = document.strip(' ')\n",
        "    return document\n",
        "\n",
        "  def classifer_train_and_predict(self,targets,X_train,X_valid,X_test):\n",
        "    '''https://towardsdatascience.com/multi-label-text-classification-with-scikit-learn-30714b7819c5\n",
        "    Uses tfidf to transform the textual documents data into numerical feature vectors\n",
        "    Uses OnevsRestClassifier using LinearSVC in order to train a seperate classifier for each class'''\n",
        "    #Set up SVM pipeline using tfidf and onevsrest classifier\n",
        "    SVM_pipeline = Pipeline([\n",
        "                ('tfidf', TfidfVectorizer()),\n",
        "                ('clf', OneVsRestClassifier(LinearSVC(), n_jobs=1)),\n",
        "            ])\n",
        "\n",
        "    #For each target label, train the classifer and predict on the test data for each document. The dictionary results will contain a key value pair such that the key is a target label and the value is an array of values representing whether a label applies to a document or not(a value of 0 at index 2 of the array means that the 3 document isn't given that label)\n",
        "    validation_results = {}\n",
        "    testing_results = {}\n",
        "    for target in targets:\n",
        "      # Train for each target\n",
        "      SVM_pipeline.fit(X_train,self.training_corpus[target])\n",
        "\n",
        "      #Make predictions for current label for all the documents in the validation dataset\n",
        "      validation_prediction = SVM_pipeline.predict(X_valid)\n",
        "      validation_results[target] = validation_prediction\n",
        "\n",
        "      #Make predictions for the current label for all documents in the testing dataset\n",
        "      testing_prediction = SVM_pipeline.predict(X_test)\n",
        "      testing_results[target] = testing_prediction\n",
        "\n",
        "    return validation_results,testing_results\n",
        "\n",
        "  def output_results_to_file(self,results,output_path,ids):\n",
        "    '''Output the results to a csv file such that the format is:\n",
        "    column1 = documentID, column:2-10= 1 or 0 representing whether a label applies to the document or not'''\n",
        "    output_df = pd.DataFrame({\n",
        "      'document_id': ids,\n",
        "      **results  # Unpack the results dictionary into columns\n",
        "    })\n",
        "    output_df.to_csv(output_path, index=False, header=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "dlEgikp6g5bn"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BiLSTM:\n",
        "  def __init__(self):\n",
        "    '''Initialise the object with 3 empty fields for the training,validation and testing corpus'''\n",
        "    self.training_corpus = []\n",
        "    self.validation_corpus = []\n",
        "    self.testing_corpus = []\n",
        "\n",
        "  def read_and_format_input_file(self, path,test=False):\n",
        "    '''Reads input csv file at the {path}, modifies the read file slightly and combines the text in plot_synopsis and title column of the read file together into a new Column called Combined_TitlePlot\n",
        "    Gets rid of plot_synopsis and title column after combining the 2 columsn together.\n",
        "    Returns the read csv file at the path after the slight processing and formatting'''\n",
        "    #Read input file and combine text in plot synosis and title column\n",
        "    corpus = pd.read_csv(path)\n",
        "    corpus['Combined_TitlePlot'] = corpus['title'] + \". \" + corpus['plot_synopsis']\n",
        "    corpus.drop(labels=['title','plot_synopsis'],axis=1,inplace=True)#Drop title and plot_synopsis column\n",
        "    #Rearrange new columns\n",
        "    if not test:\n",
        "      corpus = corpus[['ID', 'Combined_TitlePlot', 'comedy', 'cult', 'flashback',\n",
        "            'historical', 'murder', 'revenge', 'romantic', 'scifi', 'violence']]\n",
        "    else:\n",
        "      corpus = corpus[['ID', 'Combined_TitlePlot']]\n",
        "    return corpus\n",
        "\n",
        "  def clean_document(self,document):\n",
        "    '''Cleans the text in the input document.\n",
        "    https://towardsdatascience.com/multi-label-text-classification-with-scikit-learn-30714b7819c5'''\n",
        "    #Normalise document to lower case\n",
        "    document = document.lower()\n",
        "    #Deal with common apostrophe phrases\n",
        "    document = re.sub(r\"what's\", \"what is \", document)\n",
        "    document = re.sub(r\"\\'s\", \" \", document)\n",
        "    document = re.sub(r\"\\'ve\", \" have \", document)\n",
        "    document = re.sub(r\"can't\", \"can not \", document)\n",
        "    document = re.sub(r\"n't\", \" not \", document)\n",
        "    document = re.sub(r\"i'm\", \"i am \", document)\n",
        "    document = re.sub(r\"\\'re\", \" are \", document)\n",
        "    document = re.sub(r\"\\'d\", \" would \", document)\n",
        "    document = re.sub(r\"\\'ll\", \" will \", document)\n",
        "    document = re.sub(r\"\\'scuse\", \" excuse \", document)\n",
        "    #Replace text with space for any regular expression pattern that matches any non-alphanumeric character (equivalent to [^a-zA-Z0-9_])\n",
        "    document = re.sub('\\W', ' ', document)\n",
        "    #Replace sequences of a more than single space with a single space\n",
        "    document = re.sub('\\s+', ' ', document)\n",
        "    #Remove leading or trailing spaces from the document text\n",
        "    document = document.strip(' ')\n",
        "    return document\n",
        "\n",
        "  def tokenise_and_padd(self,VOCAB_SIZE):\n",
        "    '''Tokenises the text into words then tokenises the words into sequence of tokens for all of the different types of corpus(training,testing and validation)\n",
        "    It then padds the sequence tokens such that they are all of the same length\n",
        "    returns the MAX sequence lengtht used, the labels for each of the different corpora along with padded tokens as well as the non padded sequences tokens for each\n",
        "    of the different corpora'''\n",
        "    #Tokenize the text data\n",
        "    tokenizer = Tokenizer(num_words=VOCAB_SIZE,oov_token=\"<OOV>\")\n",
        "    tokenizer.fit_on_texts(self.training_corpus['Combined_TitlePlot'].values)\n",
        "\n",
        "    #Convert text tokens into sequences\n",
        "    X_train = tokenizer.texts_to_sequences(self.training_corpus['Combined_TitlePlot'].values)\n",
        "    X_valid = tokenizer.texts_to_sequences(self.validation_corpus['Combined_TitlePlot'].values)\n",
        "    X_test = tokenizer.texts_to_sequences(self.testing_corpus['Combined_TitlePlot'].values)\n",
        "\n",
        "    # MAX_SEQ_LEN = max(len(seq) for seq in X_train)\n",
        "    MAX_SEQ_LEN = 500\n",
        "\n",
        "    #Pad sequences\n",
        "    X_train_padded = pad_sequences(X_train, maxlen=MAX_SEQ_LEN, padding='post', truncating='post')\n",
        "    train_labels = self.training_corpus[OUTPUT_TARGETS].values\n",
        "\n",
        "    X_valid_padded = pad_sequences(X_valid, maxlen=MAX_SEQ_LEN, padding='post', truncating='post')\n",
        "    validation_labels = self.validation_corpus[OUTPUT_TARGETS].values\n",
        "\n",
        "    X_test_padded = pad_sequences(X_test, maxlen=MAX_SEQ_LEN, padding='post', truncating='post')\n",
        "\n",
        "    return MAX_SEQ_LEN,X_train,X_valid,X_test, X_train_padded, X_valid_padded,X_test_padded,train_labels,validation_labels\n",
        "\n",
        "  def build_and_train_model(self,VOCAB_SIZE,MAX_SEQ_LEN,EPOCHS,BATCH_SIZE,EMBEDDING_DIM,X_train_padded,train_labels):\n",
        "    '''Builds a Bi-LSTM model using 2 hidden layers and sigmoid activation function\n",
        "    After that it trains the model and returns the trained model'''\n",
        "    # Build the Bi-LSTM model\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=VOCAB_SIZE, output_dim=EMBEDDING_DIM, input_length=MAX_SEQ_LEN))\n",
        "    model.add(Bidirectional(LSTM(64,return_sequences=True)))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Bidirectional(LSTM(64)))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(9, activation='sigmoid'))\n",
        "\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc',f1_m,precision_m, recall_m])\n",
        "\n",
        "    model.fit(X_train_padded, train_labels, epochs=EPOCHS, batch_size=BATCH_SIZE)\n",
        "    return model\n",
        "\n",
        "  def predict(self,trained_model ,data_to_predict_for):\n",
        "    '''Makes predictions on the input data to the function. Uses thresholding to predict convert probabilites from the sigmoid activation funciton into binary\n",
        "    labels\n",
        "    returns the binary predictions for all of the classes on the entire dataset'''\n",
        "    # predict on validation data\n",
        "    prediction = trained_model.predict(data_to_predict_for)\n",
        "    thresholds = [0.5, 0.5, 0.5, 0.2, 0.5, 0.5, 0.5, 0.5, 0.5]\n",
        "    # y_pred_binary = (prediction > threshold).astype(int)\n",
        "    y_pred_binary = (prediction > np.array(thresholds)).astype(int)\n",
        "    return y_pred_binary\n",
        "\n",
        "  def write_results_to_file(self,binary_predictions,output_path,document_ids):\n",
        "    '''Outputs predictions to a new csv file'''\n",
        "    # Create a DataFrame with document IDs and predictions\n",
        "    df = pd.DataFrame(document_ids, columns=['Document ID'])\n",
        "    df = pd.concat([df, pd.DataFrame(binary_predictions)], axis=1)\n",
        "\n",
        "    # Write the DataFrame to a CSV file without headers\n",
        "    df.to_csv(output_path, index=False, header=False)"
      ],
      "metadata": {
        "id": "uORKyJnHXgqY"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Svm_experiment():\n",
        "  #Instantiate an instance of the SVM class\n",
        "  svm_classifier = SVM()\n",
        "\n",
        "  #Read in the training corpus and validation corpus and testing corpus\n",
        "  svm_classifier.training_corpus = svm_classifier.read_and_format_input_file(TRAINING_PATH)\n",
        "  svm_classifier.validation_corpus = svm_classifier.read_and_format_input_file(VALID_DATA_PATH)\n",
        "  svm_classifier.testing_corpus = svm_classifier.read_and_format_input_file(TEST_DATA_PATH,test=True)\n",
        "\n",
        "  #Clean the training and validation corpus as well as the testing corpus\n",
        "  svm_classifier.training_corpus['Combined_TitlePlot'] = svm_classifier.training_corpus['Combined_TitlePlot'].map(lambda doc: svm_classifier.clean_document(doc))\n",
        "  svm_classifier.validation_corpus['Combined_TitlePlot'] = svm_classifier.validation_corpus['Combined_TitlePlot'].map(lambda doc: svm_classifier.clean_document(doc))\n",
        "  svm_classifier.testing_corpus['Combined_TitlePlot'] = svm_classifier.testing_corpus['Combined_TitlePlot'].map(lambda doc: svm_classifier.clean_document(doc))\n",
        "\n",
        "  #Take the textual data from the training and valiadtion corpus\n",
        "  X_train = svm_classifier.training_corpus.Combined_TitlePlot\n",
        "  X_valid = svm_classifier.validation_corpus.Combined_TitlePlot\n",
        "  X_test = svm_classifier.testing_corpus.Combined_TitlePlot\n",
        "\n",
        "  #Train the using the training data and get predictions for each class for the documents in the validation data and testing dataset\n",
        "  Validation_Predictions_for_each_label,Testing_Predictions_for_each_label = svm_classifier.classifer_train_and_predict(OUTPUT_TARGETS,X_train,X_valid,X_test)\n",
        "\n",
        "  # Output results to file for validation predicitons\n",
        "  validation_doc_ids = svm_classifier.validation_corpus['ID'].values\n",
        "  svm_classifier.output_results_to_file(Validation_Predictions_for_each_label,TASK_A_VALID_OUTPUT_PATH,validation_doc_ids)\n",
        "\n",
        "  # Output predictions to file for testing dataset\n",
        "  testing_doc_ids = svm_classifier.testing_corpus['ID'].values\n",
        "  svm_classifier.output_results_to_file(Testing_Predictions_for_each_label,TASK_A_TEST_OUTPUT_PATH,testing_doc_ids)\n",
        "\n",
        "def bi_lstm_experiment():\n",
        "  # Instantiate the BiLSTM object\n",
        "  bilstm_model = BiLSTM()\n",
        "  #Read in the training corpus and validation corpus and testing corpus\n",
        "  bilstm_model.training_corpus = bilstm_model.read_and_format_input_file(TRAINING_PATH)\n",
        "  bilstm_model.validation_corpus = bilstm_model.read_and_format_input_file(VALID_DATA_PATH)\n",
        "  bilstm_model.testing_corpus = bilstm_model.read_and_format_input_file(TEST_DATA_PATH,test=True)\n",
        "\n",
        "  #Clean the training and validation corpus as well as the testing corpus\n",
        "  bilstm_model.training_corpus['Combined_TitlePlot'] = bilstm_model.training_corpus['Combined_TitlePlot'].map(lambda doc: bilstm_model.clean_document(doc))\n",
        "  bilstm_model.validation_corpus['Combined_TitlePlot'] = bilstm_model.validation_corpus['Combined_TitlePlot'].map(lambda doc: bilstm_model.clean_document(doc))\n",
        "  bilstm_model.testing_corpus['Combined_TitlePlot'] = bilstm_model.testing_corpus['Combined_TitlePlot'].map(lambda doc: bilstm_model.clean_document(doc))\n",
        "\n",
        "  # Tokenise the data\n",
        "  MAX_SEQ_LEN,X_train,X_valid,X_test, X_train_padded, X_valid_padded,X_test_padded,train_labels,validation_labels = bilstm_model.tokenise_and_padd(VOCAB_SIZE)\n",
        "\n",
        "  #Train the model\n",
        "  trained_model = bilstm_model.build_and_train_model(VOCAB_SIZE,MAX_SEQ_LEN,EPOCHS,BATCH_SIZE,EMBEDDING_DIM,X_train_padded,train_labels)\n",
        "\n",
        "  #Make predicitons on the validation dataset and write to file\n",
        "  valid_b_pred = bilstm_model.predict(trained_model,X_valid_padded)\n",
        "  valid_document_ids = bilstm_model.validation_corpus['ID'].values\n",
        "  bilstm_model.write_results_to_file(valid_b_pred,TASK_B_VALID_OUTPUT_PATH,valid_document_ids)\n",
        "\n",
        "  #Make predicitons on the testing dataset and write to file\n",
        "  test_b_pred = bilstm_model.predict(trained_model,X_test_padded)\n",
        "  test_document_ids = bilstm_model.testing_corpus['ID'].values\n",
        "  bilstm_model.write_results_to_file(test_b_pred,TASK_B_TEST_OUTPUT_PATH,test_document_ids)"
      ],
      "metadata": {
        "id": "53axvtF9XTrr"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def time_taken_formatting(elapsed_time):\n",
        "  hrs, remainder= divmod(elapsed_time,3600)\n",
        "  mins,remainder = divmod(remainder,60)\n",
        "  secs, ms = divmod(remainder,1)\n",
        "  formatted_time_printing = \"{:02} hrs {:02} mins {:02} secs {:3} ms\".format(int(hrs), int(mins), int(secs), round(ms * 1000))\n",
        "  print(f\"Time taken - {formatted_time_printing}\")\n",
        "\n",
        "def main():\n",
        "  Svm_experiment()\n",
        "  bi_lstm_experiment()\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "id": "4Jp92PFQpcy3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}