{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "10699403 - Task 1 - Methods a and b"
      ],
      "metadata": {
        "id": "nR1GxXhUAv9n"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SXZbh3_4_99s"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import nltk\n",
        "import numpy as np\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import string\n",
        "import pandas as pd\n",
        "import csv\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Download NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e7FKZaLEASnU"
      },
      "outputs": [],
      "source": [
        "# Function to preprocess data by tokenizing, lemmatizing, and removing stopwords and punctuation\n",
        "def preprocess_data(data, training_data=True):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    final_tokens = []\n",
        "\n",
        "    for document in data:\n",
        "        # Convert the document to lowercase\n",
        "        document = document.lower()\n",
        "        # Tokenize the document\n",
        "        tokens = word_tokenize(document)\n",
        "\n",
        "        # Lemmatize tokens, remove punctuation and stopwords\n",
        "        cleaned_tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in string.punctuation and token not in stop_words]\n",
        "        final_tokens.append(cleaned_tokens)\n",
        "\n",
        "    if training_data:\n",
        "        return final_tokens\n",
        "    # Combine the tokens into a string for non-training data\n",
        "    return ' '.join([' '.join(inner_list) for inner_list in final_tokens])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nnCproLKDIhX"
      },
      "outputs": [],
      "source": [
        "# Read training data from a CSV file\n",
        "df = pd.read_csv('./data/Training-dataset.csv')\n",
        "training_data = preprocess_data((df['title'] + ' ' + df['plot_synopsis']).tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nuuhzFiZFLRx"
      },
      "outputs": [],
      "source": [
        "# Read in validation data from a CSV file and split into word pairs for TF-IDF and Word2Vec similarity scoring\n",
        "word_pairs = []\n",
        "with open('./data/Task-1-validation-dataset.csv', 'r') as csvfile:\n",
        "  csvreader = csv.reader(csvfile)\n",
        "  for row in csvreader:\n",
        "    row[1] = preprocess_data([row[1]], training_data=False)\n",
        "    row[2] = preprocess_data([row[2]], training_data=False)\n",
        "    word_pairs.append(row)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KxxA4_p_EDdY"
      },
      "outputs": [],
      "source": [
        "# Initialize a TF-IDF vectorizer and transform the training data\n",
        "tfidf_vec = TfidfVectorizer()\n",
        "tfidf_data = []\n",
        "for data in training_data:\n",
        "    # Combine the lemmatized tokens into a string for each document\n",
        "    tfidf_data.append(' '.join(data))\n",
        "tfidf_mat = tfidf_vec.fit_transform(tfidf_data)\n",
        "term_index_dict = {term: index for index, term in enumerate(tfidf_vec.get_feature_names_out())}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sl69XDrvHkBX"
      },
      "outputs": [],
      "source": [
        "# Train a Word2Vec model on the preprocessed training data\n",
        "word2vec_model = Word2Vec(sentences=training_data, vector_size=100, window=5, min_count=1, workers=4, epochs=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EhortEdQEVi9"
      },
      "outputs": [],
      "source": [
        "# Function to find the TF-IDF vector for a given term\n",
        "def find_word_vec(term):\n",
        "  words = term.split()\n",
        "  # Extract TF-IDF vectors for each word in the term\n",
        "  word_vectors = [tfidf_mat[:, term_index_dict.get(word, None)].toarray() for word in words if word in term_index_dict]\n",
        "  if not word_vectors:\n",
        "    # If none of the words are present in the TF-IDF matrix, return a zero vector\n",
        "    return np.zeros((tfidf_mat.shape[1],))\n",
        "  # Calculate the mean vector for the term\n",
        "  mean_vector = np.mean(word_vectors, axis=0).flatten()\n",
        "  return mean_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xGp3aRMREtvj"
      },
      "outputs": [],
      "source": [
        "# Function to calculate TF-IDF cosine similarity score between two vectors\n",
        "def tfidf_cosine_similarity_score(word1_vec, word2_vec):\n",
        "  try:\n",
        "     # Reshape vectors for cosine similarity calculation\n",
        "    word1_vec = word1_vec.reshape(1, -1)\n",
        "    word2_vec = word2_vec.reshape(1, -1)\n",
        "    # Calculate cosine similarity\n",
        "    return cosine_similarity(word1_vec, word2_vec)[0][0]\n",
        "  except ValueError:\n",
        "    # Handle the case where the vectors have incompatible shapes\n",
        "    return 0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2VUbbebsIoaj"
      },
      "outputs": [],
      "source": [
        "# Function to calculate Word2Vec cosine similarity score between two words\n",
        "def word2vec_cosine_similarity_score(word1, word2):\n",
        "  word1_list = word1.split()\n",
        "  word2_list = word2.split()\n",
        "\n",
        "  # Check if all words in both lists are out-of-vocabulary (OOV) in the Word2Vec model\n",
        "  all_word1_oov = all(word not in word2vec_model.wv for word in word1_list)\n",
        "  all_word2_oov = all(word not in word2vec_model.wv for word in word2_list)\n",
        "\n",
        "  if all_word1_oov and all_word2_oov:\n",
        "     # If both word lists are OOV, return a similarity score of 0\n",
        "    return 0.0\n",
        "  else:\n",
        "    word1_vectors =[]\n",
        "    word2_vectors =[]\n",
        "\n",
        "    # Create a zero vector with the same shape as a word vector in the Word2Vec model\n",
        "    zeros_vector = np.zeros_like(word2vec_model.wv.get_vector(word2vec_model.wv.index_to_key[0]).reshape(1, -1))\n",
        "\n",
        "    if all_word1_oov:\n",
        "      # If all words in word1 are OOV, append a zero vector to the list\n",
        "      word1_vectors.append(zeros_vector)\n",
        "    else:\n",
        "      # Otherwise, append the Word2Vec vectors for each word in word1 to the list\n",
        "      for word in word1_list:\n",
        "        if word in word2vec_model.wv:\n",
        "          word1_vectors.append(word2vec_model.wv[word].reshape(1, -1))\n",
        "      # Extend the list with zero vectors to match the length of word1_list so that we have a vector for every word\n",
        "      word1_vectors.extend([np.zeros_like(word1_vectors[0]) for _ in range(len(word1_list) - len(word1_vectors))])\n",
        "\n",
        "    if all_word2_oov:\n",
        "      # If all words in word1 are OOV, append a zero vector to the list\n",
        "      word2_vectors.append(zeros_vector)\n",
        "    else:\n",
        "      # Otherwise, append the Word2Vec vectors for each word in word1 to the list\n",
        "      for word in word2_list:\n",
        "        if word in word2vec_model.wv:\n",
        "          word2_vectors.append(word2vec_model.wv[word].reshape(1, -1))\n",
        "      # Extend the list with zero vectors to match the length of word1_list so that we have a vector for every word\n",
        "      word2_vectors.extend([np.zeros_like(word2_vectors[0]) for _ in range(len(word2_list) - len(word2_vectors))])\n",
        "\n",
        "    # Calculate cosine similarity between the mean vectors of word1 and word2\n",
        "    return cosine_similarity(np.mean(word1_vectors, axis=0), np.mean(word2_vectors, axis=0))[0][0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5AqlivecFWkO"
      },
      "outputs": [],
      "source": [
        "tfidf_results = []\n",
        "word2vec_results = []\n",
        "for row in word_pairs:\n",
        "  # Calculate the validation TF-IDF cosine similarity and store the result in tfidf_results\n",
        "  tfidf_similarity = tfidf_cosine_similarity_score(find_word_vec(row[1]), find_word_vec(row[2]))\n",
        "  tfidf_results.append([row[0], tfidf_similarity])\n",
        "\n",
        "  # Calculate the validation Word2Vec cosine similarity and store the result in word2vec_results\n",
        "  word2vec_similarity = word2vec_cosine_similarity_score(row[1], row[2])\n",
        "  word2vec_results.append([row[0], word2vec_similarity])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yzNsqX5oHVKL"
      },
      "outputs": [],
      "source": [
        "# Write the validation TF-IDF results to a CSV file\n",
        "with open('./data/10699403-Task1-method-a-validation.csv', 'w', newline='') as csvfile:\n",
        "  csvwriter = csv.writer(csvfile)\n",
        "  csvwriter.writerows(tfidf_results)\n",
        "\n",
        "# Write the validation Word2Vec results to a CSV file\n",
        "with open('./data/10699403-Task1-method-b-validation.csv', 'w', newline='') as csvfile:\n",
        "  csvwriter = csv.writer(csvfile)\n",
        "  csvwriter.writerows(word2vec_results)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_word_pairs = []\n",
        "# Read in test data from a CSV file and split into word pairs for TF-IDF and Word2Vec similarity scoring\n",
        "with open('./data/Task-1-test-dataset1.csv', 'r') as csvfile:\n",
        "  csvreader = csv.reader(csvfile)\n",
        "  for row in csvreader:\n",
        "    row[1] = preprocess_data([row[1]], training_data=False)\n",
        "    row[2] = preprocess_data([row[2]], training_data=False)\n",
        "    test_word_pairs.append(row)"
      ],
      "metadata": {
        "id": "8HVYzn1ZWqlt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_test_results = []\n",
        "word2vec_test_results = []\n",
        "for row in test_word_pairs:\n",
        "  # Calculate the test TF-IDF cosine similarity and store the result in tfidf_results\n",
        "  tfidf_similarity = tfidf_cosine_similarity_score(find_word_vec(row[1]), find_word_vec(row[2]))\n",
        "  tfidf_test_results.append([row[0], tfidf_similarity])\n",
        "\n",
        "  # Calculate the test Word2Vec cosine similarity and store the result in word2vec_results\n",
        "  word2vec_similarity = word2vec_cosine_similarity_score(row[1], row[2])\n",
        "  word2vec_test_results.append([row[0], word2vec_similarity])"
      ],
      "metadata": {
        "id": "Y1huuoihXJ0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write the test TF-IDF results to a CSV file\n",
        "with open('./data/10699403-Task1-method-a.csv', 'w', newline='') as csvfile:\n",
        "  csvwriter = csv.writer(csvfile)\n",
        "  csvwriter.writerows(tfidf_test_results)\n",
        "\n",
        "# Write the test Word2Vec results to a CSV file\n",
        "with open('./data/10699403-Task1-method-b.csv', 'w', newline='') as csvfile:\n",
        "  csvwriter = csv.writer(csvfile)\n",
        "  csvwriter.writerows(word2vec_test_results)"
      ],
      "metadata": {
        "id": "FsFgtjmLXlcX"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}