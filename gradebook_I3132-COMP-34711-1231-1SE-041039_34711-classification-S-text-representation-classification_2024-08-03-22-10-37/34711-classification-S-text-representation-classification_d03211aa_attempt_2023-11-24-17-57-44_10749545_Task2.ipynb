{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "TWahD9PTde1r"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "# Initialize empty lists to store the data\n",
        "plots = []\n",
        "categories_list = []\n",
        "\n",
        "# Initialize an empty list to store the category names\n",
        "categories_str_list = []\n",
        "\n",
        "# Open the CSV file for reading\n",
        "with open('/content/Training-dataset.csv', mode='r', newline='', encoding='utf-8') as file:\n",
        "    csv_reader = csv.reader(file)\n",
        "\n",
        "    # Read the header row to get the category names\n",
        "    header_row = next(csv_reader)\n",
        "    categories_str_list = header_row[3:12]\n",
        "\n",
        "    # Iterate through each row in the CSV file\n",
        "    for row in csv_reader:\n",
        "        # Extract the plot_synopsis and add it to the plots list\n",
        "        plot_synopsis = row[2]\n",
        "        plots.append(plot_synopsis)\n",
        "\n",
        "        # Extract the categories (column indices 3 to 11) and add them as a list to the categories_list\n",
        "        categories = [int(row[i]) for i in range(3, 12)]\n",
        "        categories_list.append(categories)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "jndQHh_xgUa3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b19156ff-1ec0-4fa7-81dd-9a0155c603da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "def clean_plot(plot):\n",
        "    # Remove all characters that are not alphabetic\n",
        "    plot = re.sub(r'[^a-zA-Z ]', '', plot)\n",
        "\n",
        "    return plot\n",
        "\n",
        "# Initialize NLTK\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')  # Download the WordNet database\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Define a function to lemmatize a list of tokens\n",
        "def lemmatize_tokens(tokens):\n",
        "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "# Remove stop words\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "def process_plot(plot):\n",
        "  cleaned_plot = clean_plot(plot)\n",
        "  tokenized_plot = word_tokenize(cleaned_plot.lower())\n",
        "  lemmatized_plot = lemmatize_tokens(tokenized_plot)\n",
        "  filtered_plot = [word for word in lemmatized_plot if word not in stop_words]\n",
        "  return filtered_plot\n",
        "\n",
        "processed_plots = [process_plot(plot) for plot in plots]\n",
        "\n",
        "# join list of words back into a single string\n",
        "training_plots = [' '.join(tokens) for tokens in processed_plots]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        },
        "id": "0ejqGQiVZzfa",
        "outputId": "8817c19f-2b98-4083-c995-ef113b1f7bf5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OneVsRestClassifier(estimator=SVC(probability=True))"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>OneVsRestClassifier(estimator=SVC(probability=True))</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OneVsRestClassifier</label><div class=\"sk-toggleable__content\"><pre>OneVsRestClassifier(estimator=SVC(probability=True))</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(probability=True)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(probability=True)</pre></div></div></div></div></div></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# training data preparation\n",
        "vectorizer = TfidfVectorizer()  # Initialize a TF-IDF Vectorizer\n",
        "X = vectorizer.fit_transform(training_plots)  # Transform plots to TF-IDF feature vectors\n",
        "Y = np.array(categories_list)  # Convert categories to a numpy array for labels\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(\n",
        "    X, Y, test_size=0.2, random_state=42)  # 20% of data for testing, 80% for training\n",
        "\n",
        "# Train the multi-label SVM model\n",
        "svm = SVC(probability=True)  # Initialize the Support Vector Classifier with probability estimates\n",
        "multi_label_model = OneVsRestClassifier(svm)  # Wrap SVC in a OneVsRestClassifier for multi-label classification\n",
        "multi_label_model.fit(X_train, Y_train)  # Fit the model to the training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "l98Yf-2hMmp-"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "csv_file_path = '/content/Task-2-validation-dataset(1).csv'\n",
        "\n",
        "\n",
        "validation_ids = []\n",
        "validation_titles = []\n",
        "validation_plots = []\n",
        "\n",
        "\n",
        "with open(csv_file_path, 'r', encoding='utf-8') as file:\n",
        "    csv_reader = csv.DictReader(file)\n",
        "    for row in csv_reader:\n",
        "        validation_ids.append(row['ID'])\n",
        "        validation_titles.append(row['title'])\n",
        "        validation_plots.append(row['plot_synopsis'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "csv_file_path = '/content/Task-2-test-dataset1.csv'\n",
        "\n",
        "\n",
        "test_ids = []\n",
        "test_titles = []\n",
        "test_plots = []\n",
        "\n",
        "\n",
        "with open(csv_file_path, 'r', encoding='utf-8') as file:\n",
        "    csv_reader = csv.DictReader(file)\n",
        "    for row in csv_reader:\n",
        "        test_ids.append(row['ID'])\n",
        "        test_titles.append(row['title'])\n",
        "        test_plots.append(row['plot_synopsis'])"
      ],
      "metadata": {
        "id": "MUiXCLOKWk11"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "HKI63rnK5iS6"
      },
      "outputs": [],
      "source": [
        "processed_validation_plots = [process_plot(plot) for plot in validation_plots]\n",
        "\n",
        "# join list of words back into a single string\n",
        "validation_plots_str = [' '.join(ready_plot) for ready_plot in processed_validation_plots]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "converted_validation_plots = vectorizer.transform(validation_plots_str)\n",
        "validation_predictions = multi_label_model.predict(converted_validation_plots)"
      ],
      "metadata": {
        "id": "BjMJ0mJJSeAn"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "output_file_path = '10749545-Task2-method-a-validation.csv'\n",
        "\n",
        "# Open the file in write mode\n",
        "with open(output_file_path, 'w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "\n",
        "    # Iterate over zipped IDs and predictions\n",
        "    for id, prediction in zip(validation_ids, validation_predictions):\n",
        "        # Ensure that prediction is in list format and write to the file\n",
        "        row = [id] + list(prediction)\n",
        "        writer.writerow(row)"
      ],
      "metadata": {
        "id": "dmA_aMo9TZVw"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_test_plots = [process_plot(plot) for plot in test_plots]\n",
        "\n",
        "# join list of words back into a single string\n",
        "test_plots_str = [' '.join(plot) for plot in processed_test_plots]"
      ],
      "metadata": {
        "id": "UR5jJKPyYHzN"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "converted_test_plots = vectorizer.transform(test_plots_str)\n",
        "test_predictions = multi_label_model.predict(converted_test_plots)"
      ],
      "metadata": {
        "id": "VuebY99aYbyY"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "output_file_path = '10749545-Task2-method-a.csv'\n",
        "\n",
        "# Open the file in write mode\n",
        "with open(output_file_path, 'w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "\n",
        "    # Iterate over zipped IDs and predictions\n",
        "    for id, prediction in zip(test_ids, test_predictions):\n",
        "        # Ensure that prediction is in list format and write to the file\n",
        "        row = [id] + list(prediction)\n",
        "        writer.writerow(row)"
      ],
      "metadata": {
        "id": "pYnUCWdJYtwA"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%run '/content/task2_eval_script_student_version(1).py' /content/10749545-Task2-method-a-validation.csv '/content/Task-2-validation-dataset(1).csv'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hz9uKPbsXE8N",
        "outputId": "1a13c0b2-ba52-41a5-9832-993f1520f296"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class level: \n",
            "Class  1 precision: 0.9091 recall: 0.0571\n",
            "Class  2 precision: 0.8571 recall: 0.0972\n",
            "Class  3 precision: 0.7586 recall: 0.0748\n",
            "Class  4 precision: 0.0000 recall: 0.0000\n",
            "Class  5 precision: 0.7409 recall: 0.6644\n",
            "Class  6 precision: 0.4545 recall: 0.0211\n",
            "Class  7 precision: 0.8525 recall: 0.1793\n",
            "Class  8 precision: 0.0000 recall: 0.0000\n",
            "Class  9 precision: 0.7019 recall: 0.4429\n",
            "----------------------------\n",
            "Movie (document) level: \n",
            "Precision: 0.4384\n",
            "Recall: 0.3199\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Embedding\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Initialize a tokenizer for text data with a vocabulary of 80000 most frequent words\n",
        "tokenizer = Tokenizer(num_words=80000)\n",
        "# Fit the tokenizer on the training plots to create a word index\n",
        "tokenizer.fit_on_texts(training_plots)\n",
        "# Convert the list of training plots into a list of sequences of integers\n",
        "sequences = tokenizer.texts_to_sequences(training_plots)\n",
        "\n",
        "import numpy as np\n",
        "# Convert the category list into a numpy array for use in model training\n",
        "categories_list_np = np.array(categories_list)\n",
        "\n",
        "# Calculate the length of each plot to find the average plot\n",
        "plot_lengths = [len(plot) for plot in training_plots]\n",
        "average_plot_length = int(sum(plot_lengths) / len(plot_lengths))\n",
        "\n",
        "# Pad the sequences so that all have the same length for model input\n",
        "padded_sequences = pad_sequences(sequences, maxlen=average_plot_length)\n",
        "\n",
        "# Define the model architecture using the Sequential API\n",
        "model = Sequential()\n",
        "# Add an Embedding layer to transform indices into dense vectors of a fixed size\n",
        "model.add(Embedding(input_dim=80000, output_dim=128, input_length=average_plot_length))\n",
        "# First LSTM layer\n",
        "model.add(LSTM(64, return_sequences=True))\n",
        "# Add an LSTM layer with 16 units\n",
        "model.add(LSTM(64, return_sequences=False))\n",
        "# Add a Dense output layer with a number of neurons equal to the number of categories, using sigmoid activation\n",
        "model.add(Dense(categories_list_np.shape[1], activation='sigmoid'))\n",
        "\n",
        "# Compile the model with binary crossentropy loss and adam optimizer, tracking accuracy\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Create an EarlyStopping callback to stop training when the validation loss stops improving\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1)\n",
        "\n",
        "# Train the model on the padded sequences, using early stopping and a validation split of 20%\n",
        "model.fit(padded_sequences, categories_list_np, batch_size=132, epochs=5, callbacks=[early_stopping], validation_split=0.2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "De9teoCDZb8I",
        "outputId": "6bbec152-881b-4a64-9f22-7ae11f98af12"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "51/51 [==============================] - 460s 9s/step - loss: 0.5115 - accuracy: 0.2471 - val_loss: 0.4591 - val_accuracy: 0.2633\n",
            "Epoch 2/5\n",
            "51/51 [==============================] - 452s 9s/step - loss: 0.4607 - accuracy: 0.2584 - val_loss: 0.4585 - val_accuracy: 0.2633\n",
            "Epoch 3/5\n",
            "51/51 [==============================] - 455s 9s/step - loss: 0.4604 - accuracy: 0.2584 - val_loss: 0.4590 - val_accuracy: 0.2633\n",
            "Epoch 4/5\n",
            "51/51 [==============================] - 453s 9s/step - loss: 0.4600 - accuracy: 0.2584 - val_loss: 0.4583 - val_accuracy: 0.2633\n",
            "Epoch 5/5\n",
            "51/51 [==============================] - 456s 9s/step - loss: 0.4573 - accuracy: 0.2584 - val_loss: 0.4544 - val_accuracy: 0.2621\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7f1a2aad8ee0>"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sequenced_validation = tokenizer.texts_to_sequences(validation_plots_str)\n",
        "padded_validation= pad_sequences(sequenced_validation, maxlen=average_plot_length)\n",
        "predict = model.predict(padded_validation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9xesDSWnkcX",
        "outputId": "690c5322-8720-43da-c772-27ddc04449fb"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "38/38 [==============================] - 37s 965ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_predictions(predictions):\n",
        "    binary_predictions = []\n",
        "    for pred in predictions:\n",
        "        if np.any(pred > 0.6):\n",
        "            # If any probability is greater than 0.6, convert to 1, else 0\n",
        "            binary_pred = np.where(pred > 0.6, 1, 0)\n",
        "        else:\n",
        "            # If no probability exceeds 0.6, take the index of the max value\n",
        "            max_index = np.argmax(pred)\n",
        "            binary_pred = np.zeros_like(pred)\n",
        "            binary_pred[max_index] = 1\n",
        "        binary_predictions.append(binary_pred.astype(int))  # Cast to integer\n",
        "    return np.array(binary_predictions)\n",
        "\n",
        "validation_predictions_deep = convert_predictions(predict)"
      ],
      "metadata": {
        "id": "yLrNd1UCo1d_"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "# Path where you want to save the CSV file\n",
        "output_file_path = '10749545-Task2-method-b-validation.csv'\n",
        "\n",
        "# Open the file in write mode\n",
        "with open(output_file_path, 'w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "\n",
        "    # Iterate over zipped IDs and predictions\n",
        "    for id, prediction in zip(validation_ids, validation_predictions_deep):\n",
        "        # Ensure that prediction is in list format and write to the file\n",
        "        row = [id] + list(prediction)\n",
        "        writer.writerow(row)"
      ],
      "metadata": {
        "id": "4AYlXWEnrCrl"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequenced_test = tokenizer.texts_to_sequences(test_plots_str)\n",
        "padded_test= pad_sequences(sequenced_test, maxlen=average_plot_length)\n",
        "predict = model.predict(padded_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E-wkvXkwdXMi",
        "outputId": "fab90677-f455-47cc-f280-95bd5c6ddb40"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "38/38 [==============================] - 35s 928ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_predictions_deep = convert_predictions(predict)\n",
        "\n",
        "output_file_path = '10749545-Task2-method-b.csv'\n",
        "\n",
        "# Open the file in write mode\n",
        "with open(output_file_path, 'w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "\n",
        "    # Iterate over zipped IDs and predictions\n",
        "    for id, prediction in zip(test_ids, test_predictions_deep):\n",
        "        # Ensure that prediction is in list format and write to the file\n",
        "        row = [id] + list(prediction)\n",
        "        writer.writerow(row)"
      ],
      "metadata": {
        "id": "CoVrwDeqc6vD"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%run '/content/task2_eval_script_student_version(1).py' /content/10749545-Task2-method-b-validation.csv '/content/Task-2-validation-dataset(1).csv'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SbCdUWjAsRbS",
        "outputId": "5ce2aaec-31ad-4025-fbc1-792fda0d4262"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class level: \n",
            "Class  1 precision: 0.0000 recall: 0.0000\n",
            "Class  2 precision: 0.0000 recall: 0.0000\n",
            "Class  3 precision: 0.0000 recall: 0.0000\n",
            "Class  4 precision: 0.0000 recall: 0.0000\n",
            "Class  5 precision: 0.4911 recall: 0.9966\n",
            "Class  6 precision: 0.0000 recall: 0.0000\n",
            "Class  7 precision: 0.0000 recall: 0.0000\n",
            "Class  8 precision: 0.0000 recall: 0.0000\n",
            "Class  9 precision: 0.4444 recall: 0.0095\n",
            "----------------------------\n",
            "Movie (document) level: \n",
            "Precision: 0.4907\n",
            "Recall: 0.2727\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}