{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Natural Langauge Processing CW - Task 2: Text Classification"
      ],
      "metadata": {
        "id": "i8ujOYXZUqWK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll start by importing all the relevant libraries and loading the data that will be used across the notebook:"
      ],
      "metadata": {
        "id": "MfouD8KRdGBn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "# from sklearn.preprocessing import normalize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "import nltk\n",
        "import time\n",
        "from nltk.stem import PorterStemmer\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8lIqyPtLCNVt",
        "outputId": "82257e8a-e6a4-4241-d0cc-a427a1119c0d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the training dataset:"
      ],
      "metadata": {
        "id": "9UlYmk-fdIyl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load your training data\n",
        "train_data = pd.read_csv('./data/Training-dataset.csv')"
      ],
      "metadata": {
        "id": "Ks0Xs4ajdUEY"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the validation dataset:"
      ],
      "metadata": {
        "id": "h5KPM0W4d-tr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load your test data\n",
        "val_data = pd.read_csv('./data/Task-2-validation-dataset.csv')"
      ],
      "metadata": {
        "id": "PUjLBVz5d_UJ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the test dataset:"
      ],
      "metadata": {
        "id": "FsZZnaa9HEio"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = pd.read_csv('./data/Task-2-test-dataset.csv')"
      ],
      "metadata": {
        "id": "aUboSPByHIJi"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Naive Bayes"
      ],
      "metadata": {
        "id": "yPv1pejCU61I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To classify multi labels we will be using naive bayes:"
      ],
      "metadata": {
        "id": "RLzuG-UCiziv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the preprocessing function similar to task 1:"
      ],
      "metadata": {
        "id": "SEUUi0mteROu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Tokenization\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Lemmatization and stemming\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    stemmer = PorterStemmer()\n",
        "    tokens = [lemmatizer.lemmatize(stemmer.stem(token)) for token in tokens]\n",
        "\n",
        "    # Join tokens back into a string\n",
        "    return ' '.join(tokens)"
      ],
      "metadata": {
        "id": "iDk9mxsRCUp_"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we apply the preprocess_text function to the plot synopsis column in the training data. We also create a list of the labels. Then we create a vectorizer and fit it to the training data. We then train the multinomial naive bayes classifier as a multioutput classifier."
      ],
      "metadata": {
        "id": "yrhwBYG5eXJO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "# Preprocess the plot_synopsis column\n",
        "text = train_data['plot_synopsis'].apply(preprocess_text)\n",
        "\n",
        "# create a list of columns representing labels\n",
        "labels = train_data[['comedy', 'cult', 'flashback', 'historical', 'murder', 'revenge', 'romantic', 'scifi', 'violence']]"
      ],
      "metadata": {
        "id": "k5N7fJNwZSnU"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create a TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000, ngram_range=(1, 3))\n",
        "\n",
        "# Transform the data\n",
        "train_tfidf = vectorizer.fit_transform(text)\n",
        "\n",
        "# Train a Multinomial Naive Bayes classifier as a multi-output classifier\n",
        "classifier = MultiOutputClassifier(MultinomialNB(), n_jobs=-1)\n",
        "classifier.fit(train_tfidf, labels)\n",
        "\n",
        "end = time.time()\n",
        "time_el = end - start\n",
        "print(f'time: {time_el}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "APsQRiGNUz_S",
        "outputId": "baa30a47-5f30-47d6-bf4e-4eb03675817d"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 204.9425265789032\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To asses the performance of the model, we use the validation dataset we loaded before, apply the preprocessing, then predict the labels."
      ],
      "metadata": {
        "id": "TMchk2M3eqj3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "# Preprocess the plot_synopsis column\n",
        "val_text = val_data['plot_synopsis'].apply(preprocess_text)\n",
        "\n",
        "# Transform the test data using the same TfidfVectorizer\n",
        "val_tfidf = vectorizer.transform(val_text)\n",
        "\n",
        "# Make predictions on the test set\n",
        "val_predictions = classifier.predict(val_tfidf)\n",
        "end = time.time()\n",
        "time_el = end - start\n",
        "print(f'time: {time_el}')"
      ],
      "metadata": {
        "id": "F2RE27vbVtNU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "acbab14b-5f87-4f34-cf74-66c1774c3abd"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 25.55899167060852\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save the results in a csv file:"
      ],
      "metadata": {
        "id": "o7_OtQpZerrT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the results to a CSV file\n",
        "val_results_df = pd.DataFrame({\n",
        "    'ID': val_data['ID'],\n",
        "    'comedy': val_predictions[:, 0],\n",
        "    'cult': val_predictions[:, 1],\n",
        "    'flashback': val_predictions[:, 2],\n",
        "    'historical': val_predictions[:, 3],\n",
        "    'murder': val_predictions[:, 4],\n",
        "    'revenge': val_predictions[:, 5],\n",
        "    'romantic': val_predictions[:, 6],\n",
        "    'scifi': val_predictions[:, 7],\n",
        "    'violence': val_predictions[:, 8],\n",
        "})\n",
        "\n",
        "val_results_df.to_csv('10693727-Task2-method-a-validation.csv', index=False, header=False)"
      ],
      "metadata": {
        "id": "lJfX1VX7espR"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Results:**\n",
        "\n"
      ],
      "metadata": {
        "id": "jb54sYoRUNgh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing:"
      ],
      "metadata": {
        "id": "Scp5ZKh19ifp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have our model trained using our training dataset, and tested using the validation dataset, we can test it on unseen data. We will load the test dataset and run the model just like we did for the validation dataset, and save the results in a csv file."
      ],
      "metadata": {
        "id": "M1bd_RR19xp4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#start time\n",
        "start = time.time()\n",
        "# Preprocess the plot_synopsis column\n",
        "test_text = test_data['plot_synopsis'].apply(preprocess_text)\n",
        "\n",
        "# Transform the test data using the same TfidfVectorizer\n",
        "test_tfidf = vectorizer.transform(test_text)\n",
        "\n",
        "# Make predictions on the test set\n",
        "test_predictions = classifier.predict(test_tfidf)\n",
        "#end time\n",
        "end = time.time()\n",
        "#print the time elapsed\n",
        "elapsed_time = end - start\n",
        "print(f'Time taken to test the model: {elapsed_time} seconds')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_1X4QMfO9xp-",
        "outputId": "53245402-6e1e-4020-862b-4237f62f78ed"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time taken to test the model: 25.674399375915527 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save the results:"
      ],
      "metadata": {
        "id": "npZbZ5RB9xp-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the results to a CSV file\n",
        "test_results_df = pd.DataFrame({\n",
        "    'ID': test_data['ID'],\n",
        "    'comedy': test_predictions[:, 0],\n",
        "    'cult': test_predictions[:, 1],\n",
        "    'flashback': test_predictions[:, 2],\n",
        "    'historical': test_predictions[:, 3],\n",
        "    'murder': test_predictions[:, 4],\n",
        "    'revenge': test_predictions[:, 5],\n",
        "    'romantic': test_predictions[:, 6],\n",
        "    'scifi': test_predictions[:, 7],\n",
        "    'violence': test_predictions[:, 8],\n",
        "})\n",
        "\n",
        "test_results_df.to_csv('10693727-Task2-method-a.csv', index=False, header=False)"
      ],
      "metadata": {
        "id": "ZJROG3dL9xp-"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bi-LSTM"
      ],
      "metadata": {
        "id": "zlqUr-7RVx77"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first thing is to preprocess the text. We also created a list of labels:"
      ],
      "metadata": {
        "id": "i2IzlF81i3MA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "# Preprocess the plot_synopsis column\n",
        "train_data['plot_synopsis'] = train_data['plot_synopsis'].apply(preprocess_text)\n",
        "\n",
        "text = train_data['plot_synopsis']\n",
        "labels = train_data[['comedy', 'cult', 'flashback', 'historical', 'murder', 'revenge', 'romantic', 'scifi', 'violence']]\n",
        "# Convert labels to 0 or 1\n",
        "labels_binary = labels.applymap(lambda x: 1 if x == 1 else 0)"
      ],
      "metadata": {
        "id": "c_hcZKiEMfHu"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We tokenize the words and add pad sequence:"
      ],
      "metadata": {
        "id": "4o_Sba_SQw11"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize and pad sequences\n",
        "max_words = 10000\n",
        "max_len = 200\n",
        "tokenizer = Tokenizer(num_words=max_words, lower=True, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(text)\n",
        "text_seq = tokenizer.texts_to_sequences(text)\n",
        "text_padded = pad_sequences(text_seq, maxlen=max_len)"
      ],
      "metadata": {
        "id": "Z1RJvX1XMh_z"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build the model and compile it:"
      ],
      "metadata": {
        "id": "1EtS3M_KMpYH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the LSTM model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=max_words, output_dim=100, input_length=max_len))\n",
        "model.add(Bidirectional(LSTM(64)))\n",
        "model.add(Dense(9, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "5D-yVT20V16l"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, train the model:"
      ],
      "metadata": {
        "id": "HQvQNN1DMsat"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(text_padded, labels_binary, epochs=7, batch_size=32)\n",
        "end = time.time()\n",
        "time_el = end - start\n",
        "print(f'time: {time_el}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vNTF0idITz78",
        "outputId": "7786cdc3-3820-4e1f-c472-8199033a1561"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/7\n",
            "259/259 [==============================] - 33s 91ms/step - loss: 0.4732 - accuracy: 0.2469\n",
            "Epoch 2/7\n",
            "259/259 [==============================] - 9s 36ms/step - loss: 0.4429 - accuracy: 0.2735\n",
            "Epoch 3/7\n",
            "259/259 [==============================] - 7s 26ms/step - loss: 0.4034 - accuracy: 0.3109\n",
            "Epoch 4/7\n",
            "259/259 [==============================] - 6s 24ms/step - loss: 0.3476 - accuracy: 0.3852\n",
            "Epoch 5/7\n",
            "259/259 [==============================] - 6s 22ms/step - loss: 0.2887 - accuracy: 0.4615\n",
            "Epoch 6/7\n",
            "259/259 [==============================] - 5s 19ms/step - loss: 0.2324 - accuracy: 0.5282\n",
            "Epoch 7/7\n",
            "259/259 [==============================] - 5s 20ms/step - loss: 0.1818 - accuracy: 0.5730\n",
            "time: 261.75799441337585\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we built the model and trained it. We preprocess the training data we loaded and tokenize it:"
      ],
      "metadata": {
        "id": "ZpTNZu6xMubv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "# Preprocess the plot_synopsis column\n",
        "val_data['plot_synopsis'] = val_data['plot_synopsis'].apply(preprocess_text)\n",
        "\n",
        "# Assuming 'plot_synopsis' is the column containing plot synopses\n",
        "val = val_data['plot_synopsis']\n",
        "\n",
        "# Tokenize and pad sequences for test data\n",
        "val_seq = tokenizer.texts_to_sequences(val)\n",
        "val_padded = pad_sequences(val_seq, maxlen=max_len)\n"
      ],
      "metadata": {
        "id": "kTP1Ll-pQaft"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we predict the labels on the validation set:"
      ],
      "metadata": {
        "id": "Qieyk9mEMzU5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on the validation set\n",
        "val_predictions = model.predict(val_padded)\n",
        "\n",
        "# Convert predictions to 0 or 1\n",
        "val_predictions_binary = (val_predictions > 0.5).astype(int)\n",
        "\n",
        "end = time.time()\n",
        "\n",
        "time_el = end - start\n",
        "print(f'time: {time_el}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFiBHHdVMz8r",
        "outputId": "1cbc40a3-98c8-40d1-cb0e-723c02f40ad7"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "38/38 [==============================] - 0s 7ms/step\n",
            "time: 24.730493783950806\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now save the validation results to a csv file:"
      ],
      "metadata": {
        "id": "yyC_7re8MQKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the results to a CSV file\n",
        "val_results_df = pd.DataFrame({\n",
        "    'ID': val_data['ID'],  # Assuming 'ID' is the index of your test data\n",
        "    'comedy': val_predictions_binary[:, 0],\n",
        "    'cult': val_predictions_binary[:, 1],\n",
        "    'flashback': val_predictions_binary[:, 2],\n",
        "    'historical': val_predictions_binary[:, 3],\n",
        "    'murder': val_predictions_binary[:, 4],\n",
        "    'revenge': val_predictions_binary[:, 5],\n",
        "    'romantic': val_predictions_binary[:, 6],\n",
        "    'scifi': val_predictions_binary[:, 7],\n",
        "    'violence': val_predictions_binary[:, 8],\n",
        "})\n",
        "\n",
        "val_results_df.to_csv('10693727-Task2-method-b-validation.csv', index=False, header=False)"
      ],
      "metadata": {
        "id": "msMQglv3MQon"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#precision: 43\n",
        "# recall: 42.4"
      ],
      "metadata": {
        "id": "i_E7CO5ZDzbi"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing:"
      ],
      "metadata": {
        "id": "ltl_RsxbNk0W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have our model trained using our training dataset, and tested using the validation dataset, we can test it on unseen data. We will load the test dataset and run the model just like we did for the validation dataset, and save the results in a csv file."
      ],
      "metadata": {
        "id": "260rNeQfNk0b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#start time\n",
        "start = time.time()\n",
        "# Preprocess the plot_synopsis column\n",
        "test_data['plot_synopsis'] = test_data['plot_synopsis'].apply(preprocess_text)\n",
        "\n",
        "# Assuming 'plot_synopsis' is the column containing plot synopses\n",
        "test = test_data['plot_synopsis']\n",
        "\n",
        "# Tokenize and pad sequences for test data\n",
        "test_seq = tokenizer.texts_to_sequences(test)\n",
        "test_padded = pad_sequences(test_seq, maxlen=max_len)\n",
        "\n",
        "# Make predictions on the test set\n",
        "test_predictions = model.predict(test_padded)\n",
        "\n",
        "# Convert predictions to 0 or 1\n",
        "test_predictions_binary = (test_predictions > 0.5).astype(int)\n",
        "\n",
        "#end time\n",
        "end = time.time()\n",
        "#print the time elapsed\n",
        "elapsed_time = end - start\n",
        "print(f'Time taken to test the model: {elapsed_time} seconds')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bba8fbc-9252-45df-979c-61dc902720a0",
        "id": "A0sYzASNNk0c"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "38/38 [==============================] - 0s 7ms/step\n",
            "Time taken to test the model: 27.002696752548218 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save the results:"
      ],
      "metadata": {
        "id": "JY43icPVNk0c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the results to a CSV file\n",
        "test_results_df = pd.DataFrame({\n",
        "    'ID': test_data['ID'],\n",
        "    'comedy': test_predictions_binary[:, 0],\n",
        "    'cult': test_predictions_binary[:, 1],\n",
        "    'flashback': test_predictions_binary[:, 2],\n",
        "    'historical': test_predictions_binary[:, 3],\n",
        "    'murder': test_predictions_binary[:, 4],\n",
        "    'revenge': test_predictions_binary[:, 5],\n",
        "    'romantic': test_predictions_binary[:, 6],\n",
        "    'scifi': test_predictions_binary[:, 7],\n",
        "    'violence': test_predictions_binary[:, 8],\n",
        "})\n",
        "\n",
        "test_results_df.to_csv('10693727-Task2-method-b.csv', index=False, header=False)"
      ],
      "metadata": {
        "id": "YCkF7E4ZNk0c"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CImtB8E4bAZR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}