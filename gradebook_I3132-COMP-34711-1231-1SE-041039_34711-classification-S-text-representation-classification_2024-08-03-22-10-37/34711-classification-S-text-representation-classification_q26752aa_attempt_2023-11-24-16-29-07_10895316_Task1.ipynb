{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eDLqJKoKqu6R"
   },
   "source": [
    "# **q26752aa-Task1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "01nZKXRFgo-p"
   },
   "source": [
    "## Setup Environment\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gI4OVE11PQlA"
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "# All of these are already pre-installed with google colab but I left this just in case\n",
    "!pip install numpy\n",
    "!pip install pandas\n",
    "!pip install nltk\n",
    "!pip install scikit-learn\n",
    "!pip install gensim\n",
    "!pip install 'transformers[torch]'\n",
    "\n",
    "clear_output() # Clear output to hide visual clutter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ayscce5VnY4O"
   },
   "outputs": [],
   "source": [
    "# Main libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import torch\n",
    "import time\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Paths for input and output csv files\n",
    "USERNAME = \"10895316\"\n",
    "INPUT_FOLDER = \"data\"\n",
    "OUTPUT_FOLDER = \"data\"\n",
    "TRAINING_PATH = f\"{INPUT_FOLDER}/Training-dataset.csv\"\n",
    "VALIDATION_PATH = f\"{INPUT_FOLDER}/Task-1-validation-dataset.csv\"\n",
    "TEST_PATH = f\"{INPUT_FOLDER}/Task-1-test-dataset1.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ewp_nBZe9cyG"
   },
   "outputs": [],
   "source": [
    "# Function which calculates the time measured from a start time\n",
    "def measure_time(start_time):\n",
    "    end_time = time.time()\n",
    "    return end_time - start_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cfAJUf1ihfFp"
   },
   "source": [
    "## Load Data\n",
    "---\n",
    "For training, I do not include the movie **title** because this can sometimes obscure the actual context of the initial few words in the synopsis. Additionally, titles tend to contain little information to represent a word or even wrong ones entirely.\n",
    "<br>\n",
    "<br>\n",
    "For example (movie titles):\n",
    "* **Orca** - A single word title which provides zero information.\n",
    "\n",
    "\n",
    "\n",
    "* **Blood and Chocolate** - Semantically, these words are not similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "f5qJ3gsMg_St",
    "outputId": "2500280d-a5ec-4472-fc9e-b8e0efa556b1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>plot_synopsis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>After a recent amount of challenges, Billy Lo ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In the crime-ridden city of Tremont, renowned ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lankester Merrin is a veteran Catholic priest ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"Serendipity Through Seasons\" is a heartwarmin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Young and naive 19-year-old slacker, Adam (Jac...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       plot_synopsis\n",
       "0  After a recent amount of challenges, Billy Lo ...\n",
       "1  In the crime-ridden city of Tremont, renowned ...\n",
       "2  Lankester Merrin is a veteran Catholic priest ...\n",
       "3  \"Serendipity Through Seasons\" is a heartwarmin...\n",
       "4  Young and naive 19-year-old slacker, Adam (Jac..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the .csv training dataset as pandas dataframe\n",
    "training_set = pd.read_csv(TRAINING_PATH, engine='python', encoding='utf-8')\n",
    "validation_set = pd.read_csv(VALIDATION_PATH, engine='python', encoding='utf-8', header=None)\n",
    "test_set = pd.read_csv(TEST_PATH, engine='python', encoding='utf-8', header=None)\n",
    "\n",
    "# Ignore all columns except 'plot_synopsis'\n",
    "training_set = training_set.loc[:, ['plot_synopsis']]\n",
    "training_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iMPRXxrGoLz0"
   },
   "source": [
    "## Method B - **word2vec** model\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hrESlqvi4D8x"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "STEMMER = False\n",
    "BIGRAM = False\n",
    "VECTOR_SIZE = 200\n",
    "WINDOW_SIZE = 10\n",
    "MIN_COUNT = 1\n",
    "EPOCHS = 10\n",
    "ALPHA = 0.025\n",
    "MIN_ALPHA = 0.00005\n",
    "SHRINK_WINDOWS = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gTj9PCaaLyXl"
   },
   "source": [
    "Import tokenizer and stopwords, then download them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import tokenizer and stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "juyCui6a3YLk"
   },
   "source": [
    "Pre-process and prepare training data as tokens for the *word2vec* model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "id": "1hkQpHkM-Ay1",
    "outputId": "217cea8d-bb79-4e1e-c3c1-7085eec3afae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed to pre-process text: 42.40755796432495 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>plot_synopsis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[after, a, recent, amount, of, challenges, bil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[in, the, city, of, tremont, renowned, investi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[lankester, merrin, is, a, veteran, catholic, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[serendipity, through, seasons, is, a, heartwa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[young, and, naive, slacker, adam, jack, lives...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       plot_synopsis\n",
       "0  [after, a, recent, amount, of, challenges, bil...\n",
       "1  [in, the, city, of, tremont, renowned, investi...\n",
       "2  [lankester, merrin, is, a, veteran, catholic, ...\n",
       "3  [serendipity, through, seasons, is, a, heartwa...\n",
       "4  [young, and, naive, slacker, adam, jack, lives..."
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Preprocess the text\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = nltk.PorterStemmer()\n",
    "\n",
    "# Function which prepocesses and tokenizes text\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text.lower()) # Case folding and tokenize\n",
    "    if STEMMER:\n",
    "        tokens = [stemmer.stem(token) for token in tokens if token.isalpha() and token]  # Remove punctuation then stem\n",
    "    else:\n",
    "        tokens = [token for token in tokens if token.isalpha() and token] # Remove punctuation\n",
    "    return tokens\n",
    "\n",
    "# Apply preprocessing to the plot_synopsis column\n",
    "training_tokens = pd.DataFrame(data=training_set['plot_synopsis'].apply(preprocess_text))\n",
    "\n",
    "print(f\"Time elapsed to pre-process text: {measure_time(start_time)} seconds\")\n",
    "training_tokens.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oIIhqS8ILGTf"
   },
   "source": [
    "Train the *word2vec* model on the pre-processed tokenized training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p0aVu8Mt1DJM",
    "outputId": "c7a70b5c-57a0-482a-fe9b-dbbbb945b6d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed to train word2vec model: 45.6935453414917 seconds\n"
     ]
    }
   ],
   "source": [
    "# Import word2vec model from Gensim\n",
    "from gensim.models import Word2Vec, KeyedVectors, Phrases\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Convert dataframe to array list\n",
    "sentences = training_tokens['plot_synopsis'].tolist()\n",
    "\n",
    "# Train a bigram detector for multiword phrases\n",
    "if BIGRAM:\n",
    "    bigram_transformer = Phrases(sentences)\n",
    "    sentences = bigram_transformer[sentences]\n",
    "\n",
    "# Train Word2Vec model\n",
    "w2v_model = Word2Vec(\n",
    "    sentences=sentences,\n",
    "    vector_size=VECTOR_SIZE,\n",
    "    window=WINDOW_SIZE,\n",
    "    min_count=MIN_COUNT,\n",
    "    epochs=EPOCHS,\n",
    "    alpha=ALPHA,\n",
    "    min_alpha=MIN_ALPHA,\n",
    "    shrink_windows=SHRINK_WINDOWS,\n",
    "    )\n",
    "\n",
    "# Store and use only the KeyedVectors instance to reduce memory.\n",
    "w2v_model = w2v_model.wv\n",
    "\n",
    "print(f\"Time elapsed to train word2vec model: {measure_time(start_time)} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9nlsbAJ8LayZ"
   },
   "source": [
    "Define the function which will calculate the cosine similarity between two input terms using the *word2vec* model and it's generated embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lcnptmm_SS6Q"
   },
   "outputs": [],
   "source": [
    "def calculate_cosine_similarity_word2vec(term1, term2):\n",
    "    # Split multi-word terms into individual words\n",
    "    terms1 = term1.lower().split()\n",
    "    terms2 = term2.lower().split()\n",
    "\n",
    "    # Keep words that are in vocabulary\n",
    "    if STEMMER:\n",
    "        valid_terms1 = [stemmer.stem(term) for term in terms1 if stemmer.stem(term) in w2v_model.key_to_index]\n",
    "        valid_terms2 = [stemmer.stem(term) for term in terms2 if stemmer.stem(term) in w2v_model.key_to_index]\n",
    "    else:\n",
    "        valid_terms1 = [term for term in terms1 if term in w2v_model.key_to_index]\n",
    "        valid_terms2 = [term for term in terms2 if term in w2v_model.key_to_index]\n",
    "\n",
    "    # If one of valid terms are empty, then return 0.5\n",
    "    if not valid_terms1 or not valid_terms2:\n",
    "        return 0.5\n",
    "\n",
    "    # Normalise embeddings by calculating mean of valid terms\n",
    "    embedding_term1 = np.mean([w2v_model[term] for term in valid_terms1], axis=0)\n",
    "    embedding_term2 = np.mean([w2v_model[term] for term in valid_terms2], axis=0)\n",
    "\n",
    "    # Return cosine similarity\n",
    "    return cosine_similarity([embedding_term1], [embedding_term2])[0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BYXmuSBSrV8M"
   },
   "source": [
    "## Method C - **BERT** model\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8W496o4z1CEu"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "MODEL_NAME = \"bert-large-uncased-whole-word-masking\"\n",
    "TRUNCATE = True\n",
    "MAX_LENGTH = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NJCq3PbULqLd",
    "outputId": "132ca319-d227-465a-a726-d7d4e0cd104f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AAlsh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import libraries from PyTorch and Hugging Face\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "bert_model = AutoModel.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e3v9a2ACMEVe"
   },
   "source": [
    "Define the function which will calculate the cosine similarity between two input terms using the pre-trained model and it's generated embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tx9szJC76j-N"
   },
   "outputs": [],
   "source": [
    "# Function which calculates cosine similarity using the pretrained model for two inputs terms\n",
    "def calculate_cosine_similarity_bert(term1, term2):\n",
    "    # Tokenize the terms and obtain BERT embeddings\n",
    "    inputs_tokens1 = tokenizer(term1.lower(), return_tensors=\"pt\", truncation=TRUNCATE, padding='max_length', max_length=MAX_LENGTH)\n",
    "    inputs_tokens2 = tokenizer(term2.lower(), return_tensors=\"pt\", truncation=TRUNCATE, padding='max_length', max_length=MAX_LENGTH)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Obtain embeddings for tokenised terms in last hidden state\n",
    "        embeddings_term1 = bert_model(**inputs_tokens1)['last_hidden_state']\n",
    "        embeddings_term2 = bert_model(**inputs_tokens2)['last_hidden_state']\n",
    "\n",
    "    # Average all the token embeddings to get a single embedding for the whole term\n",
    "    avg_term1 = embeddings_term1.mean(dim=1).squeeze().numpy()\n",
    "    avg_term2 = embeddings_term2.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "    # Return cosine similarity\n",
    "    return cosine_similarity([avg_term1], [avg_term2])[0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JrVDVq_GrR-z"
   },
   "source": [
    "## Evaluation Functions\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8fy5n3K7EvXN"
   },
   "outputs": [],
   "source": [
    "# Function which reads pair words in evaluation dataset, then calculates\n",
    "# cosine similarity between them using the correct model, and outputs the results to a .csv file\n",
    "def evaluate(method, dataset, test=False):\n",
    "    # Evaluation for task 1\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Create new dataframe from ID\n",
    "    output = pd.DataFrame(dataset[0])\n",
    "\n",
    "    # Create similarities column initialised with 0.5 column\n",
    "    output['similarity'] = 0.5\n",
    "\n",
    "    # Iterate through the dataset and calculate the cosine similarity between word pairs\n",
    "    for index, row in dataset.iterrows():\n",
    "        # Get the word pairs\n",
    "        term1, term2 = row[1], row[2]\n",
    "\n",
    "        # Choose the model\n",
    "        if method == \"b\":\n",
    "            similarity = calculate_cosine_similarity_word2vec(term1, term2)\n",
    "        else:\n",
    "            similarity = calculate_cosine_similarity_bert(term1, term2)\n",
    "\n",
    "        # Store similarity in the output\n",
    "        output.loc[index, 'similarity'] = similarity\n",
    "\n",
    "    # Create path name and .csv file\n",
    "    if test:\n",
    "        output_path = f\"{OUTPUT_FOLDER}/{USERNAME}-Task1-method-{method}.csv\"\n",
    "    else:\n",
    "        output_path = f\"{OUTPUT_FOLDER}/{USERNAME}-Task1-method-{method}-validation.csv\"\n",
    "    output.to_csv(output_path, header=False, index=False)\n",
    "\n",
    "    print(f\"Time elapsed to evaluate model: {measure_time(start_time)} seconds\")\n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bbtyUzK48idd"
   },
   "source": [
    "## Evaluation on Validation Set\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5kyznLrWOFji"
   },
   "source": [
    "Evaluation and accuracy result for 'Method B - **Word2vec** model' on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZsthX0UUyDfO",
    "outputId": "5da62d59-0b18-47ed-8bf7-cf981c42e049"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed to evaluate model: 0.09208345413208008 seconds\n",
      "Accuracy: 0.6555555555555556\n"
     ]
    }
   ],
   "source": [
    "# Calculate cosine similarities for validation set and run marking script\n",
    "output_path_b = evaluate(method=\"b\", dataset=validation_set)\n",
    "!python task1_eval_script_student_version.py {output_path_b} {VALIDATION_PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TRjrCfULOUTi"
   },
   "source": [
    "Evaluation and accuracy result for 'Method C - Pre-trained **BERT** model' on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9gtG84MZMx6r",
    "outputId": "bb34ca5e-c041-41d6-a986-871f358917d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed to evaluate model: 554.2399432659149 seconds\n",
      "Accuracy: 0.6222222222222222\n"
     ]
    }
   ],
   "source": [
    "# Calculate cosine similarities for validation set and run marking script\n",
    "output_path_c = evaluate(method=\"c\", dataset=validation_set)\n",
    "!python task1_eval_script_student_version.py {output_path_c} {VALIDATION_PATH}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nOdHnZ4C80zu"
   },
   "source": [
    "## Evaluation on Test Set\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x_KYwBnX1rOp"
   },
   "source": [
    "Evaluation and accuracy result for 'Method B - **Word2vec** model' on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9bwC_FtD82wn",
    "outputId": "1b7b83f7-d626-4f48-d287-90ad449e30dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed to evaluate model: 0.06305837631225586 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'data/10895316-Task1-method-b.csv'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate cosine similarities for test set\n",
    "evaluate(method=\"b\", dataset=test_set, test=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XmUJVNFW1uFi"
   },
   "source": [
    "Evaluation and accuracy result for 'Method C - **Pre-trained** model' on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yvLDlA0j82y-",
    "outputId": "6c3a3041-e28b-41c5-dd1a-e6a25f5eaf64"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed to evaluate model: 375.5004234313965 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'data/10895316-Task1-method-c.csv'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate cosine similarities for test set\n",
    "evaluate(method=\"c\", dataset=test_set, test=True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "BYXmuSBSrV8M"
   ],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
