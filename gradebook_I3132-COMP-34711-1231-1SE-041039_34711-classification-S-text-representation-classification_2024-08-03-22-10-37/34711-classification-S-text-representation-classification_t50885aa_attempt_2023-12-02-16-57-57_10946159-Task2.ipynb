{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYG0TFT26BRa"
      },
      "source": [
        "# **Task 2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AL0OcCYO6ra8"
      },
      "source": [
        "Method a\n",
        "\n",
        "**NaiÌˆve Bayes classifier**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "id": "5FLOApzkParh"
      },
      "outputs": [],
      "source": [
        "# importing libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import word_tokenize\n",
        "from sklearn.metrics import accuracy_score, classification_report, multilabel_confusion_matrix\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "import numpy as np\n",
        "import nltk\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qL0KTe-bRYd4",
        "outputId": "c53fc430-d8e4-4798-fafa-d82c938da782"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 132,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "id": "3_KuHapFRig-"
      },
      "outputs": [],
      "source": [
        "#function to get tag for words\n",
        "def tagfunc(nltk_tag):\n",
        "  if nltk_tag.startswith('J'):\n",
        "    return wordnet.ADJ\n",
        "  elif nltk_tag.startswith('V'):\n",
        "    return wordnet.VERB\n",
        "  elif nltk_tag.startswith('N'):\n",
        "    return wordnet.NOUN\n",
        "  elif nltk_tag.startswith('R'):\n",
        "    return wordnet.ADV\n",
        "  return None\n",
        "\n",
        "# function  for lemmitization\n",
        "def lemfunc(w):\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  tag=nltk.pos_tag([w])[0]\n",
        "  wordnet_tagged = [tag[0], tagfunc(tag[1])]\n",
        "  desc=wordnet_tagged[1]\n",
        "  if desc != None:\n",
        "    return (lemmatizer.lemmatize(w,desc))\n",
        "  else:\n",
        "    return (lemmatizer.lemmatize(w))\n",
        "\n",
        "# tokenization\n",
        "def tokenfunc(i):\n",
        "  t = nltk.word_tokenize(i)\n",
        "  return t\n",
        "\n",
        "#function for filtering\n",
        "def filteringfunc(w, stop_words):\n",
        "  ft = []\n",
        "  for word in w:\n",
        "    if word.isalpha() and word.lower() not in stop_words:\n",
        "      ft.append(lemfunc(word))\n",
        "  return ft\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# stop words\n",
        "custom_stopwords = {'movie', 'film', 'story', 'character', 'cinema'}\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stop_words.update(custom_stopwords)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "id": "v678LQH-NMAC"
      },
      "outputs": [],
      "source": [
        "# preprocessing training dataset\n",
        "cfp = './data/Training-dataset.csv'\n",
        "filed = pd.read_csv(cfp)\n",
        "data = filed['plot_synopsis'].tolist()\n",
        "final = []\n",
        "for i in data:\n",
        "  final.append(filteringfunc(tokenfunc(i), stop_words))\n",
        "filed['_processed'] = [' '.join(tokens) for tokens in final]\n",
        "trainingX = filed['_processed']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "id": "457yLAkJNWf5"
      },
      "outputs": [],
      "source": [
        "# preprocessing validation dataset\n",
        "cfp = './data/Task-2-validation-dataset.csv'\n",
        "filedVal = pd.read_csv(cfp)\n",
        "data = filedVal['plot_synopsis'].tolist()\n",
        "final = []\n",
        "for i in data:\n",
        "  final.append(filteringfunc(tokenfunc(i), stop_words))\n",
        "filedVal['_processed'] = [' '.join(tokens) for tokens in final]\n",
        "validationX = filedVal['_processed']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "id": "OQ4_DjP_NMJP"
      },
      "outputs": [],
      "source": [
        "# preprocessing test dataset\n",
        "cfp = './data/Task-2-test-dataset2.csv'\n",
        "filedTest = pd.read_csv(cfp)\n",
        "data = filedTest['plot_synopsis'].tolist()\n",
        "final = []\n",
        "for i in data:\n",
        "  final.append(filteringfunc(tokenfunc(i), stop_words))\n",
        "filedTest['_processed'] = [' '.join(tokens) for tokens in final]\n",
        "TestX = filedTest['_processed']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "id": "2bxJ57JqZlBa"
      },
      "outputs": [],
      "source": [
        "\n",
        "st = time.time()\n",
        "ll = MultiLabelBinarizer()\n",
        "ll.fit(filed[2:])\n",
        "trainingY = ll.transform(filed[2:])\n",
        "trainingY = filed.drop(['ID', 'title', 'plot_synopsis', '_processed'], axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {
        "id": "7VpIhgyScVWu"
      },
      "outputs": [],
      "source": [
        "\n",
        "v = CountVectorizer(max_df=0.75, min_df=0.01, ngram_range=(1, 3))\n",
        "c = OneVsRestClassifier(MultinomialNB(alpha=0.0001))\n",
        "#creating the pipeline\n",
        "pipeline = make_pipeline(v, c)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kIvA3msFcWAn",
        "outputId": "e68520be-04c5-43d3-b9dd-5de08a1b3fb7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "12.2162766456604\n"
          ]
        }
      ],
      "source": [
        "pipeline.fit(trainingX, trainingY)\n",
        "et=time.time()\n",
        "print(et-st)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xie8ajfhcWG6",
        "outputId": "a4b1ba73-fbcd-4c8a-afe8-74582f686099"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dimensionality of vectors: 4731\n"
          ]
        }
      ],
      "source": [
        "# Dimentionality of vectors\n",
        "print(\"Dimensionality of vectors:\", len(pipeline.named_steps['countvectorizer'].get_feature_names_out()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "id": "gYweNC-gxo9u"
      },
      "outputs": [],
      "source": [
        "# Function to calculate predicted labels\n",
        "def calculate_predicted_labels(pipeline, X):\n",
        "    st = time.time()\n",
        "    pred_lab = (pipeline.predict_proba(X) >= 0.84).astype(int)\n",
        "    et = time.time()\n",
        "    print(et-st)\n",
        "    return pred_lab\n",
        "\n",
        "# Function to create predicted labels dataframe\n",
        "def create_predicted_labels_dataframe(predicted_labels, df, y):\n",
        "    predicted_labels_df = pd.DataFrame(predicted_labels, columns = y.columns)\n",
        "    predicted_labels_df['ID'] = df['ID'].values\n",
        "    predicted_labels_df = predicted_labels_df[['ID'] + [col for col in predicted_labels_df.columns if col != 'ID']]\n",
        "    return predicted_labels_df\n",
        "\n",
        "# Function to save predicted labels to csv file\n",
        "def save_predicted_labels_to_csv(predicted_labels_df, output_path):\n",
        "    predicted_labels_df.to_csv(output_path, header=False, index=False)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rm4B5kehyCkN",
        "outputId": "097fd172-802e-4acb-ef89-939b63eda106"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.9269180297851562\n",
            "[[0 0 0 ... 1 0 0]\n",
            " [0 0 0 ... 0 0 1]\n",
            " [0 0 0 ... 1 0 0]\n",
            " ...\n",
            " [0 1 0 ... 0 0 1]\n",
            " [1 0 1 ... 1 0 0]\n",
            " [1 0 0 ... 0 0 1]]\n"
          ]
        }
      ],
      "source": [
        "# Applying on validation data\n",
        "pred_lab = calculate_predicted_labels(pipeline, validationX)\n",
        "print(pred_lab)\n",
        "predicted_labels_df = create_predicted_labels_dataframe(pred_lab, filedVal, trainingY)\n",
        "op = './data/10946159-Task2-method-a-validation.csv'\n",
        "save_predicted_labels_to_csv(predicted_labels_df,op)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TugIVaE8273X",
        "outputId": "aadb4f36-2352-4f5d-fd0b-84d9a00ac788"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Class level: \n",
            "Class  1 precision: 0.2843 recall: 0.5086\n",
            "Class  2 precision: 0.4244 recall: 0.5911\n",
            "Class  3 precision: 0.4221 recall: 0.4422\n",
            "Class  4 precision: 0.1184 recall: 0.3750\n",
            "Class  5 precision: 0.7261 recall: 0.6936\n",
            "Class  6 precision: 0.3390 recall: 0.5021\n",
            "Class  7 precision: 0.5239 recall: 0.7172\n",
            "Class  8 precision: 0.1279 recall: 0.3548\n",
            "Class  9 precision: 0.5867 recall: 0.7167\n",
            "----------------------------\n",
            "Movie (document) level: \n",
            "Precision: 0.4995\n",
            "Recall: 0.6282\n"
          ]
        }
      ],
      "source": [
        "#!python3 '/content/task2_eval_script_student_version.py' '10946159-Task2-method-a-validation.csv' 'Task-2-validation-dataset.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mFL3YNuNyCSK",
        "outputId": "3d16afea-7ea0-4fe4-b5d9-5517e6e4332d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.9017114639282227\n",
            "[[0 0 0 ... 1 0 0]\n",
            " [0 0 0 ... 1 0 0]\n",
            " [1 1 1 ... 1 0 0]\n",
            " ...\n",
            " [0 1 0 ... 0 0 0]\n",
            " [0 1 1 ... 0 0 0]\n",
            " [1 0 0 ... 0 0 0]]\n"
          ]
        }
      ],
      "source": [
        "# Applying on test data\n",
        "predi_lab = calculate_predicted_labels(pipeline, TestX)\n",
        "print(predi_lab)\n",
        "predicted_labels_df = create_predicted_labels_dataframe(predi_lab, filedTest, trainingY)\n",
        "op = './data/10946159-Task2-method-a.csv'\n",
        "save_predicted_labels_to_csv(predicted_labels_df, op)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
