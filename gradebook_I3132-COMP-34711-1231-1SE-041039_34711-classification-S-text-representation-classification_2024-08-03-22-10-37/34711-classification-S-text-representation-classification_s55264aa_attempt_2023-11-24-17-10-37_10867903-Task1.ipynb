{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1"
      ],
      "metadata": {
        "id": "Cqgusnuwt5Jc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "jE_Mi8kkvsPl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gensim\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ml_GVEHIvuva",
        "outputId": "6103b237-a13c-49c6-e257-7e09274f1de1"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## a) Sparse Represntation BoW with tf*idf"
      ],
      "metadata": {
        "id": "rxiPoJ9bt7qy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TFIDFSimilarityCalculator Class"
      ],
      "metadata": {
        "id": "Wv6wv_Qbven4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CosineSimilarityTFIDF:\n",
        "  \"\"\"\n",
        "  A class for calculating cosine similarity using TF-IDF representation.\n",
        "\n",
        "  This class reads training and validation datasets, preprocesses the training data, and provides methods\n",
        "  for calculating cosine similarity between terms based on their TF-IDF vectors.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, training_data_path, validation_data_path):\n",
        "    \"\"\"\n",
        "    Initializes the CosineSimilarityTFIDF class.\n",
        "\n",
        "    The constructor reads training and validation datasets from CSV files, preprocesses the training data,\n",
        "    and calculates the TF-IDF matrix, feature names, and TF-IDF array.\n",
        "\n",
        "    input parameters:\n",
        "    training_data_path - str\n",
        "        The file path to the CSV file containing the training dataset.\n",
        "    validation_data_path - str\n",
        "        The file path to the CSV file containing the validation dataset.\n",
        "    \"\"\"\n",
        "    self.training_data = pd.DataFrame(pd.read_csv(training_data_path))\n",
        "    self.validation_data = pd.read_csv(validation_data_path, header=None, names=['index', 'term1', 'term2', 'goldscore'])\n",
        "\n",
        "    self.processed_data = [self._preprocessing(synopsis) for synopsis in self.training_data['plot_synopsis']]\n",
        "    self.tfidf_matrix, self.feature_names, self.tfidf_vectorizer= self._calculate_tfidf()\n",
        "\n",
        "  def _preprocessing(self, raw):\n",
        "    \"\"\"\n",
        "    The function takes in a string and tokenises it.\n",
        "    Next it applies a lemmatizer from the import WordNetLemmatizer.\n",
        "    Given each tokenised word, it checks whether the word only contains alphabetical letters as well as making it lower case to prevent duplicates.\n",
        "    A list of the preprocessed tokens is returned ready to be used for training.\n",
        "\n",
        "    input paramters:\n",
        "    raw - string\n",
        "\n",
        "    output:\n",
        "    preprocessed_string - string\n",
        "\n",
        "    \"\"\"\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = word_tokenize(raw)\n",
        "    tokens = [lemmatizer.lemmatize(t.lower()) for t in tokens if t.isalpha()]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "  def _calculate_tfidf(self):\n",
        "    \"\"\"\n",
        "    The function initializes a TF-IDF vectorizer from the scikit-learn library.\n",
        "    It then fits and transforms the preprocessed data using the vectorizer to obtain a TF-IDF matrix.\n",
        "    Feature names are extracted from the vectorizer, and the TF-IDF matrix is converted to a NumPy array.\n",
        "\n",
        "    output:\n",
        "    tfidf_matrix - scipy.sparse.csr_matrix\n",
        "        The TF-IDF matrix representing the document-term matrix.\n",
        "    feature_names - numpy.ndarray\n",
        "        An array containing the feature names (unique words) corresponding to the columns of the TF-IDF matrix.\n",
        "\n",
        "    \"\"\"\n",
        "    tfidf_vectorizer = TfidfVectorizer(ngram_range=(1,3))\n",
        "    tfidf_matrix = tfidf_vectorizer.fit_transform((self.processed_data))\n",
        "    feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "    return tfidf_matrix, feature_names, tfidf_vectorizer\n",
        "\n",
        "  def tfidf_sim(self, term1, term2):\n",
        "    \"\"\"\n",
        "    This method first preprocesses both terms and checks if they exist in the TF-IDF vectorizer's vocabulary.\n",
        "    Next, it retrieves the indices of the terms in the TF-IDF matrix.\n",
        "    Then the function extracts the TF-IDF vectors for the two terms.\n",
        "    After that, it calculates the cosine similarity between the vectors.\n",
        "\n",
        "    If either term is not in the vocabulary, the similarity is set to 0.\n",
        "\n",
        "    input parameters:\n",
        "    term1 - str\n",
        "        The first term for cosine similarity calculation.\n",
        "    term2 - str\n",
        "        The second term for cosine similarity calculation.\n",
        "\n",
        "    output:\n",
        "    similarity - float\n",
        "        The cosine similarity between the TF-IDF representations of the two terms.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    idx_term1 = self.tfidf_vectorizer.vocabulary_.get(term1)\n",
        "    idx_term2 = self.tfidf_vectorizer.vocabulary_.get(term2)\n",
        "\n",
        "    # If both terms are in the vocabulary, calculating cosine similarity\n",
        "    if idx_term1 is not None and idx_term2 is not None:\n",
        "        # Extracting the TF-IDF vectors for the two terms\n",
        "        vector1 = self.tfidf_matrix.getcol(idx_term1)\n",
        "        vector2 = self.tfidf_matrix.getcol(idx_term2)\n",
        "\n",
        "\n",
        "        similarity = cosine_similarity(vector1.T, vector2.T)[0][0]\n",
        "    else:\n",
        "        # If either term is not in the vocabulary, return 0\n",
        "        similarity = 0\n",
        "\n",
        "    return similarity\n",
        "\n",
        "\n",
        "  def calculate_cosine_similarity_validation_tfidf(self, row):\n",
        "    \"\"\"\n",
        "    The function takes a DataFrame row containing 'term1' and 'term2', and computes the cosine similarity\n",
        "    between the two terms using the tf*idf method.\n",
        "\n",
        "    input parameters:\n",
        "    row - pandas.Series\n",
        "        A row from the validation dataset DataFrame containing 'term1' and 'term2' columns.\n",
        "\n",
        "    output:\n",
        "    similarity_result - float\n",
        "        The cosine similarity between the TF-IDF representations of 'term1' and 'term2'.\n",
        "    \"\"\"\n",
        "    pair = (row['term1'], row['term2'])\n",
        "    similarity_result = self.tfidf_sim(row['term1'], row['term2'])\n",
        "    return similarity_result\n",
        "\n",
        "  def run_validation_tfidf(self, output_path):\n",
        "    \"\"\"\n",
        "    The function applies the 'calculate_cosine_similarity_validation_tfidf' method to each row in the validation dataset.\n",
        "    Then it creates a DataFrame with the results and saves it to a csv file to the output path.\n",
        "\n",
        "    input parameters:\n",
        "    output_path - str\n",
        "        The file path where the TF-IDF cosine similarity results will be saved in CSV format.\n",
        "    \"\"\"\n",
        "    self.validation_data['cosine_result_tfidf'] = self.validation_data.apply(self.calculate_cosine_similarity_validation_tfidf, axis=1)\n",
        "    output_data_tfidf = self.validation_data[['index', 'cosine_result_tfidf']]\n",
        "    output_data_tfidf.to_csv(output_path, index=False, header=False)"
      ],
      "metadata": {
        "id": "pvDPgOBzuO28"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create The Class\n",
        "This will do the preprocessing and sets up for running validation"
      ],
      "metadata": {
        "id": "F6Q8jmPTuTbQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "calculator_tfidf_validation = CosineSimilarityTFIDF('/content/drive/MyDrive/data/Training-dataset.csv',\n",
        "                                       '/content/drive/MyDrive/data/Task-1-validation-dataset.csv')"
      ],
      "metadata": {
        "id": "WTer7NJquT42"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This will do the preprocessing and sets up for running test"
      ],
      "metadata": {
        "id": "Iz4Q9fw-HktX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "calculator_tfidf_test = CosineSimilarityTFIDF('/content/drive/MyDrive/data/Training-dataset.csv',\n",
        "                                       '/content/drive/MyDrive/data/Task-1-test-dataset1.csv')"
      ],
      "metadata": {
        "id": "Q3HiKcjqHoJr"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(calculator_tfidf_test.feature_names))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EPhTQhRw1kR_",
        "outputId": "6569f0e3-3cf3-4df0-f771-3dd8ebb61d74"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6175856\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Caluclate cosine Similarity\n",
        "validation:"
      ],
      "metadata": {
        "id": "fRsTjUWjuVcn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "calculator_tfidf_validation.run_validation_tfidf('/content/drive/MyDrive/data/10867903-Task1-method-a-validation.csv')"
      ],
      "metadata": {
        "id": "F7a78wMkuXGF"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "test:"
      ],
      "metadata": {
        "id": "8s3FMNjUIARa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "calculator_tfidf_test.run_validation_tfidf('/content/drive/MyDrive/data/10867903-Task1-method-a.csv')"
      ],
      "metadata": {
        "id": "RKd88Vz7IBoQ"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run Validation\n",
        "Check the accuracy of the model"
      ],
      "metadata": {
        "id": "1mpg8OBtua8L"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4d81T12lKjVl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## b) Dense Static Represntation Word2Vec"
      ],
      "metadata": {
        "id": "03-jueRDunhz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word2VecSimilarityCalculator Class"
      ],
      "metadata": {
        "id": "20vqBihxwAGF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Word2VecSimilarityCalculator:\n",
        "  def __init__(self, training_data_path, validation_data_path):\n",
        "    \"\"\"\n",
        "    Initializes the Word2VecSimilarityCalculator class.\n",
        "\n",
        "    The constructor reads training and validation datasets from CSV files, preprocesses the training data,\n",
        "    and calculates the word2vec model.\n",
        "\n",
        "    input parameters:\n",
        "    training_data_path - str\n",
        "        The file path to the CSV file containing the training dataset.\n",
        "    validation_data_path - str\n",
        "        The file path to the CSV file containing the validation dataset.\n",
        "    \"\"\"\n",
        "    self.training_data = pd.DataFrame(pd.read_csv(training_data_path))\n",
        "    self.validation_data = pd.read_csv(validation_data_path, header=None, names=['index', 'term1', 'term2', 'goldscore'])\n",
        "\n",
        "    self.processed_data = [self._preprocessing(synopsis) for synopsis in self.training_data['plot_synopsis']]\n",
        "    self.tokenized_data = [word_tokenize(doc) for doc in self.processed_data]\n",
        "    self.model = self._train_word2vec_model(self.tokenized_data)\n",
        "\n",
        "  def _preprocessing(self, raw):\n",
        "    \"\"\"\n",
        "    The function takes in a string and tokenises it.\n",
        "    Next it applies a lemmatizer from the import WordNetLemmatizer.\n",
        "    Given each tokenised word, it checks whether the word only contains alphabetical letters as well as making it lower case to prevent duplicates.\n",
        "    A list of the preprocessed tokens is returned ready to be used for training.\n",
        "\n",
        "    input paramters:\n",
        "    raw - string\n",
        "\n",
        "    output:\n",
        "    preprocessed_string - string\n",
        "\n",
        "    \"\"\"\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = word_tokenize(raw)\n",
        "    tokens = [lemmatizer.lemmatize(t.lower()) for t in tokens if t.isalpha()]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "  def _train_word2vec_model(self, tokenized_data):\n",
        "    \"\"\"\n",
        "    The function takes in the tokenized data and retuns the trained word2vec model.\n",
        "    The hyperparaemters used are based on previously experimented values.\n",
        "\n",
        "    input parameters:\n",
        "    tokenized_data - list of lists\n",
        "        tokenized data from preprocessing\n",
        "\n",
        "    output:\n",
        "    word2vec_model - gensim.models.Word2Vec\n",
        "        The trained Word2Vec model.\n",
        "    \"\"\"\n",
        "    return gensim.models.Word2Vec(\n",
        "        sentences=tokenized_data,\n",
        "        vector_size=100,\n",
        "        window=7,\n",
        "        min_count=1,\n",
        "        sg=1,\n",
        "        epochs=6\n",
        "    )\n",
        "\n",
        "  def get_word2vec_representation(self, term):\n",
        "    \"\"\"\n",
        "    The function checks the type of the term. If it is a list, then checks for each word in the list if it is in the word2vec model, then append the representation of the word to term_vector.\n",
        "    Otherwise, if the term is a single string, then check if that word is in the word2vec model, then append the representation of the word to term_vector.\n",
        "    Finally, rturn the average of the word2vec representations in term_vector if it's not empty.\n",
        "    Otherwise, returns None.\n",
        "\n",
        "\n",
        "    input parameters:\n",
        "    term - str or list\n",
        "        If it's a string, it represents a single-word term or a preprocessed term.\n",
        "        If it's a list, it represents a multi-word term, and the function retrieves\n",
        "\n",
        "    output:\n",
        "    term_vector - numpy.ndarray or None\n",
        "        The Word2Vec representation of the term. If the term is not present\n",
        "        in the Word2Vec model vocabulary, returns None.\n",
        "\n",
        "    \"\"\"\n",
        "    term_vector = []\n",
        "    if isinstance(term, list):\n",
        "        # Multi-word term\n",
        "        for word in term:\n",
        "            if word in self.model.wv:\n",
        "                term_vector.append(self.model.wv[word])\n",
        "    elif isinstance(term, str):\n",
        "        # Single-word term or preprocessed term\n",
        "        tokens = word_tokenize(term)\n",
        "        for word in tokens:\n",
        "            if word in self.model.wv:\n",
        "                term_vector.append(self.model.wv[word])\n",
        "\n",
        "    return sum(term_vector) / len(term_vector) if term_vector else None\n",
        "\n",
        "  def calculate_cosine_similarity(self, pair):\n",
        "    \"\"\"\n",
        "    This function takes a pair of terms, preprocesses and tokenizes them, and obtains their word2vec\n",
        "    representations using the `get_word2vec_representation` method.\n",
        "    Then it calculates the cosine similarity between the two term vectors using the `cosine_similarity` function.\n",
        "\n",
        "    input parameters:\n",
        "    pair - tuple\n",
        "        A pair of terms for which the cosine similarity is to be calculated.\n",
        "\n",
        "    output:\n",
        "    cosine_similarity - float\n",
        "        The cosine similarity between the two terms.\n",
        "    \"\"\"\n",
        "    term1, term2 = pair\n",
        "    term1_vector = self.get_word2vec_representation(word_tokenize(self._preprocessing(term1)))\n",
        "    term2_vector = self.get_word2vec_representation(word_tokenize(self._preprocessing(term2)))\n",
        "    if term1_vector is not None and term2_vector is not None:\n",
        "        similarity = cosine_similarity([term1_vector], [term2_vector])[0][0]\n",
        "        return similarity\n",
        "    else:\n",
        "        return 0.0\n",
        "\n",
        "  def calculate_cosine_similarity_validation_word2vec(self, row):\n",
        "    \"\"\"\n",
        "    The function takes a DataFrame row containing 'term1' and 'term2', and computes the cosine similarity\n",
        "    between the two terms using the word2vec method.\n",
        "\n",
        "    input parameters:\n",
        "    row - pandas.Series\n",
        "        A row from the validation dataset DataFrame containing 'term1' and 'term2' columns.\n",
        "\n",
        "    output:\n",
        "    similarity_result - float\n",
        "        The cosine similarity between the word2vec representations of 'term1' and 'term2'.\n",
        "    \"\"\"\n",
        "    pair = (row['term1'], row['term2'])\n",
        "    similarity_result = self.calculate_cosine_similarity(pair)\n",
        "    return similarity_result\n",
        "\n",
        "  def run_validation(self, output_path):\n",
        "    \"\"\"\n",
        "    The function applies the 'calculate_cosine_similarity_validation_word2vec' method to each row in the validation dataset.\n",
        "    Then it creates a DataFrame with the results and saves it to a csv file to the output path.\n",
        "\n",
        "    input parameters:\n",
        "    output_path - str\n",
        "        The file path where the word2vec cosine similarity results will be saved in CSV format.\n",
        "    \"\"\"\n",
        "    self.validation_data['cosine_result'] = self.validation_data.apply(self.calculate_cosine_similarity_validation_word2vec, axis=1)\n",
        "    output_data = self.validation_data[['index', 'cosine_result']]\n",
        "    output_data.to_csv(output_path, index=False, header=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "YOV7s994u-1X"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create The Class\n",
        "This will do the preprocessing and sets up for running validation"
      ],
      "metadata": {
        "id": "o8LVd7pwvDdK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "calculator_w2v_validation = Word2VecSimilarityCalculator('/content/drive/MyDrive/data/Training-dataset.csv',\n",
        "                                         '/content/drive/MyDrive/data/Task-1-validation-dataset.csv')"
      ],
      "metadata": {
        "id": "Kw2uuFoqvEAn"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This will do the preprocessing and sets up for running test"
      ],
      "metadata": {
        "id": "HTRjNL_XFosl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "calculator_w2v_test = Word2VecSimilarityCalculator('/content/drive/MyDrive/data/Training-dataset.csv',\n",
        "                                         '/content/drive/MyDrive/data/Task-1-test-dataset1.csv')"
      ],
      "metadata": {
        "id": "qRLILaENFfO_"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Caluclate cosine Similarity\n",
        "validation:"
      ],
      "metadata": {
        "id": "uZm52pjCvI21"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "calculator_w2v_validation.run_validation('/content/drive/MyDrive/data/10867903-Task1-method-b-validation.csv')"
      ],
      "metadata": {
        "id": "vlt2RtyNvLXW"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "test:"
      ],
      "metadata": {
        "id": "fjkaGZq4F5PJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "calculator_w2v_test.run_validation('/content/drive/MyDrive/data/10867903-Task1-method-b.csv')"
      ],
      "metadata": {
        "id": "MnKwgMMvF2Qz"
      },
      "execution_count": 30,
      "outputs": []
    }
  ]
}