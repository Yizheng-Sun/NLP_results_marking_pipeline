{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Task 2\n"
      ],
      "metadata": {
        "id": "HRLGt79ytKZs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "nWEzOzLUQo0d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense\n",
        "from keras.layers import Layer\n",
        "import keras.backend as K\n",
        "import numpy as np\n",
        "import nltk\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers import Bidirectional, Attention\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gNssmeTRQrhE",
        "outputId": "96a52144-6a19-4a8d-86e8-8adf40490d57"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "tX1qW_XEaTx_"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attention Layer Class"
      ],
      "metadata": {
        "id": "IGR9dyjmtRB8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionLayer(Layer):\n",
        "    \"\"\"\n",
        "    This layer computes attention weights for input sequences and performs a weighted sum of the sequence elements.\n",
        "    \"\"\"\n",
        "    def __init__(self, **kwargs):\n",
        "        super(AttentionLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.W = self.add_weight(name='attention_weight',\n",
        "                                 shape=(input_shape[-1], 1),\n",
        "                                 initializer='random_normal',\n",
        "                                 trainable=True)\n",
        "        self.b = self.add_weight(name='attention_bias',\n",
        "                                 shape=(input_shape[1], 1),\n",
        "                                 initializer='zeros',\n",
        "                                 trainable=True)\n",
        "        super(AttentionLayer, self).build(input_shape)\n",
        "\n",
        "    def call(self, x):\n",
        "        e = K.tanh(K.dot(x, self.W) + self.b)\n",
        "        a = K.softmax(e, axis=1)\n",
        "        output = x * a\n",
        "        return K.sum(output, axis=1)"
      ],
      "metadata": {
        "id": "XZ50JXCjeKVx"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  MultiLabelClassifierPlotSynopsis Class\n",
        "For preprocessing, training and predicting (for both naieve and lstm models)"
      ],
      "metadata": {
        "id": "an2qCi8ftW-_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiLabelClassifierPlotSynopsis:\n",
        "  \"\"\"\n",
        "    Multi-label Classifier for Movie Plot Synopses using both LSTM and Naive Bayes models.\n",
        "\n",
        "    This class performs preprocessing, and trainig for both LSTM and Naive Bayes models to predict the labels (genres) for a given plot synopsis.\n",
        "  \"\"\"\n",
        "  def __init__(self, training_data_path, validation_data_path):\n",
        "    \"\"\"\n",
        "    Initializes the MultiLabelClassifierPlotSynopsis class.\n",
        "\n",
        "    The constructor reads training and validation datasets from CSV files, preprocesses the training data,\n",
        "    and calculates the word2vec model.\n",
        "\n",
        "    input parameters:\n",
        "    training_data_path - str\n",
        "        The file path to the CSV file containing the training dataset.\n",
        "    validation_data_path - str\n",
        "        The file path to the CSV file containing the validation dataset.\n",
        "    \"\"\"\n",
        "    self.training_df = pd.DataFrame(pd.read_csv(training_data_path))\n",
        "    self.validation_df = pd.read_csv(validation_data_path)\n",
        "\n",
        "  def preprocess_text(self, raw):\n",
        "    \"\"\"\n",
        "    Tokenize, convert to lowercase, remove stopwords, and lemmatize the input text.\n",
        "\n",
        "    input paramters:\n",
        "    raw - string\n",
        "\n",
        "    output:\n",
        "    preprocessed_string - string\n",
        "\n",
        "    \"\"\"\n",
        "    tokens = word_tokenize(raw)\n",
        "    tokens = [token.lower() for token in tokens if token.isalpha()]\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    custom_stopwords = {'movie', 'film', 'story', 'character'}\n",
        "    stop_words.update(custom_stopwords)\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "  def preprocess_data(self):\n",
        "    \"\"\"\n",
        "      Apply text preprocessing to the 'plot_synopsis' column in both training and validation datasets.\n",
        "    \"\"\"\n",
        "    self.training_df['processed_text'] = self.training_df['plot_synopsis'].apply(self.preprocess_text)\n",
        "    self.validation_df['processed_text'] = self.validation_df['plot_synopsis'].apply(self.preprocess_text)\n",
        "\n",
        "  def tokenize_and_pad(self, max_sequence_length):\n",
        "    \"\"\"\n",
        "      Tokenize and pad the preprocessed text data for input to the LSTM model.\n",
        "\n",
        "      input paramters:\n",
        "      max_sequence_length - int\n",
        "          Maximum sequence length for padding.\n",
        "    \"\"\"\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(self.training_df['processed_text'])\n",
        "    X_train_seq = tokenizer.texts_to_sequences(self.training_df['processed_text'])\n",
        "    X_val_seq = tokenizer.texts_to_sequences(self.validation_df['processed_text'])\n",
        "    self.input_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "    self.X_train_pad = pad_sequences(X_train_seq, maxlen=max_sequence_length)\n",
        "    self.X_val_pad = pad_sequences(X_val_seq, maxlen=max_sequence_length)\n",
        "\n",
        "  def build_lstm_model(self, max_sequence_length):\n",
        "    \"\"\"\n",
        "      Build the LSTM model.\n",
        "\n",
        "      input paramters:\n",
        "      max_sequence_length - int\n",
        "          Maximum sequence length for padding.\n",
        "    \"\"\"\n",
        "    self.model = Sequential()\n",
        "    self.model.add(Embedding(input_dim=self.input_size, output_dim=100, input_length=max_sequence_length))\n",
        "    self.model.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
        "    self.model.add(AttentionLayer())\n",
        "    self.model.add(Dense(len(self.labels), activation='sigmoid'))\n",
        "    self.model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "  def train_lstm_model(self, epochs=7, batch_size=128, validation_split=0.1, patience=3):\n",
        "    \"\"\"\n",
        "      Train the LSTM model.\n",
        "      The hyper-parameters have already been experimented on prior.\n",
        "\n",
        "      input paramters:\n",
        "        epochs: int\n",
        "            Number of epochs for training.\n",
        "\n",
        "        batch_size: int\n",
        "            Batch size for training.\n",
        "\n",
        "        validation_split: float\n",
        "            Fraction of training data to be used as validation data.\n",
        "\n",
        "        patience: int\n",
        "            Number of epochs with no improvement after which training will be stopped.\n",
        "    \"\"\"\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=patience)\n",
        "    self.model.fit(self.X_train_pad, self.y_train, batch_size=batch_size, epochs=epochs, validation_split=validation_split, callbacks=[early_stopping])\n",
        "\n",
        "  def predict_labels_lstm(self, threshold=0.3):\n",
        "    \"\"\"\n",
        "      Predict labels for the validation dataset using the trained LSTM model.\n",
        "\n",
        "        input paramters:\n",
        "        threshold: float\n",
        "            Threshold for binary classification.\n",
        "\n",
        "        output:\n",
        "        lstm_predicted_labels_df - DataFrame\n",
        "            Predicted labels for the validation dataset.\n",
        "    \"\"\"\n",
        "    lstm_probabilities = self.model.predict(self.X_val_pad)\n",
        "    lstm_predicted_labels = (lstm_probabilities >= threshold).astype(int)\n",
        "\n",
        "    lstm_predicted_labels[:, 3] = (lstm_probabilities[:, 3] >= 0.05).astype(int)\n",
        "    lstm_predicted_labels[:, 7] = (lstm_probabilities[:, 7] >= 0.1).astype(int)\n",
        "\n",
        "    lstm_predicted_labels_df = pd.DataFrame(lstm_predicted_labels, columns=self.labels)\n",
        "    lstm_predicted_labels_df['ID'] = self.validation_df['ID']\n",
        "    lstm_predicted_labels_df = lstm_predicted_labels_df[['ID'] + [col for col in lstm_predicted_labels_df.columns if col != 'ID']]\n",
        "\n",
        "    return lstm_predicted_labels_df\n",
        "\n",
        "  def run_lstm(self, output_path, max_sequence_length=4000, epochs=7, batch_size=128, validation_split=0.1, patience=3, threshold=0.3):\n",
        "    \"\"\"\n",
        "        Run the entire pipeline for the LSTM model.\n",
        "        The hyper-parameters have already been experimented on prior.\n",
        "\n",
        "        input paramters:\n",
        "        output_path: str\n",
        "            File path to save the predicted labels.\n",
        "\n",
        "        max_sequence_length: int\n",
        "            Maximum sequence length for padding.\n",
        "\n",
        "        epochs: int\n",
        "            Number of epochs for training.\n",
        "\n",
        "        batch_size: int\n",
        "            Batch size for training.\n",
        "\n",
        "        validation_split: float\n",
        "            Fraction of training data to be used as validation data.\n",
        "\n",
        "        patience: int\n",
        "            Number of epochs with no improvement after which training will be stopped.\n",
        "\n",
        "        threshold: float\n",
        "            Threshold for binary classification.\n",
        "    \"\"\"\n",
        "    self.labels = self.training_df.columns[3:]\n",
        "    self.preprocess_data()\n",
        "    self.tokenize_and_pad(max_sequence_length)\n",
        "    self.y_train = self.training_df[self.labels]\n",
        "\n",
        "    self.build_lstm_model(max_sequence_length)\n",
        "    self.train_lstm_model(epochs=epochs, batch_size=batch_size, validation_split=validation_split, patience=patience)\n",
        "\n",
        "    lstm_predicted_labels_df = self.predict_labels_lstm(threshold=threshold)\n",
        "\n",
        "    lstm_predicted_labels_df.to_csv(output_path, header=False, index=False)\n",
        "\n",
        "\n",
        "  def train_naive_bayes_classifier(self):\n",
        "    \"\"\"\n",
        "    Train a Naive Bayes classifier using CountVectorizer.\n",
        "\n",
        "    Returns:\n",
        "    Pipeline: Trained pipeline containing CountVectorizer and Multinomial Naive Bayes classifier.\n",
        "    \"\"\"\n",
        "    pipeline = Pipeline([\n",
        "        ('vectorizer', CountVectorizer(max_df=0.75, min_df=0.01, ngram_range=(1, 3))),\n",
        "        ('classifier', OneVsRestClassifier(MultinomialNB(alpha=10)))\n",
        "    ])\n",
        "\n",
        "    X = self.training_df['processed_text']\n",
        "    y_columns = self.training_df.drop(['ID', 'title', 'plot_synopsis', 'processed_text'], axis=1)\n",
        "    y = y_columns\n",
        "    pipeline.fit(X, y)\n",
        "    return pipeline\n",
        "\n",
        "  def predict_naive_bayes(self, pipeline, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Predict labels for the validation dataset using the Naive Bayes classifier.\n",
        "\n",
        "    Parameters:\n",
        "    pipeline (Pipeline): Trained pipeline containing CountVectorizer and Multinomial Naive Bayes classifier.\n",
        "    threshold (float): Threshold for binary classification.\n",
        "\n",
        "    Returns:\n",
        "    DataFrame: Predicted labels for the validation dataset.\n",
        "    \"\"\"\n",
        "    X_val = self.validation_df['processed_text']\n",
        "    probabilities = pipeline.predict_proba(X_val)\n",
        "\n",
        "    predicted_labels = np.array([(probabilities >= threshold).astype(int) for prob in probabilities])\n",
        "\n",
        "    predicted_labels_df = pd.DataFrame(predicted_labels[0], columns=self.labels)\n",
        "    predicted_labels_df['ID'] = self.validation_df['ID'].values\n",
        "    predicted_labels_df = predicted_labels_df[['ID'] + [col for col in predicted_labels_df.columns if col != 'ID']]\n",
        "    return predicted_labels_df\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TYS2tAMgQuBT"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## validation"
      ],
      "metadata": {
        "id": "nSYgkf88OGeC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### lstm"
      ],
      "metadata": {
        "id": "ybhwmeclOKFJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_classifier = MultiLabelClassifierPlotSynopsis('/content/drive/MyDrive/data/Training-dataset.csv', '/content/drive/MyDrive/data/Task-2-validation-dataset.csv')\n",
        "lstm_classifier.run_lstm('/content/drive/MyDrive/data/10867903-Task2-method-b-validation.csv')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OpbQPV8UOMMe",
        "outputId": "832a53b8-cfda-4a24-9dea-d8e4fdc0e94b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/7\n",
            "59/59 [==============================] - 39s 492ms/step - loss: 0.4975 - accuracy: 0.2584 - val_loss: 0.4576 - val_accuracy: 0.2518\n",
            "Epoch 2/7\n",
            "59/59 [==============================] - 27s 463ms/step - loss: 0.4587 - accuracy: 0.2603 - val_loss: 0.4535 - val_accuracy: 0.2518\n",
            "Epoch 3/7\n",
            "59/59 [==============================] - 28s 476ms/step - loss: 0.4510 - accuracy: 0.2577 - val_loss: 0.4513 - val_accuracy: 0.2518\n",
            "Epoch 4/7\n",
            "59/59 [==============================] - 27s 449ms/step - loss: 0.4408 - accuracy: 0.2573 - val_loss: 0.4441 - val_accuracy: 0.2518\n",
            "Epoch 5/7\n",
            "59/59 [==============================] - 25s 431ms/step - loss: 0.4272 - accuracy: 0.2481 - val_loss: 0.4374 - val_accuracy: 0.2446\n",
            "Epoch 6/7\n",
            "59/59 [==============================] - 27s 459ms/step - loss: 0.4125 - accuracy: 0.2656 - val_loss: 0.4304 - val_accuracy: 0.2579\n",
            "Epoch 7/7\n",
            "59/59 [==============================] - 27s 461ms/step - loss: 0.4012 - accuracy: 0.2659 - val_loss: 0.4330 - val_accuracy: 0.2567\n",
            "38/38 [==============================] - 4s 82ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### naive"
      ],
      "metadata": {
        "id": "0QWyUY2dOPCi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "naive_bayes_classifier = lstm_classifier.train_naive_bayes_classifier()\n",
        "naive_bayes_predicted_labels_df = lstm_classifier.predict_naive_bayes(naive_bayes_classifier, threshold=0.5)\n",
        "naive_bayes_predicted_labels_df.to_csv('/content/drive/MyDrive/data/10867903-Task2-method-a-validation.csv', header=False, index=False)\n"
      ],
      "metadata": {
        "id": "b08mbXnKNpzN"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test\n",
        "### lstm"
      ],
      "metadata": {
        "id": "8vXygT_UJ1YT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_classifier = MultiLabelClassifierPlotSynopsis('/content/drive/MyDrive/data/Training-dataset.csv', '/content/drive/MyDrive/data/Task-2-test-dataset1.csv')\n",
        "lstm_classifier.run_lstm('/content/drive/MyDrive/data/10867903-Task2-method-b.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVomX8z3Og3y",
        "outputId": "efa24cd6-9a5e-439b-c93e-8c241470c776"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/7\n",
            "59/59 [==============================] - 31s 471ms/step - loss: 0.5017 - accuracy: 0.2316 - val_loss: 0.4574 - val_accuracy: 0.2518\n",
            "Epoch 2/7\n",
            "59/59 [==============================] - 26s 434ms/step - loss: 0.4606 - accuracy: 0.2603 - val_loss: 0.4577 - val_accuracy: 0.2518\n",
            "Epoch 3/7\n",
            "59/59 [==============================] - 26s 444ms/step - loss: 0.4573 - accuracy: 0.2603 - val_loss: 0.4525 - val_accuracy: 0.2518\n",
            "Epoch 4/7\n",
            "59/59 [==============================] - 27s 461ms/step - loss: 0.4472 - accuracy: 0.2592 - val_loss: 0.4464 - val_accuracy: 0.2482\n",
            "Epoch 5/7\n",
            "59/59 [==============================] - 25s 427ms/step - loss: 0.4362 - accuracy: 0.2531 - val_loss: 0.4459 - val_accuracy: 0.2324\n",
            "Epoch 6/7\n",
            "59/59 [==============================] - 27s 450ms/step - loss: 0.4234 - accuracy: 0.2507 - val_loss: 0.4403 - val_accuracy: 0.2409\n",
            "Epoch 7/7\n",
            "59/59 [==============================] - 26s 448ms/step - loss: 0.4130 - accuracy: 0.2473 - val_loss: 0.4338 - val_accuracy: 0.2421\n",
            "38/38 [==============================] - 4s 82ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### naiev\n"
      ],
      "metadata": {
        "id": "0uVEftgPMUNZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "naive_bayes_classifier = lstm_classifier.train_naive_bayes_classifier()\n",
        "naive_bayes_predicted_labels_df = lstm_classifier.predict_naive_bayes(naive_bayes_classifier, threshold=0.5)\n",
        "naive_bayes_predicted_labels_df.to_csv('/content/drive/MyDrive/data/10867903-Task2-method-a.csv', header=False, index=False)"
      ],
      "metadata": {
        "id": "qM1JRHTUMZkd"
      },
      "execution_count": 19,
      "outputs": []
    }
  ]
}