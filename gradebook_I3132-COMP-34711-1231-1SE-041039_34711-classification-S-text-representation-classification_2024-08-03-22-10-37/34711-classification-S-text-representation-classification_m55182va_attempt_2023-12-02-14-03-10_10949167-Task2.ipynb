{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hgn1zBebvSSI"
      },
      "source": [
        "Method 1\\\n",
        "**NaiÌˆve Bayes classifier**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "270rGXqONDAR",
        "outputId": "f57c7655-31c5-4160-c254-cd9a3a0adaa7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import word_tokenize\n",
        "from sklearn.metrics import accuracy_score, classification_report, multilabel_confusion_matrix\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "import numpy as np\n",
        "import nltk\n",
        "from sklearn.pipeline import Pipeline\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "# Load your movie dataset\n",
        "df = pd.read_csv('./data/Training-dataset.csv')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "ul_list =[]\n",
        "def get_wordnet_pos(nltk_tag):\n",
        "\n",
        "    if nltk_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif nltk_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif nltk_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif nltk_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return None\n",
        "ul_list = []\n",
        "for index, row in df.iterrows():\n",
        "  words = word_tokenize(row['plot_synopsis'])\n",
        "  Y=[]\n",
        "  for word in words:\n",
        "    if word.isalpha() and word.lower() not in stop_words:\n",
        "      t=nltk.pos_tag([word])[0]\n",
        "      wordnet_tagged = [t[0], get_wordnet_pos(t[1])]\n",
        "      d=wordnet_tagged[1]\n",
        "      if d !=None:\n",
        "        Y.append(lemmatizer.lemmatize(word,d))\n",
        "      else:\n",
        "        Y.append(lemmatizer.lemmatize(word))\n",
        "  ul_list.append(' '.join(Y))\n",
        "df['processed_text']=ul_list\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "MzrO62gWcD2U"
      },
      "outputs": [],
      "source": [
        "# Extract features and labels\n",
        "X = df['processed_text']\n",
        "y = df[['comedy', 'cult', 'flashback', 'historical', 'murder', 'revenge', 'romantic', 'scifi', 'violence']]\n",
        "dftest = pd.read_csv('./data/Task-2-validation-dataset(1).csv')\n",
        "ul_list =[]\n",
        "def get_wordnet_pos(nltk_tag):\n",
        "\n",
        "    if nltk_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif nltk_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif nltk_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif nltk_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return None\n",
        "ul_list = []\n",
        "for index, row in dftest.iterrows():\n",
        "  words = word_tokenize(row['plot_synopsis'])\n",
        "  Y=[]\n",
        "  for word in words:\n",
        "    if word.isalpha() and word.lower() not in stop_words:\n",
        "      t=nltk.pos_tag([word])[0]\n",
        "      wordnet_tagged = [t[0], get_wordnet_pos(t[1])]\n",
        "      d=wordnet_tagged[1]\n",
        "      if d !=None:\n",
        "        Y.append(lemmatizer.lemmatize(word,d))\n",
        "      else:\n",
        "        Y.append(lemmatizer.lemmatize(word))\n",
        "  ul_list.append(' '.join(Y))\n",
        "dftest['processed_text']=ul_list\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ZID3fdpDFUkf"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "stime = time.time()\n",
        "mlb1 = MultiLabelBinarizer()\n",
        "ytr = mlb1.fit_transform(df[2:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "EOTrlrBcMcG3"
      },
      "outputs": [],
      "source": [
        "# Define the pipeline\n",
        "\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "pipeline = make_pipeline(\n",
        "    CountVectorizer(max_df=0.75, min_df=0.01, ngram_range=(1, 4)),\n",
        "    OneVsRestClassifier(MultinomialNB(alpha=0.0001))\n",
        ")\n",
        "\n",
        "# Assuming df['processed_text'] contains your training data and labels\n",
        "xtr = df['processed_text']\n",
        "ytr = df.drop(['ID', 'title', 'plot_synopsis', 'processed_text'], axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DR8nc4bNjBzM",
        "outputId": "69c0d90b-76e8-490a-eb4f-9c636b2cbe0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "32.15846109390259\n"
          ]
        }
      ],
      "source": [
        "# Training the pipeline\n",
        "pipeline.fit(xtr, ytr)\n",
        "etime=time.time()\n",
        "print(etime-stime)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "7a7ZmhyvHorH"
      },
      "outputs": [],
      "source": [
        "# Assuming dftest['processed_text'] contains your test data\n",
        "X_val = dftest['processed_text']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "JVLe9kxKPw-9"
      },
      "outputs": [],
      "source": [
        "# Assuming dftest['processed_text'] contains your test data\n",
        "X_val = dftest['processed_text']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DeRZYCFpQWuF",
        "outputId": "240d8cb5-ccd9-49c7-d5b7-fb8d06b76c12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.88242244720459\n"
          ]
        }
      ],
      "source": [
        "stime = time.time()\n",
        "# Predicting probs\n",
        "probs = pipeline.predict_proba(X_val)\n",
        "\n",
        "# Apply threshold to determine labels\n",
        "threshold = 0.84\n",
        "predictedlabels = (probs >= threshold).astype(int)\n",
        "etime=time.time()\n",
        "print(etime-stime)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KHmHiXUcQag2",
        "outputId": "44cdbdfd-9f92-4241-9ff8-610883656b2e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0 0 0 0 0 0 1 0 0]\n"
          ]
        }
      ],
      "source": [
        "print(predictedlabels[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "NrfoPzjFQcwZ"
      },
      "outputs": [],
      "source": [
        "# Creating a DataFrame for predictions\n",
        "predictedlabelsdf = pd.DataFrame(predictedlabels, columns=ytr.columns)\n",
        "predictedlabelsdf['ID'] = dftest['ID'].values\n",
        "predictedlabelsdf = predictedlabelsdf[['ID'] + [col for col in predictedlabelsdf.columns if col != 'ID']]\n",
        "\n",
        "# Saving to CSV file\n",
        "predictedlabelsdf.to_csv('./data/10949167-Task2-method-a-validation.csv', header=False, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cpjHsas5Q30Q",
        "outputId": "d0e191fd-c245-4039-b78c-f8c3166bc9cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Class level: \n",
            "Class  1 precision: 0.2853 recall: 0.5086\n",
            "Class  2 precision: 0.4194 recall: 0.5789\n",
            "Class  3 precision: 0.4226 recall: 0.4456\n",
            "Class  4 precision: 0.1200 recall: 0.3750\n",
            "Class  5 precision: 0.7233 recall: 0.6885\n",
            "Class  6 precision: 0.3419 recall: 0.5063\n",
            "Class  7 precision: 0.5275 recall: 0.7276\n",
            "Class  8 precision: 0.1205 recall: 0.3226\n",
            "Class  9 precision: 0.5879 recall: 0.7167\n",
            "----------------------------\n",
            "Movie (document) level: \n",
            "Precision: 0.5001\n",
            "Recall: 0.6281\n"
          ]
        }
      ],
      "source": [
        "#!python3 '/content/task2_eval_script_student_version(1).py' /content/10949167-Task2-method-a-validation.csv '/content/Task-2-validation-dataset(1).csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYyNpK8YVR21",
        "outputId": "fa6f6c52-62d9-4c72-f0fe-1ee9f427f0a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1200\n",
            "1.855191946029663\n"
          ]
        }
      ],
      "source": [
        "# Extract features and labels\n",
        "dftest1 = pd.read_csv('./data/Task-2-test-dataset2.csv')\n",
        "ul_list =[]\n",
        "index=0\n",
        "row=0\n",
        "for index, row in dftest1.iterrows():\n",
        "  words = word_tokenize(row['plot_synopsis'])\n",
        "  Y=[]\n",
        "  for word in words:\n",
        "    if word.isalpha() and word.lower() not in stop_words:\n",
        "      t=nltk.pos_tag([word])[0]\n",
        "      wordnet_tagged = [t[0], get_wordnet_pos(t[1])]\n",
        "      d=wordnet_tagged[1]\n",
        "      if d !=None:\n",
        "        Y.append(lemmatizer.lemmatize(word,d))\n",
        "      else:\n",
        "        Y.append(lemmatizer.lemmatize(word))\n",
        "  ul_list.append(' '.join(Y))\n",
        "print(len(ul_list))\n",
        "dftest1['processed_text']=ul_list\n",
        "# Assuming dftest['processed_text'] contains your test data\n",
        "X_test = dftest1['processed_text']\n",
        "# # Predicting probs\n",
        "import time\n",
        "s_time = time.time()\n",
        "probs = pipeline.predict_proba(X_test)\n",
        "threshold = 0.84\n",
        "predictedlabels = (probs >= threshold).astype(int)\n",
        "e_time = time.time()\n",
        "print(e_time-s_time)\n",
        "# Creating a DataFrame for predictions\n",
        "predictedlabelsdf = pd.DataFrame(predictedlabels, columns=ytr.columns)\n",
        "predictedlabelsdf['ID'] = dftest1['ID'].values\n",
        "predictedlabelsdf = predictedlabelsdf[['ID'] + [col for col in predictedlabelsdf.columns if col != 'ID']]\n",
        "\n",
        "# Saving to CSV file\n",
        "predictedlabelsdf.to_csv('./data/10949167-Task2-method-a.csv', header=False, index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
