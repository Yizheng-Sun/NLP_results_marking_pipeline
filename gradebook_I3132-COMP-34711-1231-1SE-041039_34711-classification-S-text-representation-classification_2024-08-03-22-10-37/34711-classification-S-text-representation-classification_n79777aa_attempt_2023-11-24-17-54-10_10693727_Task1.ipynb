{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Natural Language Processing CW - Task 1: Distributional Semantics\n"
      ],
      "metadata": {
        "id": "ZA51HIUtHfrc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before starting, we will load the datasets, import relevant libraries, etc. These will be used across the whole notebook."
      ],
      "metadata": {
        "id": "cAL-AO-ekpdq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing the necessary libraries:"
      ],
      "metadata": {
        "id": "VafoSvdHasT_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from gensim.models import Word2Vec\n",
        "import numpy as np\n",
        "from gensim.models import Phrases\n",
        "import string\n",
        "import nltk\n",
        "import time\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "_a1eJO5taxHb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c03fd2ee-fde6-48f7-b1e2-33929f1aaabc"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading the training dataset:"
      ],
      "metadata": {
        "id": "6-QfMqyPbI_6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = pd.read_csv('./data/Training-dataset.csv')"
      ],
      "metadata": {
        "id": "25C6YQ-vbMD0"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the validation test dataset:\n"
      ],
      "metadata": {
        "id": "_p_OIqS4FhF4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load test data\n",
        "val_data = pd.read_csv('./data/Task-1-validation-dataset.csv', names=['id', 'term1', 'term2', 'similarity'])"
      ],
      "metadata": {
        "id": "51WQ5P0kThWh"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before starting, it is good to visualize the dataframe to understand the kind of data we are working with."
      ],
      "metadata": {
        "id": "XDaCfWMYPjnV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        },
        "id": "FN57xKI1PVn_",
        "outputId": "c5888108-34bf-471a-a4a5-51877e6c9640"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                     ID                        title  \\\n",
              "0  8f5203de-b2f8-4c0c-b0c1-835ba92422e9                   Si wang ta   \n",
              "1  6416fe15-6f8a-41d4-8a78-3e8f120781c7          Shattered Vengeance   \n",
              "2  4979fe9a-0518-41cc-b85f-f364c91053ca                 L'esorciccio   \n",
              "3  b672850b-a1d9-44ed-9cff-025ee8b61e6f  Serendipity Through Seasons   \n",
              "4  b4d8e8cc-a53e-48f8-be6a-6432b928a56d                The Liability   \n",
              "\n",
              "                                       plot_synopsis  comedy  cult  flashback  \\\n",
              "0  After a recent amount of challenges, Billy Lo ...       0     0          0   \n",
              "1  In the crime-ridden city of Tremont, renowned ...       0     0          0   \n",
              "2  Lankester Merrin is a veteran Catholic priest ...       0     1          0   \n",
              "3  \"Serendipity Through Seasons\" is a heartwarmin...       0     0          0   \n",
              "4  Young and naive 19-year-old slacker, Adam (Jac...       0     0          1   \n",
              "\n",
              "   historical  murder  revenge  romantic  scifi  violence  \n",
              "0           0       1        1         0      0         1  \n",
              "1           0       1        1         1      0         1  \n",
              "2           0       0        0         0      0         0  \n",
              "3           0       0        0         1      0         0  \n",
              "4           0       0        0         0      0         0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c528e6da-756d-4255-9a85-9e6cea31b527\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>title</th>\n",
              "      <th>plot_synopsis</th>\n",
              "      <th>comedy</th>\n",
              "      <th>cult</th>\n",
              "      <th>flashback</th>\n",
              "      <th>historical</th>\n",
              "      <th>murder</th>\n",
              "      <th>revenge</th>\n",
              "      <th>romantic</th>\n",
              "      <th>scifi</th>\n",
              "      <th>violence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>8f5203de-b2f8-4c0c-b0c1-835ba92422e9</td>\n",
              "      <td>Si wang ta</td>\n",
              "      <td>After a recent amount of challenges, Billy Lo ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>6416fe15-6f8a-41d4-8a78-3e8f120781c7</td>\n",
              "      <td>Shattered Vengeance</td>\n",
              "      <td>In the crime-ridden city of Tremont, renowned ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4979fe9a-0518-41cc-b85f-f364c91053ca</td>\n",
              "      <td>L'esorciccio</td>\n",
              "      <td>Lankester Merrin is a veteran Catholic priest ...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>b672850b-a1d9-44ed-9cff-025ee8b61e6f</td>\n",
              "      <td>Serendipity Through Seasons</td>\n",
              "      <td>\"Serendipity Through Seasons\" is a heartwarmin...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>b4d8e8cc-a53e-48f8-be6a-6432b928a56d</td>\n",
              "      <td>The Liability</td>\n",
              "      <td>Young and naive 19-year-old slacker, Adam (Jac...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c528e6da-756d-4255-9a85-9e6cea31b527')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c528e6da-756d-4255-9a85-9e6cea31b527 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c528e6da-756d-4255-9a85-9e6cea31b527');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-6baa33be-ec54-4c66-b75b-e8746581cc75\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6baa33be-ec54-4c66-b75b-e8746581cc75')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-6baa33be-ec54-4c66-b75b-e8746581cc75 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data['plot_synopsis']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hRkdG0LFPcfV",
        "outputId": "cd5dfb32-0083-4a48-d995-8aa25dd5a1e5"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       After a recent amount of challenges, Billy Lo ...\n",
              "1       In the crime-ridden city of Tremont, renowned ...\n",
              "2       Lankester Merrin is a veteran Catholic priest ...\n",
              "3       \"Serendipity Through Seasons\" is a heartwarmin...\n",
              "4       Young and naive 19-year-old slacker, Adam (Jac...\n",
              "                              ...                        \n",
              "8252    After serving an eight month sentence for brea...\n",
              "8253    The Mystery Inc. crew head to Chicago for a ta...\n",
              "8254    Through its run, Another Life revolved around ...\n",
              "8255    At the North Bend Psychiatric Hospital in 1966...\n",
              "8256    The film is a depiction of various scenes, usu...\n",
              "Name: plot_synopsis, Length: 8257, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (a) Bag-of-Words with tf*idf (sparse representation):\n"
      ],
      "metadata": {
        "id": "m_571jLkIyPe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF-IDF (Term Frequency-Inverse Document Frequency) is a numercial statistic that evaluates the importance of a word in a document against all documents.\n",
        "To implement TF-IDF for this task, we will use TfidfVectorizer (from scikit-learn) to create a matrix that contains documents (rows) and terms (columns) in the vocabulary, and stores the TF-IDF scores.\n",
        "Uisng the matrix, we will then calculate the cosine similarity between 2 terms."
      ],
      "metadata": {
        "id": "l24Dsjvfak0g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we can start working, we have to preprocess our data. This is important to reduce noise and improve the model performance. The function preprocess_text does the following:\n",
        "\n",
        "\n",
        "*   Lowercase the text.\n",
        "*   Tokenize the text (breaks down text into discrete units/words).\n",
        "*   Removing stopwords.\n",
        "*   Remove punctuations.\n",
        "*   Lemmatization to reduce words to their root form.\n"
      ],
      "metadata": {
        "id": "W7055D25a1J9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Tokenization\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Remove stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "    # Remove punctuation\n",
        "    tokens = [token for token in tokens if token not in string.punctuation]\n",
        "\n",
        "    # Lemmatization and stemming\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "    # Join tokens back into a string\n",
        "    return ' '.join(tokens)"
      ],
      "metadata": {
        "id": "f7oIXSYwbFB_"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the training data we loaded, we have to extract the plot synopsis which we will be using as our corpus. We process the synposes, and create a TF-IDF representation for it."
      ],
      "metadata": {
        "id": "40fzLGTrbVaI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#start the clock to calculate the length of the process of training\n",
        "start = time.time()"
      ],
      "metadata": {
        "id": "8ISdiHKbVjpP"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the plot_synopsis column using the preprocess_text function\n",
        "corpus = train_data['plot_synopsis'].apply(preprocess_text)\n",
        "\n",
        "# Create TF-IDF matrix representation from our corpus\n",
        "# The N-gram range is set to (1,3), meaning the model will consider unigrams, bigrams, and trigarms\n",
        "tfidf_vect = TfidfVectorizer(ngram_range=(1, 3))\n",
        "# Transform the corpus into a tfidf matrix\n",
        "matrix = tfidf_vect.fit_transform(corpus)"
      ],
      "metadata": {
        "id": "nZgRYhqEFfE1"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Check the length of our vocabulary\n",
        "len(tfidf_vect.vocabulary_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HryvvA7bDDCg",
        "outputId": "e17e2f48-40d8-4404-b1ac-89da51a05e1b"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6157454"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have created the TF-IDF representation for our corpus, we can see how the model performs on our validation dataset. But before doing that, we need to think of Out-of-Vocabulary words. Since our model is fitted using our corpus, the validation dataset may contain unseen words. To handle that, we will check for synonyms using wordnet."
      ],
      "metadata": {
        "id": "n3qAE9QlT9VR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function synonym takes in a term as an input and uses WordNet to find relevant synonyms. It returns the list of synonyms for a given term. It is used to handle OOV. If the word is not in our vocabulary, then we check if the synonyms are."
      ],
      "metadata": {
        "id": "CtoMaMo1XAb-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def synonym(term):\n",
        "  #initialize lemmatizer and an empty set to store synonyms\n",
        "  lem = WordNetLemmatizer()\n",
        "  syn = set()\n",
        "  #iterating over the set of synonyms for a given term from wordnet\n",
        "  for s in wordnet.synsets(term):\n",
        "    #iterate over the individual synonyms in the set\n",
        "    for l in s.lemmas():\n",
        "      #lemmatize the synonym and add it to the set\n",
        "      lem_syn = lem.lemmatize(l.name())\n",
        "      syn.add(lem_syn)\n",
        "  return list(syn)"
      ],
      "metadata": {
        "id": "drygz8Z_ZP4x"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The check_vocab is a function that checks if a term is present in our vocabulary. If it is, then we retreive the index, if not then we return None. This helps identify OOV words."
      ],
      "metadata": {
        "id": "-JH32uTLX2lu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def check_vocab(term, tfidf_vect, matrix):\n",
        "  #check if the term is present in our vocabulary\n",
        "    if term in tfidf_vect.vocabulary_:\n",
        "      #retreive index of the term\n",
        "        indx = tfidf_vect.vocabulary_[term]\n",
        "        #extract the column and convert it to an array and faltten it\n",
        "        return matrix[:, indx].toarray().flatten()\n",
        "    #if the term is OOV return None\n",
        "    return None"
      ],
      "metadata": {
        "id": "wRHeY5nbfUCm"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The last function we have is cos_sim, which takes in the 2 terms we want to calculate the similarity for, along with our tfidf matrix and vectorizer. The first thing is to vectorize the terms using check_vocab function. If the vecttors are returned, it calculates the cosine similarity. Otherwise, it looks at synonyms using the synoyms function. If valid vectors are found, it calculates the similarity, otherwise it returns a similarity of 1 as a default."
      ],
      "metadata": {
        "id": "7i6aRBrHZdxl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cos_sim(term1, term2, tfidf_vect, matrix):\n",
        "    # Vectorize terms using check_vocab function\n",
        "    v1 = check_vocab(term1, tfidf_vect, matrix)\n",
        "    v2 = check_vocab(term2, tfidf_vect, matrix)\n",
        "\n",
        "    # check for valid vectors and calculate their cosine similarity\n",
        "    if v1 is not None and v2 is not None:\n",
        "        return cosine_similarity([v1], [v2])[0][0]\n",
        "\n",
        "    # Handle synonyms\n",
        "    syn1 = synonym(term1)\n",
        "    syn2 = synonym(term2)\n",
        "\n",
        "    #iterate over the first term's synonyms\n",
        "    for s1 in syn1:\n",
        "      #vectorize the synonym\n",
        "        v1_syn = check_vocab(s1, tfidf_vect, matrix)\n",
        "        #check if the vector is valid\n",
        "        if v1_syn is not None:\n",
        "            #iterate over the second term's synonyms\n",
        "            for s2 in syn2:\n",
        "                #vectorize the synonym\n",
        "                v2_syn = check_vocab(s2, tfidf_vect, matrix)\n",
        "                #check if the vector is valid\n",
        "                if v2_syn is not None:\n",
        "                    #calculate the synonyms cosine similarity\n",
        "                    return cosine_similarity([v1_syn], [v1_syn])[0][0]\n",
        "\n",
        "    return 0.5  # Default similarity if no valid vectors are found"
      ],
      "metadata": {
        "id": "YBli0AqGfV87"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate the cosine similarity between term1 and term2 in the test data:"
      ],
      "metadata": {
        "id": "vZqq5HzeGTFY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Empty list to store the cosine similarity values\n",
        "similarity_values = []\n",
        "#interating over the rows in our validation data\n",
        "for i, row in val_data.iterrows():\n",
        "    term1 = row['term1']\n",
        "    term2 = row['term2']\n",
        "    #use the cos_sim function to calculate the cosine similarity between the terms\n",
        "    similarity = cos_sim(term1, term2, tfidf_vect, matrix)\n",
        "    #append the calculated similarity to the list\n",
        "    similarity_values.append(similarity)\n",
        "#add a new column to our test datafarme with the cosine similarity calculated\n",
        "val_data['cosine_similarity'] = similarity_values\n"
      ],
      "metadata": {
        "id": "xqRlXsm7faJV"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#stop the clock\n",
        "end = time.time()\n",
        "#calculate the elapsed time\n",
        "elapsed_time = end - start\n",
        "print(f'Time taken to preprocess, train, and validate the model: {elapsed_time} seconds')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UdOnw7-4VwUl",
        "outputId": "3127ef16-42b0-4bf9-e76b-9851c5dfd40b"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time taken to preprocess, train, and validate the model: 142.7440857887268 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving the results in a csv file:"
      ],
      "metadata": {
        "id": "c-oZITiOGaeU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save results to a CSV file\n",
        "result_df = pd.DataFrame({'id': val_data['id'], 'cosine_similarity': val_data['cosine_similarity']})\n",
        "result_df.to_csv('10693727-Task1-method-a-validation.csv', index=False, header=False)\n"
      ],
      "metadata": {
        "id": "9HWW1nKK_NCU"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Results:**\n",
        "\n",
        "I have played around with the preprocessing function, and found that removing punctuation and lemmatization improves the accuracy of the model to 61%. In addition to checking for synonyms."
      ],
      "metadata": {
        "id": "x5RRQ48gc4r6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing:"
      ],
      "metadata": {
        "id": "9g58xzk20uWu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have our model trained using our training dataset, and tested using the validation dataset, we can test it on unseen data. We will load the test dataset and run the model just like we did for the validation dataset, and save the results in a csv file."
      ],
      "metadata": {
        "id": "T6nwVTk909kV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the test dataset\n",
        "test_data = pd.read_csv('./data/Task-1-test-dataset.csv', names=['id', 'term1', 'term2'])\n",
        "#start clock\n",
        "start = time.time()\n",
        "#Empty list to store the cosine similarity values\n",
        "test_similarity_values = []\n",
        "#interating over the rows in our validation data\n",
        "for i, row in test_data.iterrows():\n",
        "    term1 = row['term1']\n",
        "    term2 = row['term2']\n",
        "    #use the cos_sim function to calculate the cosine similarity between the terms\n",
        "    test_similarity = cos_sim(term1, term2, tfidf_vect, matrix)\n",
        "    #append the calculated similarity to the list\n",
        "    test_similarity_values.append(test_similarity)\n",
        "#add a new column to our test datafarme with the cosine similarity calculated\n",
        "test_data['cosine_similarity'] = test_similarity_values\n",
        "#end time\n",
        "end = time.time()\n",
        "#print the time elapsed\n",
        "elapsed_time = end - start\n",
        "print(f'Time taken to test the model: {elapsed_time} seconds')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_k00G8-1UGG",
        "outputId": "2ab3d464-84c6-4d73-ee1b-876cb2c0e018"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time taken to test the model: 8.274048328399658 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save the results:"
      ],
      "metadata": {
        "id": "AK9CH6z12m7O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save results to a CSV file\n",
        "test_result = pd.DataFrame({'id': test_data['id'], 'cosine_similarity': test_data['cosine_similarity']})\n",
        "test_result.to_csv('10693727-Task1-method-a.csv', index=False, header=False)"
      ],
      "metadata": {
        "id": "m8516UEl3UIt"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (b) word2vec (dense static representation)"
      ],
      "metadata": {
        "id": "ff29fjJZJUZ8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word2Vec is an NLP technique used for learning distributed representations of terms in a continious vector space. In this implementation, Word2Vec is used to generate vectors to represent terms based on semantic terms. Which are then used to calculate the cosine similarity between 2 terms."
      ],
      "metadata": {
        "id": "WC_jCW8WjHS6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will be using the preprocess_text function from our previous method to preprocess the data."
      ],
      "metadata": {
        "id": "C6nG8EdnkN4k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#start time\n",
        "start = time.time()\n",
        "# Preprocess training data\n",
        "corpus = [preprocess_text(doc) for doc in train_data['plot_synopsis']]"
      ],
      "metadata": {
        "id": "-3JFl5bjvTsa"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We train a bigram detector using Phrases (gensim) to identify multi term words. We then use that to train the Word2Vec model. This will help capture individual and multi term words that are frequently occuring."
      ],
      "metadata": {
        "id": "bPIoQaNMvbit"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train a bigram detector.\n",
        "bigram = Phrases(corpus, min_count=1, threshold=1)\n",
        "#create a phraser object\n",
        "bigram_text = Phrases(bigram[corpus])\n",
        "\n",
        "# Train Word2Vec model\n",
        "model = Word2Vec(sentences=list(bigram_text[corpus]), vector_size=100, window=5, min_count=5, workers=3, sg=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6AT3QZenvcLZ",
        "outputId": "f0635373-ce2e-44c3-af60-5a133b688f2c"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.word2vec:Each 'sentences' item should be a list of words (usually unicode strings). First item here is instead plain <class 'str'>.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vector_dimension = model.vector_size\n",
        "\n",
        "print(f\"Word2Vec Vector Dimension: {vector_dimension}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1BrYzyHAQ7OD",
        "outputId": "afe705e4-0cc4-409c-c9c4-5928b0d9c7c1"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word2Vec Vector Dimension: 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The get_vector function uses the Word2Vec model we trained above. It initialzes an empty array to accumulate the word vectors. It iterates through the document and checks if the word is in the vocablary, if it is not then it checks for synonyms."
      ],
      "metadata": {
        "id": "eirsHHFCz5_7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_vector(text):\n",
        "    #initialize an array to accumlate vector\n",
        "    vector = np.zeros((1, model.vector_size))\n",
        "    #iterate over the words in the document\n",
        "    for word in text:\n",
        "        #check if the word is in the vocabulary\n",
        "        if word in model.wv:\n",
        "            # if the word is in the vocabulary, add it to the vector sum\n",
        "            vector += model.wv[word]\n",
        "        else:\n",
        "            # If the word is not in the vocabulary, try finding a synonym\n",
        "            synonyms = []\n",
        "            for syn in wordnet.synsets(word):\n",
        "                for lemma in syn.lemmas():\n",
        "                    synonyms.append(lemma.name())\n",
        "            # iterate over the synonyms and add the vector of the first found synonym\n",
        "            for synonym in synonyms:\n",
        "                if synonym in model.wv:\n",
        "                    vector += model.wv[synonym]\n",
        "\n",
        "    return vector + 1  # Add 1 to the entire vector"
      ],
      "metadata": {
        "id": "S8LdHMFNz6bP"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the validation dataset that we loaded previously, we will assess the performance of the model. We create an empty column in the dataframe where we will store the calculated similarities. Then loop over each pair of terms in the validation set and preprocess it using the preprocess_text function. We then get the vectors for the terms using get_vector function. Using the vectors, we calculate the cosine similarity."
      ],
      "metadata": {
        "id": "Fvr5oKPw0DOS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add a column for calculated similarities to the validation DataFrame\n",
        "val_data['calculated_similarity'] = np.nan\n",
        "\n",
        "# Test the model using the test data\n",
        "for i, pair in val_data.iterrows():\n",
        "    term1 = preprocess_text(pair[\"term1\"])\n",
        "    term2 = preprocess_text(pair[\"term2\"])\n",
        "\n",
        "    # Get Word2Vec vectors for the terms\n",
        "    vector_term1 = get_vector(term1)\n",
        "    vector_term2 = get_vector(term2)\n",
        "\n",
        "    # Calculate cosine similarity using Word2Vec vectors\n",
        "    similarity_word2vec = cosine_similarity(vector_term1, vector_term2)[0][0]\n",
        "\n",
        "    # Update the 'Similarity_Word2Vec' column\n",
        "    val_data.loc[i, 'calculated_similarity'] = similarity_word2vec\n",
        "\n",
        "#stop the clock\n",
        "end = time.time()\n",
        "#calculate the elapsed time\n",
        "elapsed_time = end - start\n",
        "print(f'Time taken to preprocess, train, and validate the model: {elapsed_time} seconds')"
      ],
      "metadata": {
        "id": "E79Z2IXiJaMK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a0a4f81-ea19-4122-ef9f-c998c4efc33e"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time taken to preprocess, train, and validate the model: 227.48058700561523 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save the results:"
      ],
      "metadata": {
        "id": "renzA-dS0EO6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the updated test DataFrame to a new CSV file\n",
        "val_data[['id', 'calculated_similarity']].to_csv('10693727-Task1-method-b-validation.csv', header=False, index=False)"
      ],
      "metadata": {
        "id": "088_kSRU0EyM"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Results:**\n",
        "\n",
        "For this method, when using CBOW, the accuracy ranged between 48% to 51%. When implementing the Bigram detector from gensim, and using Skip-gram, the accuracy increased to 54%."
      ],
      "metadata": {
        "id": "jb54sYoRUNgh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing:"
      ],
      "metadata": {
        "id": "Scp5ZKh19ifp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have our model trained using our training dataset, and tested using the validation dataset, we can test it on unseen data. We will load the test dataset and run the model just like we did for the validation dataset, and save the results in a csv file."
      ],
      "metadata": {
        "id": "M1bd_RR19xp4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#start time\n",
        "start = time.time()\n",
        "# Add a column for calculated similarities to the test DataFrame\n",
        "test_data['calculated_similarity'] = np.nan\n",
        "\n",
        "# Test the model using the test data\n",
        "for i, pair in test_data.iterrows():\n",
        "    term1 = preprocess_text(pair[\"term1\"])\n",
        "    term2 = preprocess_text(pair[\"term2\"])\n",
        "\n",
        "    # Get Word2Vec vectors for the terms\n",
        "    vector_term1 = get_vector(term1)\n",
        "    vector_term2 = get_vector(term2)\n",
        "\n",
        "    # Calculate cosine similarity using Word2Vec vectors\n",
        "    similarity_word2vec = cosine_similarity(vector_term1, vector_term2)[0][0]\n",
        "\n",
        "    # Update the 'Similarity_Word2Vec' column\n",
        "    test_data.loc[i, 'calculated_similarity'] = similarity_word2vec\n",
        "#end time\n",
        "end = time.time()\n",
        "#print the time elapsed\n",
        "elapsed_time = end - start\n",
        "print(f'Time taken to test the model: {elapsed_time} seconds')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_1X4QMfO9xp-",
        "outputId": "263fb669-be7c-4b1a-f608-db217039d3d2"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time taken to test the model: 0.1435244083404541 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save the results:"
      ],
      "metadata": {
        "id": "npZbZ5RB9xp-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the updated test DataFrame to a new CSV file\n",
        "test_data[['id', 'calculated_similarity']].to_csv('10693727-Task1-method-b.csv', header=False, index=False)"
      ],
      "metadata": {
        "id": "ZJROG3dL9xp-"
      },
      "execution_count": 69,
      "outputs": []
    }
  ]
}