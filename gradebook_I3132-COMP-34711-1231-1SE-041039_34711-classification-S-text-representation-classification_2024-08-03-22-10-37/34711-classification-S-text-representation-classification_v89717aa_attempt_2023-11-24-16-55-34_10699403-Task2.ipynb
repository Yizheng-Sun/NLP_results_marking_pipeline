{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "10699403 - Task 2 methods a and b"
      ],
      "metadata": {
        "id": "rg2hVyA0PNxj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u3JTW6MJ7aRv"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "# Download NLTK data\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Qwjxv8o5709x"
      },
      "outputs": [],
      "source": [
        "# Function to preprocess data by converting to lowercase, removing non-alphanumeric characters, tokenizing, removing stopwords, and lemmatizing\n",
        "def preprocess_data(data):\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  stopWords = set(stopwords.words('english'))\n",
        "\n",
        "  # Convert to lowercase\n",
        "  data = data.lower()\n",
        "  # Remove non-alphanumeric characters\n",
        "  data = re.sub(r\"[^\\w\\d'\\s]+\", '', data)\n",
        "\n",
        "  # Tokenize the data\n",
        "  tokens = word_tokenize(data)\n",
        "  # Remove stopwords\n",
        "  cleaned_tokens =  [token for token in tokens if token not in stopWords]\n",
        "\n",
        "  #Lemmatize the tokens\n",
        "  final_tokens = [lemmatizer.lemmatize(token) for token in cleaned_tokens]\n",
        "\n",
        "  return ' '.join(final_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "hgc4Dd9x8roD"
      },
      "outputs": [],
      "source": [
        "# Read in the training data from the CSV file\n",
        "training_data = pd.read_csv('./data/Training-dataset.csv')\n",
        "# Apply preprocessing to the training data\n",
        "processed_document_data = training_data['title'] + ' ' + training_data['plot_synopsis'].apply(preprocess_data)\n",
        "genres = training_data[['comedy', 'cult', 'flashback', 'historical', 'murder', 'revenge', 'romantic', 'scifi', 'violence']].values\n",
        "genres_list = ['comedy', 'cult', 'flashback', 'historical', 'murder', 'revenge', 'romantic', 'scifi', 'violence']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJircwhx-_Tn"
      },
      "outputs": [],
      "source": [
        "# Create a pipeline for Naive Bayes classifier using TF-IDF features\n",
        "nb_pipeline = make_pipeline(TfidfVectorizer(max_features=10000, ngram_range=(1, 4), sublinear_tf=True, use_idf=False),\n",
        "                            MultiOutputClassifier(MultinomialNB(alpha=0.01, fit_prior=False)))\n",
        "nb_pipeline.fit(processed_document_data, genres)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nDk36VMRFr7m"
      },
      "outputs": [],
      "source": [
        "# Tokenize and pad sequences for input to the LSTM model\n",
        "tokenizer = Tokenizer(num_words=25000)\n",
        "tokenizer.fit_on_texts(processed_document_data)\n",
        "X_train_tokenized = tokenizer.texts_to_sequences(processed_document_data)\n",
        "X_train_padded = pad_sequences(X_train_tokenized, maxlen=450)\n",
        "\n",
        "# Build a Sequential model with an Embedding layer, Bidirectional LSTM layer, and Dense output layer\n",
        "model = Sequential()\n",
        "model.add(Embedding(25001, 128, input_length=450))\n",
        "model.add(Bidirectional(LSTM(128)))\n",
        "model.add(Dense(genres.shape[1], activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the LSTM model\n",
        "model.fit(X_train_padded, genres, epochs=2, batch_size=32, validation_split=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "rs8RP0QCA5fH"
      },
      "outputs": [],
      "source": [
        "# Read validation data from a CSV file\n",
        "validation_data = pd.read_csv('/data/Task-2-validation-dataset.csv')\n",
        "# Apply preprocessing to the validation data so that it is in consistent format to the training data\n",
        "validation_document_data = validation_data['title'] + ' ' + validation_data['plot_synopsis'].apply(preprocess_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "BLXgyt4JBpdJ"
      },
      "outputs": [],
      "source": [
        "# Predict genres using Naive Bayes classifier for the validation data and save the results to a CSV file\n",
        "validation_nb_prediction = nb_pipeline.predict(validation_document_data)\n",
        "validation_nb_prediction_df = pd.DataFrame(validation_nb_prediction, columns=genres_list)\n",
        "validation_nb_output = pd.concat([validation_data[['ID']], validation_nb_prediction_df], axis=1)\n",
        "validation_nb_output.to_csv(\"./data/10699403-Task2-method-a-validation.csv\", header=False, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZZ8lIGi2Z6Ko"
      },
      "outputs": [],
      "source": [
        "# Tokenize and pad sequences for the validation set to input to the LSTM model and get the LSTM prediction\n",
        "validation_lstm_tokenized = tokenizer.texts_to_sequences(validation_document_data)\n",
        "validation_lstm_pad_tokenized = pad_sequences(validation_lstm_tokenized, maxlen=450)\n",
        "validation_lstm_prediction = model.predict(validation_lstm_pad_tokenized)\n",
        "\n",
        "# Convert validation predictions to binary format (each genre 0 or 1 instead of probability) and save results to a CSV file\n",
        "validation_lstm_binary_predictions = (validation_lstm_prediction >= 0.3).astype(int)\n",
        "validation_lstm_predicted_labels = pd.DataFrame(validation_lstm_binary_predictions, columns=training_data.columns[3:])\n",
        "validation_lstm_output = pd.concat([validation_data[['ID']], validation_lstm_predicted_labels], axis=1)\n",
        "validation_lstm_output.to_csv(\"./data/10699403-Task2-method-b-validation.csv\", header=False, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "itlLzIWCC3mq"
      },
      "outputs": [],
      "source": [
        "# Read test data from a CSV file\n",
        "test_data = pd.read_csv('./data/Task-2-test-dataset1.csv')\n",
        "# Apply preprocessing to the test data so that it is in consistent format to the training data\n",
        "test_document_data = test_data['title'] + ' ' + test_data['plot_synopsis'].apply(preprocess_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "WYuJbCVCDyOd"
      },
      "outputs": [],
      "source": [
        "# Predict genres using Naive Bayes classifier for the test data and save the results to a CSV file\n",
        "test_nb_prediction = nb_pipeline.predict(test_document_data)\n",
        "test_nb_prediction_df = pd.DataFrame(test_nb_prediction, columns=genres_list)\n",
        "test_nb_output = pd.concat([test_data[['ID']], test_nb_prediction_df], axis=1)\n",
        "test_nb_output.to_csv(\"./data/10699403-Task2-method-a.csv\", header=False, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HTUBIEn2f8kG"
      },
      "outputs": [],
      "source": [
        "# Tokenize and pad sequences for the test set to input to the LSTM model and get the LSTM prediction\n",
        "test_lstm_tokenized = tokenizer.texts_to_sequences(test_document_data)\n",
        "test_lstm_pad_tokenized = pad_sequences(test_lstm_tokenized, maxlen=350)\n",
        "test_lstm_prediction = model.predict(test_lstm_pad_tokenized)\n",
        "\n",
        "# Convert LSTM predictions to binary format and save test results to a CSV file\n",
        "test_lstm_binary_predictions = (test_lstm_prediction >= 0.3).astype(int)\n",
        "test_lstm_predicted_labels = pd.DataFrame(test_lstm_binary_predictions, columns=training_data.columns[3:])\n",
        "test_lstm_output = pd.concat([test_data[['ID']], test_lstm_predicted_labels], axis=1)\n",
        "test_lstm_output.to_csv(\"./data/10699403-Task2-method-b.csv\", header=False, index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}