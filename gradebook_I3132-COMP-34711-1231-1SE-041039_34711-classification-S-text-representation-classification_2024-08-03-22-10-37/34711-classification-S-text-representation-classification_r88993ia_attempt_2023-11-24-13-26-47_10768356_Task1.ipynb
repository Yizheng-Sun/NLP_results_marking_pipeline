{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jhTN9o7Xjf9R",
        "outputId": "b79bd947-e855-4d29-bcc1-81e1aac9bc00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package names to /root/nltk_data...\n",
            "[nltk_data]   Package names is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import unicodedata\n",
        "import re\n",
        "import string\n",
        "import time\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords, words\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from scipy.sparse import coo_matrix\n",
        "\n",
        "from gensim.models import Word2Vec, KeyedVectors\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('words')\n",
        "nltk.download('names')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_dataset_path = r\"./data/Training-dataset.csv\"\n",
        "validation_dataset_path = r\"./data/Task-1-validation-dataset.csv\"\n",
        "test_dataset_path = r\"./data/Task-1-test-dataset1.csv\""
      ],
      "metadata": {
        "id": "GOKKvFOgW4_P"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text pre-processing for both models:"
      ],
      "metadata": {
        "id": "Nd06PXI5gaC9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialise the lemmatizer and the word sets needed for preprocessing.\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "english_vocab = set(words.words()) - stop_words\n",
        "names = [name.lower() for name in nltk.corpus.names.words()]\n",
        "\n",
        "def remove_accents(data):\n",
        "  \"\"\" Removes accents from a word. \"\"\"\n",
        "  return ''.join(x for x in unicodedata.normalize('NFKD', data) if x in string.ascii_letters or x == \" \")\n",
        "\n",
        "def process_text(text):\n",
        "  \"\"\" A function for tokenization, stopwords & OOV words filtering, and lemmatization. \"\"\"\n",
        "  # Remove punctuation\n",
        "  # replace dashes with ' ' and replace everything else with a ''.\n",
        "  pattern = re.compile('[%s]' % re.escape(string.punctuation.replace('-', '')))\n",
        "  text = re.sub(pattern, '', text.replace('-', ' '))\n",
        "\n",
        "  # Tokenize\n",
        "  tokens = word_tokenize(text)\n",
        "\n",
        "  # Keep words only, remove extra whitespace, turn to lowercase, remove accents\n",
        "  tokens = [remove_accents(token.strip().lower()) for token in tokens if token.isalpha()]\n",
        "\n",
        "  # If a token is a name, change it to: '<NAME>'\n",
        "  # If a token is OOV, change it to '<UKN>'\n",
        "  # Otherwise, lemmatize it\n",
        "  temp = []\n",
        "  for token in tokens:\n",
        "    if token in names:\n",
        "        temp.append('<NAME>')\n",
        "    else:\n",
        "      if token in english_vocab:\n",
        "        temp.append(lemmatizer.lemmatize(token))\n",
        "      else:\n",
        "        if token not in stop_words:\n",
        "          temp.append('<UKN>')\n",
        "  tokens = temp\n",
        "\n",
        "  return tokens"
      ],
      "metadata": {
        "id": "g5VMHf--gbuM"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the training data\n",
        "df = pd.read_csv(training_dataset_path, usecols=['title', 'plot_synopsis'])\n",
        "\n",
        "# Pre-process the data\n",
        "df['plot_synopsis'] = df['plot_synopsis'].apply(process_text)"
      ],
      "metadata": {
        "id": "ablf0CC6g0GF"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model 1: a) A sparse representation with tf.idf**"
      ],
      "metadata": {
        "id": "9dWQOsxahJIL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_similarity_model1(phrase1, phrase2, vectorizer, tf_idf):\n",
        "  \"\"\" Calculates the consine similarity between 2 phrases.\n",
        "\n",
        "      It sums the vectors of the words in each phrase, then it returns\n",
        "      the similarity between the summed vectors.\n",
        "  \"\"\"\n",
        "\n",
        "  # Split the phrase\n",
        "  phrase1 = phrase1.split()\n",
        "  # Initialise a sparse matrix (it has 1 element - which is 0 - at (0,0))\n",
        "  vector1 = coo_matrix(([0], ([0], [0])), shape=(1, tf_idf.shape[0]))\n",
        "\n",
        "  for word in phrase1:\n",
        "    # Apply the same pre-processing done to the training set to each word\n",
        "    word = lemmatizer.lemmatize(word)\n",
        "    if word in names:\n",
        "      word = '<NAME>'\n",
        "    if vectorizer.vocabulary_.get(word, -1) == -1:\n",
        "      word = '<UKN>'\n",
        "\n",
        "    # Add the current vector to the total vector\n",
        "    index = vectorizer.vocabulary_.get(word, -1)\n",
        "    vector1 += tf_idf[:, index].reshape(1, -1)\n",
        "\n",
        "\n",
        "  phrase2 = phrase2.split()\n",
        "  vector2 = coo_matrix(([0], ([0], [0])), shape=(1, tf_idf.shape[0]))\n",
        "\n",
        "  for word in phrase2:\n",
        "    word = lemmatizer.lemmatize(word)\n",
        "    if word in names:\n",
        "      word = '<NAME>'\n",
        "    if vectorizer.vocabulary_.get(word, -1) == -1:\n",
        "      word = '<UKN>'\n",
        "\n",
        "    index = vectorizer.vocabulary_.get(word, -1)\n",
        "    vector2 += tf_idf[:, index].reshape(1, -1)\n",
        "\n",
        "  # Return the similarity\n",
        "  return cosine_similarity(vector1, vector2)[0, 0]"
      ],
      "metadata": {
        "id": "izl1RU8SZ2Dh"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model1(vectorizer, tf_idf, input_path, output_path, validation=True):\n",
        "  \"\"\" Evaluates the model on a validation/test set.\n",
        "\n",
        "      It does the following:\n",
        "        1- Reads the input file\n",
        "        2- Calculate the similarity between the given pairs.\n",
        "        3- Store the results in the specified path.\n",
        "  \"\"\"\n",
        "\n",
        "  if validation:\n",
        "    column_headers = ['id', 'word1', 'word2', 'similarity']\n",
        "  else:\n",
        "    column_headers = ['id', 'word1', 'word2']\n",
        "\n",
        "  # Get the validation data\n",
        "  evaluation_df = pd.read_csv(input_path, header=None, names=column_headers)\n",
        "\n",
        "  # Calculate the similarity for each 2 words in the validation set\n",
        "  evaluation_df['similarity'] = evaluation_df.apply(lambda row: calculate_similarity_model1(row['word1'], row['word2'], vectorizer, tf_idf), axis=1)\n",
        "\n",
        "  # Store the 'id' and 'similarity' to a csv file\n",
        "  result_df = evaluation_df[['id', 'similarity']]\n",
        "  result_df.to_csv(output_path, index=False, header=False)"
      ],
      "metadata": {
        "id": "Bzq5pR4sZ5Lc"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Join the tokens (to allow TfidfVectorizer to create a tf.idf matrix)\n",
        "df['text'] = df['plot_synopsis'].apply(' '.join)\n",
        "\n",
        "# Create a tf.idf for both unigrams and bigrams\n",
        "vectorizer = TfidfVectorizer()\n",
        "tf_idf = vectorizer.fit_transform(df['text'])"
      ],
      "metadata": {
        "id": "Sw0BeaT6mLuJ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vocabulary length\n",
        "len(vectorizer.get_feature_names_out())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jdiDkzyPsdek",
        "outputId": "76fe60e6-41fa-4988-f078-78bb8479ea71"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25678"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Validation dataset:"
      ],
      "metadata": {
        "id": "RRt6hY3mbXWx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Record the start time\n",
        "start_time = time.time()\n",
        "\n",
        "# Evaluate the model\n",
        "evaluate_model1(vectorizer, tf_idf, validation_dataset_path, '10768356-Task1-method-a-validation.csv')\n",
        "\n",
        "# Record the end time\n",
        "end_time = time.time()\n",
        "\n",
        "# Calculate and print the elapsed time\n",
        "elapsed_time = end_time - start_time\n",
        "print(f\"Time taken: {elapsed_time} seconds\")"
      ],
      "metadata": {
        "id": "YMRJxFRBaELO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24ab53e4-73ba-4c53-fd4b-6aeeb0392651"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time taken: 2.983353853225708 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test dataset:"
      ],
      "metadata": {
        "id": "_pN3m7wLbcDR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Record the start time\n",
        "start_time = time.time()\n",
        "\n",
        "# Evaluate the model\n",
        "evaluate_model1(vectorizer, tf_idf, test_dataset_path, '10768356-Task1-method-a.csv', validation=False)\n",
        "\n",
        "# Record the end time\n",
        "end_time = time.time()\n",
        "\n",
        "# Calculate and print the elapsed time\n",
        "elapsed_time = end_time - start_time\n",
        "print(f\"Time taken: {elapsed_time} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8KnDIfXRbatl",
        "outputId": "d69e5767-2ff3-4809-a2b8-972f18bb4161"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time taken: 2.181042194366455 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model 2: b) A dense static representation (Word2Vec)**"
      ],
      "metadata": {
        "id": "E5BO2wTHmuVG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_similarity_model2(model, phrase1, phrase2):\n",
        "  \"\"\" Calculates the consine similarity between 2 phrases.\n",
        "\n",
        "      It uses the mean vector of the words in each phrase to\n",
        "      calculate the similarity.\n",
        "  \"\"\"\n",
        "\n",
        "  phrase1 = phrase1.split()\n",
        "  vector1 = np.array(np.zeros(model.vector_size))\n",
        "  for word in phrase1:\n",
        "    # Apply the same pre-processing done to the training set to each word\n",
        "    word = lemmatizer.lemmatize(word)\n",
        "    if word in names:\n",
        "      word = '<NAME>'\n",
        "    if word not in model.wv.index_to_key:\n",
        "      word = '<UKN>'\n",
        "\n",
        "    # Append the current vector to the array\n",
        "    vector1 = np.vstack((vector1, model.wv[word]))\n",
        "\n",
        "  phrase2 = phrase2.split()\n",
        "  vector2 = np.array(np.zeros(model.vector_size))\n",
        "  for word in phrase2:\n",
        "    word = lemmatizer.lemmatize(word)\n",
        "    if word in names:\n",
        "      word = '<NAME>'\n",
        "    if word not in model.wv.index_to_key:\n",
        "      word = '<UKN>'\n",
        "\n",
        "    vector2 = np.vstack((vector2, model.wv[word]))\n",
        "\n",
        "  # Calculate the mean vectors\n",
        "  vector1 = vector1.mean(axis=0)\n",
        "  vector2 = vector2.mean(axis=0)\n",
        "\n",
        "  # Return the similarity\n",
        "  return cosine_similarity(vector1.reshape(1, -1), vector2.reshape(1, -1))[0, 0]"
      ],
      "metadata": {
        "id": "BJxy8URWoSg9"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model2(model, input_path, output_path, validation=True):\n",
        "  \"\"\" Evaluates the model on a validation/test set.\n",
        "\n",
        "      It does the following:\n",
        "        1- Reads the input file\n",
        "        2- Calculate the similarity between the given pairs.\n",
        "        3- Store the results in the specified path.\n",
        "  \"\"\"\n",
        "\n",
        "  if validation:\n",
        "    column_headers = ['id', 'word1', 'word2', 'similarity']\n",
        "  else:\n",
        "    column_headers = ['id', 'word1', 'word2']\n",
        "\n",
        "  # Get the validation data\n",
        "  evaluation_df = pd.read_csv(input_path, header=None, names=['id', 'word1', 'word2', 'similarity'])\n",
        "\n",
        "  # Calculate the similarity for each 2 words in the validation set\n",
        "  evaluation_df['similarity'] = evaluation_df.apply(lambda row: calculate_similarity_model2(model, row['word1'], row['word2']), axis=1)\n",
        "\n",
        "  # Store the 'id' and 'similarity' to a csv file\n",
        "  result_df = evaluation_df[['id', 'similarity']]\n",
        "  result_df.to_csv(output_path, index=False, header=False)"
      ],
      "metadata": {
        "id": "IgqfEX18oKo3"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model.\n",
        "word2vec_model = Word2Vec(sentences=df['plot_synopsis'],\n",
        "                          vector_size=100,    # Dimensionality of the word vectors (embedding size)\n",
        "                          window=10,          # Maximum distance between the current and predicted word within a sentence\n",
        "                          sg=1,               # sg=1: Skip-gram model is used\n",
        "                          epochs=5)           # Number of iterations over the dataset during training"
      ],
      "metadata": {
        "id": "QS_LwrWkpQGG"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Validation dataset:"
      ],
      "metadata": {
        "id": "dOmn0u4vbjiF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Record the start time\n",
        "start_time = time.time()\n",
        "\n",
        "# Evaluate the model\n",
        "evaluate_model2(word2vec_model, validation_dataset_path, '10768356-Task1-method-b-validation.csv')\n",
        "\n",
        "# Record the end time\n",
        "end_time = time.time()\n",
        "\n",
        "# Calculate and print the elapsed time\n",
        "elapsed_time = end_time - start_time\n",
        "print(f\"Time taken: {elapsed_time} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fwNdVRWXcQnC",
        "outputId": "cdc659a8-1535-4452-e963-9deee9dd395a"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time taken: 0.24012970924377441 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test dataset:"
      ],
      "metadata": {
        "id": "FRQpljysbm0K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Record the start time\n",
        "start_time = time.time()\n",
        "\n",
        "# Evaluate the model\n",
        "evaluate_model2(word2vec_model, test_dataset_path, '10768356-Task1-method-b.csv', validation=False)\n",
        "\n",
        "# Record the end time\n",
        "end_time = time.time()\n",
        "\n",
        "# Calculate and print the elapsed time\n",
        "elapsed_time = end_time - start_time\n",
        "print(f\"Time taken: {elapsed_time} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6G38OJdbmXU",
        "outputId": "22a27c44-4961-4953-a665-fc2756a0c343"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time taken: 0.15571928024291992 seconds\n"
          ]
        }
      ]
    }
  ]
}