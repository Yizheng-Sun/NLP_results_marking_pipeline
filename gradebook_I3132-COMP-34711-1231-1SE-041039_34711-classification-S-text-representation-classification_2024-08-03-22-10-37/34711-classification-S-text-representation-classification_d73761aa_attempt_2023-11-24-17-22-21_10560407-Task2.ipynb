{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qo8xiEct8du5"
      },
      "outputs": [],
      "source": [
        "#If any other libraries need installing, please add them here\n",
        "\n",
        "# %pip install sklearn\n",
        "# %pip install gensim\n",
        "# %pip install nltk\n",
        "# %pip install keras\n",
        "\n",
        "import pandas as pd\n",
        "import string\n",
        "import nltk\n",
        "\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.layers import TextVectorization\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Bidirectional, LSTM, Dense, BatchNormalization, Dropout\n",
        "from keras.callbacks import EarlyStopping\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"wordnet\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t9unzMf_8nmG"
      },
      "outputs": [],
      "source": [
        "df_train = pd.read_csv(\"./data/Training-dataset.csv\", usecols = [0,2,3,4,5,6,7,8,9,10,11])\n",
        "df_validation = pd.read_csv(\"./data/Task-2-validation-dataset.csv\", usecols = [0,2,3,4,5,6,7,8,9,10,11])\n",
        "df_test = pd.read_csv(\"./data/Task-2-test-dataset1.csv\", usecols = [0,2])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels = ['comedy', 'cult', 'flashback', 'historical', 'murder', 'revenge', 'romantic', 'scifi', 'violence']\n",
        "train_Y = df_train[labels].values\n",
        "validation_Y = df_validation[labels].values"
      ],
      "metadata": {
        "id": "HcWSnpvY-AIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>SVM with Doc2Vec Document Embeddings</h1>"
      ],
      "metadata": {
        "id": "BYlPfKAmByDz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7eKYbUno_a9m"
      },
      "outputs": [],
      "source": [
        "#Function pre-processes documents via normalisation, punctuation removal, tokenization, stop word removal and lemmatization\n",
        "\n",
        "def d2v_preprocess_text(text):\n",
        "\n",
        "    text = text.lower()\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "\n",
        "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    lemma = nltk.wordnet.WordNetLemmatizer()\n",
        "    tokens = [lemma.lemmatize(word) for word in tokens]\n",
        "\n",
        "    return tokens\n",
        "\n",
        "df_train['plot_synopsis_d2v_processed'] = df_train['plot_synopsis'].apply(d2v_preprocess_text)\n",
        "train_documents = df_train['plot_synopsis_d2v_processed'].tolist()\n",
        "\n",
        "df_validation['plot_synopsis_d2v_processed'] = df_validation['plot_synopsis'].apply(d2v_preprocess_text)\n",
        "validation_documents = df_validation['plot_synopsis_d2v_processed'].tolist()\n",
        "\n",
        "df_test['plot_synopsis_d2v_processed'] = df_test['plot_synopsis'].apply(d2v_preprocess_text)\n",
        "test_documents = df_test['plot_synopsis_d2v_processed'].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Doc2Vec Model with specified parameters\n",
        "\n",
        "tagged_data = [TaggedDocument(words=doc, tags=[str(i)]) for i, doc in enumerate(train_documents)]\n",
        "model_doc = Doc2Vec(vector_size=200,\n",
        "                window=1,\n",
        "                min_count=1,\n",
        "                workers=2)\n",
        "\n",
        "model_doc.build_vocab(tagged_data)\n",
        "model_doc.train(tagged_data, total_examples=model_doc.corpus_count, epochs=10)\n",
        "\n",
        "#Document embeddings for training, validation and test created from the trained Doc2Vec Model\n",
        "train_X = np.array([model_doc.infer_vector(doc) for doc in train_documents])\n",
        "validation_X = np.array([model_doc.infer_vector(doc) for doc in validation_documents])\n",
        "test_X = np.array([model_doc.infer_vector(doc) for doc in test_documents])"
      ],
      "metadata": {
        "id": "6rRdTuAUBxaM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t_rxl8VSJZNH"
      },
      "outputs": [],
      "source": [
        "#SVM multi-label classification model\n",
        "\n",
        "svm_classifier = SVC(class_weight='balanced')\n",
        "ova_classifier = MultiOutputClassifier(svm_classifier)\n",
        "ova_classifier.fit(train_X, train_Y)\n",
        "\n",
        "#Prediciting document labels for validation data\n",
        "val_predicition_Y = ova_classifier.predict(validation_X)\n",
        "\n",
        "# #Prediciting document labels for test data\n",
        "test_predicition_Y = ova_classifier.predict(test_X)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Save label predicitions for validation and test datasets to CSV\n",
        "\n",
        "df_svm_validation = pd.concat([pd.DataFrame({'ID': df_validation['ID']}), pd.DataFrame(val_predicition_Y, columns=labels)], axis=1)\n",
        "df_svm_validation.to_csv(\"10560407-Task2-method-a-validation.csv\", index=False, header=False)\n",
        "\n",
        "df_svm_test = pd.concat([pd.DataFrame({'ID': df_test['ID']}), pd.DataFrame(test_predicition_Y, columns=labels)], axis=1)\n",
        "df_svm_test.to_csv(\"10560407-Task2-method-a.csv\", index=False, header=False)"
      ],
      "metadata": {
        "id": "8Snj9S0rR4ag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Bi-LSTM</h1>"
      ],
      "metadata": {
        "id": "iUMpVXiVBhHB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Function pre-processes documents via normalisation, tokenisation, punctuation removal, stop word removal and lemmatization\n",
        "\n",
        "def bilstm_preprocess_text(text):\n",
        "\n",
        "    text = text.lower()\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "\n",
        "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    lemma = nltk.wordnet.WordNetLemmatizer()\n",
        "    tokens = [lemma.lemmatize(word) for word in tokens]\n",
        "\n",
        "    sentence = ' '.join(tokens)\n",
        "\n",
        "    return sentence\n",
        "\n",
        "df_train['plot_synopsis_bilstm_processed'] = df_train['plot_synopsis'].apply(bilstm_preprocess_text)\n",
        "df_validation['plot_synopsis_bilstm_processed'] = df_validation['plot_synopsis'].apply(bilstm_preprocess_text)\n",
        "df_test['plot_synopsis_bilstm_processed'] = df_test['plot_synopsis'].apply(bilstm_preprocess_text)"
      ],
      "metadata": {
        "id": "ieFyrdZsNZzP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create vector representations of documents using TextVectorization\n",
        "\n",
        "max_words = 120000\n",
        "max_sequence_length = 400\n",
        "\n",
        "vectorizer = TextVectorization(max_tokens=max_words, output_mode='int', output_sequence_length=max_sequence_length)\n",
        "vectorizer.adapt(df_train['plot_synopsis_bilstm_processed'].values)\n",
        "\n",
        "train_X_bilstm = vectorizer(df_train['plot_synopsis_bilstm_processed'].values)\n",
        "validation_X_bilstm = vectorizer(df_validation['plot_synopsis_bilstm_processed'].values)\n",
        "test_X_bilstm = vectorizer(df_test['plot_synopsis_bilstm_processed'].values)"
      ],
      "metadata": {
        "id": "NywcluhVNvuw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the Bi-LSTM model\n",
        "\n",
        "#Class weights are calculated to accouint for under represented labels within the training data\n",
        "class_counts = np.sum(train_Y, axis=0)\n",
        "total_samples = len(train_Y)\n",
        "class_weights = total_samples / (len(class_counts) * class_counts)\n",
        "class_weight_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
        "\n",
        "embedding_dim = 500\n",
        "lstm_units = 64\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_sequence_length, mask_zero=True))\n",
        "model.add(Bidirectional(LSTM(units=lstm_units, return_sequences=True)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Bidirectional(LSTM(units=lstm_units)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(train_Y.shape[1], activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "TqvGhAARNCMv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training of the model\n",
        "\n",
        "epochs = 5\n",
        "batch_size = 64\n",
        "early_stopping = EarlyStopping(monitor='val_accuracy', patience=2, restore_best_weights=True) #Tracks validation dataset accuracy at each epoch to allow for early stopping if accuracy diminshes\n",
        "\n",
        "model.fit(train_X_bilstm,\n",
        "          train_Y,\n",
        "          epochs=epochs,\n",
        "          batch_size=batch_size,\n",
        "          validation_data=(validation_X_bilstm, validation_Y),\n",
        "          class_weight=class_weight_dict,\n",
        "          callbacks=[early_stopping])"
      ],
      "metadata": {
        "id": "a6N1-KuEBoGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Bi-LSTM only calculates predicited probabilities of labels, so thresholding is used to determine actual labels\n",
        "threshold = 0.12\n",
        "\n",
        "#Prediciting document labels for validation data\n",
        "val_predicition_Y_bilstm = (model.predict(validation_X_bilstm) > threshold).astype(int)\n",
        "\n",
        "\n",
        "#Prediciting document labels for test data\n",
        "test_predicition_Y_bilstm = (model.predict(test_X_bilstm) > threshold).astype(int)"
      ],
      "metadata": {
        "id": "epkQS2CsT7hs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Save label predicitions for validation and test datasets to CSV\n",
        "\n",
        "df_bilstm_validation = pd.concat([pd.DataFrame({'ID': df_validation['ID']}), pd.DataFrame(val_predicition_Y_bilstm, columns=labels)], axis=1)\n",
        "df_bilstm_validation.to_csv(\"10560407-Task2-method-b-validation.csv\", index=False, header=False)\n",
        "\n",
        "df_bilstm_test = pd.concat([pd.DataFrame({'ID': df_test['ID']}), pd.DataFrame(test_predicition_Y_bilstm, columns=labels)], axis=1)\n",
        "df_bilstm_test.to_csv(\"10560407-Task2-method-b.csv\", index=False, header=False)"
      ],
      "metadata": {
        "id": "gcO0bwRa_D-Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}