{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Import Packages"
      ],
      "metadata": {
        "id": "2-u-HH9t-aUa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lGDXNBUncOEW",
        "outputId": "fe2b8931-c857-4a98-9394-7b90ff30ca9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Embedding, Bidirectional, Dense, Dropout, GlobalMaxPool1D, LSTM\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Create Functions & Class"
      ],
      "metadata": {
        "id": "psMnRQKt-cov"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TextClassification:\n",
        "\n",
        "  def __init__(self) -> None:\n",
        "    self.method = 0\n",
        "\n",
        "  def preprocess_data(self, document: list) -> list:\n",
        "    \"\"\"\n",
        "    Applies the following to the data: tokenization, stop words, stemming, punctuation & lowercase\n",
        "    :param document: list of data to be cleaned\n",
        "    :return: list of cleaned data\n",
        "    \"\"\"\n",
        "\n",
        "    # Tokenization\n",
        "    tokenized = nltk.tokenize.word_tokenize(document)\n",
        "\n",
        "    # Stop words\n",
        "    stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n",
        "    cleaned_words = [word for word in tokenized if word not in stop_words]\n",
        "\n",
        "    # Stemming\n",
        "    porter_stemmer = nltk.stem.PorterStemmer()\n",
        "    cleaned_words = [porter_stemmer.stem(word) for word in cleaned_words]\n",
        "\n",
        "    # Punctuation\n",
        "    punctuations = set(string.punctuation)\n",
        "    cleaned_words = [word for word in cleaned_words if word not in punctuations]\n",
        "\n",
        "    # Lowercase\n",
        "    cleaned_words = ' '.join([term.lower() for term in cleaned_words])\n",
        "\n",
        "    return cleaned_words\n",
        "\n",
        "  def classification(self, train_data: list, test_data: list) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Traditional classifier using SVM with preprocessing\n",
        "    :param train_data: training data for the SVM\n",
        "    :param test_data: testing data for the SVM\n",
        "    :return: a dataframe with id and predicted scores for categories\n",
        "    \"\"\"\n",
        "\n",
        "    # Preprocess data first\n",
        "    X = [self.preprocess_data(doc) for doc in train_data['plot_synopsis']]\n",
        "    y = [self.preprocess_data(doc) for doc in test_data['plot_synopsis']]\n",
        "\n",
        "    categories = train_data.columns[3:]\n",
        "\n",
        "    # CSV template\n",
        "    results = pd.DataFrame({\n",
        "        'ID': test_data['ID'],\n",
        "        'comedy': 0,\n",
        "        'cult': 0,\n",
        "        'flashback': 0,\n",
        "        'historical': 0,\n",
        "        'murder': 0,\n",
        "        'revenge': 0,\n",
        "        'romantic': 0,\n",
        "        'scifi': 0,\n",
        "        'violence': 0,\n",
        "    })\n",
        "\n",
        "    # Pipeline, uses tfidf and then multi-label classifies\n",
        "    pipeline = Pipeline([\n",
        "        ('tfidf', TfidfVectorizer(stop_words='english', min_df=5, ngram_range=(1,2), sublinear_tf=True)),\n",
        "        ('clf', OneVsRestClassifier(LinearSVC(), n_jobs=1)),\n",
        "    ])\n",
        "\n",
        "    # For each category calculate predictions\n",
        "    for category in categories:\n",
        "      pipeline.fit(X, train_data[category])\n",
        "      prediction = pipeline.predict(y)\n",
        "      results[category] = prediction\n",
        "\n",
        "    # Write results\n",
        "    results.to_csv(f'results_{self.method}.csv', header=False, index=False)\n",
        "\n",
        "    return results\n",
        "\n",
        "  def deep_learning(self, train_data: list, test_data: list):\n",
        "    \"\"\"\n",
        "    Deep learning classifier using LSTM with preprocessing\n",
        "    :param train_data: training data for the LSTM\n",
        "    :param test_data: testing data for the LSTM\n",
        "    :return: a dataframe with id and predicted scores for categories\n",
        "    \"\"\"\n",
        "\n",
        "    # Preprocess data first\n",
        "    X = [self.preprocess_data(doc) for doc in train_data['plot_synopsis']]\n",
        "    y = [self.preprocess_data(doc) for doc in test_data['plot_synopsis']]\n",
        "\n",
        "    # Load & save if needed\n",
        "    # np.save('X.npy', X)\n",
        "    # np.save('y.npy', y)\n",
        "    # X = np.load('X.npy')\n",
        "    # y = np.load('y.npy')\n",
        "\n",
        "    categories = train_data.columns[3:]\n",
        "    max_len = 1200\n",
        "\n",
        "    # Tokenize & vocab\n",
        "    tokenize = Tokenizer()\n",
        "    tokenize.fit_on_texts(train_data['plot_synopsis'])\n",
        "    vocab_size = len(tokenize.word_index) + 1\n",
        "\n",
        "    train_encoded = tokenize.texts_to_sequences(X)\n",
        "    train_padded = pad_sequences(train_encoded, padding='post', maxlen=max_len)\n",
        "\n",
        "    test_encoded = tokenize.texts_to_sequences(y)\n",
        "    test_padded = pad_sequences(test_encoded, padding='post', maxlen=max_len)\n",
        "\n",
        "    # LSTM params\n",
        "    lstm_size = 128\n",
        "    dense_val = 9\n",
        "    dropout_val = 0.1\n",
        "\n",
        "    # Create model\n",
        "    model = Sequential([\n",
        "        Embedding(vocab_size, lstm_size, input_length=max_len),\n",
        "        Bidirectional(LSTM(lstm_size, return_sequences=True)),\n",
        "        Dropout(dropout_val),\n",
        "        Bidirectional(LSTM(int(lstm_size/2))),\n",
        "        Dropout(dropout_val),\n",
        "        Dense(dense_val, activation='sigmoid'),\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "    print(model.summary())\n",
        "\n",
        "    history = model.fit(train_padded, train_data[categories], batch_size=32, epochs=5)\n",
        "\n",
        "    # Save & load model if needed\n",
        "    # model.save('model.keras')\n",
        "    # model = load_model('model.keras')\n",
        "\n",
        "    probabilities = model.predict(test_padded)\n",
        "    predictions = (probabilities > 0.5).astype(int)\n",
        "\n",
        "    # CSV template\n",
        "    results = pd.DataFrame(data=predictions, columns=categories)\n",
        "    results.insert(0, 'ID', test_data['ID'], True)\n",
        "\n",
        "    # Write results\n",
        "    results.to_csv(f'results_{self.method}.csv', header=False, index=False)\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "YH1_2f9HgGRi"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Traditional Classification (SVM)"
      ],
      "metadata": {
        "id": "LOO9BolnH02a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main() -> None:\n",
        "  tc = TextClassification()\n",
        "\n",
        "  tc.method = 0\n",
        "\n",
        "  files_path = './data/'\n",
        "  train_data_name = 'Training-dataset.csv'\n",
        "  test_data_name = 'Task-2-test-dataset1.csv'\n",
        "\n",
        "  train_data = pd.read_csv(f'{files_path}/{train_data_name}')\n",
        "  test_data = pd.read_csv(f'{files_path}/{test_data_name}')\n",
        "\n",
        "  cl = tc.classification(train_data, test_data)\n",
        "  print(cl)\n",
        "\n",
        "test = main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N0y5IuxoHg31",
        "outputId": "0ae13e2b-fd83-431a-bd45-25716230d934"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                        ID  comedy  cult  flashback  \\\n",
            "0     9484ac61-0e30-4799-9998-6f74f4cbb204       0     0          0   \n",
            "1     55942d28-b6a2-4662-ab55-a66783a86a56       0     0          0   \n",
            "2     b71ed317-04cd-42f5-a380-d21dfea2bd36       0     0          0   \n",
            "3     5689b1b2-88cd-4c22-9114-0850ba539280       0     0          0   \n",
            "4     a0d9062e-f539-4043-bc9e-2a2ed589477b       0     0          0   \n",
            "...                                    ...     ...   ...        ...   \n",
            "1195  8978047a-ec54-412a-bcee-070fe1fb055c       0     1          0   \n",
            "1196  f1f04933-e298-4f65-bbeb-bc61a567a688       0     0          0   \n",
            "1197  a033955d-12c2-4549-bafd-ca8e84615f1b       0     0          0   \n",
            "1198  9464e84d-36b6-4b69-b0fb-f3c0546a8b10       0     0          0   \n",
            "1199  93ec8a32-0f64-4965-ba02-5b369ed16ca4       0     0          0   \n",
            "\n",
            "      historical  murder  revenge  romantic  scifi  violence  \n",
            "0              0       0        0         0      0         0  \n",
            "1              0       0        0         1      0         0  \n",
            "2              0       0        0         1      0         0  \n",
            "3              0       1        0         0      0         0  \n",
            "4              0       1        0         0      0         0  \n",
            "...          ...     ...      ...       ...    ...       ...  \n",
            "1195           0       0        0         0      0         1  \n",
            "1196           0       0        0         0      0         1  \n",
            "1197           0       0        0         0      0         1  \n",
            "1198           0       0        0         0      0         0  \n",
            "1199           0       0        1         0      0         0  \n",
            "\n",
            "[1200 rows x 10 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Deep Learning (Bi-LSTM)"
      ],
      "metadata": {
        "id": "eFZUX4-xH5GD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main() -> None:\n",
        "  tc = TextClassification()\n",
        "\n",
        "  tc.method = 1\n",
        "\n",
        "  files_path = './data/'\n",
        "  train_data_name = 'Training-dataset.csv'\n",
        "  test_data_name = 'Task-2-test-dataset1.csv'\n",
        "\n",
        "  train_data = pd.read_csv(f'{files_path}/{train_data_name}')\n",
        "  test_data = pd.read_csv(f'{files_path}/{test_data_name}')\n",
        "\n",
        "  cl = tc.deep_learning(train_data, test_data)\n",
        "  print(cl)\n",
        "\n",
        "test = main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eh5BqMaQHxOp",
        "outputId": "e35e8709-3308-468b-a2d5-7e8d88614764"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_17\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_18 (Embedding)    (None, 1200, 128)         13876736  \n",
            "                                                                 \n",
            " bidirectional_19 (Bidirect  (None, 1200, 256)         263168    \n",
            " ional)                                                          \n",
            "                                                                 \n",
            " dropout_30 (Dropout)        (None, 1200, 256)         0         \n",
            "                                                                 \n",
            " bidirectional_20 (Bidirect  (None, 128)               164352    \n",
            " ional)                                                          \n",
            "                                                                 \n",
            " dropout_31 (Dropout)        (None, 128)               0         \n",
            "                                                                 \n",
            " dense_20 (Dense)            (None, 9)                 1161      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14305417 (54.57 MB)\n",
            "Trainable params: 14305417 (54.57 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/5\n",
            "259/259 [==============================] - 66s 232ms/step - loss: 0.4608\n",
            "Epoch 2/5\n",
            "259/259 [==============================] - 53s 204ms/step - loss: 0.4221\n",
            "Epoch 3/5\n",
            "259/259 [==============================] - 49s 190ms/step - loss: 0.3704\n",
            "Epoch 4/5\n",
            "259/259 [==============================] - 45s 175ms/step - loss: 0.3051\n",
            "Epoch 5/5\n",
            "259/259 [==============================] - 44s 168ms/step - loss: 0.2446\n",
            "38/38 [==============================] - 3s 58ms/step\n",
            "                                        ID  comedy  cult  flashback  \\\n",
            "0     9484ac61-0e30-4799-9998-6f74f4cbb204       0     0          1   \n",
            "1     55942d28-b6a2-4662-ab55-a66783a86a56       0     0          0   \n",
            "2     b71ed317-04cd-42f5-a380-d21dfea2bd36       0     0          1   \n",
            "3     5689b1b2-88cd-4c22-9114-0850ba539280       0     0          0   \n",
            "4     a0d9062e-f539-4043-bc9e-2a2ed589477b       0     0          0   \n",
            "...                                    ...     ...   ...        ...   \n",
            "1195  8978047a-ec54-412a-bcee-070fe1fb055c       0     0          1   \n",
            "1196  f1f04933-e298-4f65-bbeb-bc61a567a688       0     1          0   \n",
            "1197  a033955d-12c2-4549-bafd-ca8e84615f1b       0     1          0   \n",
            "1198  9464e84d-36b6-4b69-b0fb-f3c0546a8b10       0     0          0   \n",
            "1199  93ec8a32-0f64-4965-ba02-5b369ed16ca4       0     0          1   \n",
            "\n",
            "      historical  murder  revenge  romantic  scifi  violence  \n",
            "0              0       0        0         0      0         0  \n",
            "1              0       1        0         1      0         0  \n",
            "2              0       0        0         1      0         0  \n",
            "3              0       1        0         0      0         0  \n",
            "4              0       1        0         0      0         0  \n",
            "...          ...     ...      ...       ...    ...       ...  \n",
            "1195           0       1        0         0      0         1  \n",
            "1196           0       0        0         0      0         0  \n",
            "1197           0       1        0         0      0         1  \n",
            "1198           0       1        1         0      0         1  \n",
            "1199           0       1        0         0      0         1  \n",
            "\n",
            "[1200 rows x 10 columns]\n"
          ]
        }
      ]
    }
  ]
}