{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# The validation data for method b (LSTM)"
      ],
      "metadata": {
        "id": "0OQQhbo3-YDy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4p5Ooi93d-wi",
        "outputId": "241f3753-9515-469d-f1b9-31bf5881ad5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "259/259 [==============================] - 43s 155ms/step - loss: 0.4734 - accuracy: 0.2551\n",
            "Epoch 2/5\n",
            "259/259 [==============================] - 41s 157ms/step - loss: 0.4393 - accuracy: 0.2868\n",
            "Epoch 3/5\n",
            "259/259 [==============================] - 40s 156ms/step - loss: 0.3955 - accuracy: 0.3420\n",
            "Epoch 4/5\n",
            "259/259 [==============================] - 39s 151ms/step - loss: 0.3460 - accuracy: 0.3999\n",
            "Epoch 5/5\n",
            "259/259 [==============================] - 41s 158ms/step - loss: 0.2954 - accuracy: 0.4747\n",
            "38/38 [==============================] - 2s 32ms/step\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Function to preprocess text data\n",
        "def clean_text(input_text):\n",
        "    tokens = word_tokenize(input_text)\n",
        "    lowercased_tokens = []\n",
        "    for word in tokens:\n",
        "        lowercased_tokens.append(word.lower())\n",
        "\n",
        "    alphabetic_words = []\n",
        "    for word in lowercased_tokens:\n",
        "        if word.isalpha():\n",
        "            alphabetic_words.append(word)\n",
        "\n",
        "    common_words = set(stopwords.words('english'))\n",
        "    meaningful_words = []\n",
        "    for word in alphabetic_words:\n",
        "        if word not in common_words:\n",
        "            meaningful_words.append(word)\n",
        "\n",
        "    stemmer = SnowballStemmer('english')\n",
        "    stemmed_words = []\n",
        "    for word in meaningful_words:\n",
        "        stemmed_word = stemmer.stem(word)\n",
        "        stemmed_words.append(stemmed_word)\n",
        "\n",
        "    processed_text = ' '.join(stemmed_words)\n",
        "    return processed_text\n",
        "\n",
        "\n",
        "training_data = pd.read_csv('./data/Training-dataset.csv')\n",
        "training_data['cleaned_plot'] = training_data['plot_synopsis'].apply(clean_text)\n",
        "text_tokenizer = Tokenizer(num_words=10000)\n",
        "text_tokenizer.fit_on_texts(training_data['cleaned_plot'])\n",
        "train_sequences = text_tokenizer.texts_to_sequences(training_data['cleaned_plot'])\n",
        "train_data_padded = pad_sequences(train_sequences, maxlen=200)\n",
        "target_labels = training_data.iloc[:, 3:12].values\n",
        "\n",
        "lstm_model = Sequential()\n",
        "lstm_model.add(Embedding(10000, 128, input_length=200))\n",
        "lstm_model.add(LSTM(64))\n",
        "lstm_model.add(Dense(target_labels.shape[1], activation='sigmoid'))\n",
        "lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "lstm_model.fit(train_data_padded, target_labels, epochs=5, batch_size=32)\n",
        "\n",
        "# Process the dataset\n",
        "dataset_path ='./data/Task-2-test-dataset1.csv'\n",
        "#dataset_path = './data/Task-2-validation-dataset.csv'\n",
        "data = pd.read_csv(dataset_path)\n",
        "data['cleaned_plot'] = data['plot_synopsis'].apply(clean_text)\n",
        "sequences = text_tokenizer.texts_to_sequences(data['cleaned_plot'])\n",
        "data_padded = pad_sequences(sequences, maxlen=200)\n",
        "\n",
        "predicted_probs = lstm_model.predict(data_padded)\n",
        "predicted_labels = (predicted_probs > 0.29).astype(int) #best value for the threshold\n",
        "prediction_df = pd.DataFrame(predicted_labels, columns=['comedy', 'cult', 'flashback', 'historical', 'murder', 'revenge', 'romantic', 'scifi', 'violence'])\n",
        "prediction_df.insert(0, 'ID', data['ID'])\n",
        "prediction_df.to_csv('10879201-Task2-method-b.csv', index=False, header=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The validation data for method a (SVM)"
      ],
      "metadata": {
        "id": "CQwqOIe5-2Jg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Function to preprocess text data\n",
        "def clean_text(input_text):\n",
        "    tokens = word_tokenize(input_text)\n",
        "    lowercased_tokens = []\n",
        "    for word in tokens:\n",
        "        lowercased_tokens.append(word.lower())\n",
        "\n",
        "    alphabetic_words = []\n",
        "    for word in lowercased_tokens:\n",
        "        if word.isalpha():\n",
        "            alphabetic_words.append(word)\n",
        "\n",
        "    common_words = set(stopwords.words('english'))\n",
        "    meaningful_words = []\n",
        "    for word in alphabetic_words:\n",
        "        if word not in common_words:\n",
        "            meaningful_words.append(word)\n",
        "\n",
        "    stemmer = SnowballStemmer('english')\n",
        "    stemmed_words = []\n",
        "    for word in meaningful_words:\n",
        "        stemmed_word = stemmer.stem(word)\n",
        "        stemmed_words.append(stemmed_word)\n",
        "\n",
        "    processed_text = ' '.join(stemmed_words)\n",
        "    return processed_text\n",
        "\n",
        "training_data = pd.read_csv('./data/Training-dataset.csv')\n",
        "training_data['cleaned_plot'] = training_data['plot_synopsis'].apply(clean_text)\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=10000, ngram_range=(1, 2))\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(training_data['cleaned_plot'])\n",
        "\n",
        "target_labels = training_data.iloc[:, 3:12].values\n",
        "svm_model = OneVsRestClassifier(SVC(kernel='linear', probability=True))\n",
        "svm_model.fit(X_train_tfidf, target_labels)\n",
        "\n",
        "#dataset_path = './data/Task-2-validation-dataset.csv'\n",
        "dataset_path = './data/Task-2-test-dataset1.csv'\n",
        "data = pd.read_csv(dataset_path)\n",
        "data['cleaned_plot'] = data['plot_synopsis'].apply(clean_text)\n",
        "X_tfidf = tfidf_vectorizer.transform(data['cleaned_plot'])\n",
        "\n",
        "predicted_labels = svm_model.predict(X_tfidf)\n",
        "prediction_df = pd.DataFrame(predicted_labels, columns=['comedy', 'cult', 'flashback', 'historical', 'murder', 'revenge', 'romantic', 'scifi', 'violence'])\n",
        "prediction_df.insert(0, 'ID', data['ID'])\n",
        "prediction_df.to_csv('10879201-Task2-method-a.csv', index=False, header=False)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qHHJ17D2-DLE",
        "outputId": "25303504-2cda-4011-e666-15533b0820fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    }
  ]
}