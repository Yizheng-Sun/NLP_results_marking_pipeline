{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jvAE2J6zlcPW",
        "outputId": "559be182-1c51-4698-a8f9-a9c8a8baa763"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Import necessary libaries\n",
        "import nltk,string,csv,re,time\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from gensim.models.phrases import Phrases\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"stopwords\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vudwOhaXld_R",
        "outputId": "2948d537-f936-478c-dd4a-ccf22aa093e8"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Constants for both BoW and Word2Vec\n",
        "STOP_WORDS = set(stopwords.words('english'))\n",
        "\n",
        "TRAINING_PATH = './data/Training-dataset.csv'\n",
        "VALID_DATA_PATH = './data/Task-1-validation-dataset.csv'\n",
        "TEST_DATA_PATH = './data/Task-1-test-dataset2.csv'\n",
        "\n",
        "TASK_A_VALID_OUTPUT_PATH = './data/10879475-Task1-method-a-validation.csv'\n",
        "TASK_A_TEST_OUTPUT_PATH = './data/10879475-Task1-method-a.csv'\n",
        "\n",
        "TASK_B_VALID_OUTPUT_PATH = './data/10879475-Task1-method-b-validation.csv'\n",
        "TASK_B_TEST_OUTPUT_PATH = './data/10879475-Task1-method-b.csv'\n",
        "\n",
        "LEMMATIZER = WordNetLemmatizer()\n",
        "\n",
        "\n",
        "class BoWTFIDF:\n",
        "  def __init__(self,training_corpus):\n",
        "    '''Initialise BoW with the training corpus such that a pre processing function is automatically called on the input value in order to clearn the corpus and this processed corpus is then stored as a field of the class'''\n",
        "    self.processed_corpus = training_corpus.apply(self.__pre_process_corpus__)\n",
        "\n",
        "  def __pre_process_corpus__(self,plot_document):\n",
        "    '''Expects a single document, it tokenises the document in to a list of tokens using word_tokenize from gensim, it then removes unncessary things like stopwords and punctuations\n",
        "    then it goes on to lemmatize the remaining tokens. It then joins the remaining tokens back and returns this'''\n",
        "    #Tokenize the text\n",
        "    tokens = word_tokenize(plot_document)\n",
        "\n",
        "    #Remove punctuation\n",
        "    tokens = [token.lower() for token in tokens if token not in STOP_WORDS and token not in string.punctuation]\n",
        "\n",
        "    #lemmatize after removing some words\n",
        "    tokens = [LEMMATIZER.lemmatize(token) for token in tokens]\n",
        "\n",
        "    #Lowercase text\n",
        "    processed_document = ' '.join(tokens)\n",
        "    return processed_document\n",
        "\n",
        "  def calcualte_similarity(self,word1, word2,tfidf,tfidf_matrix):\n",
        "    '''Finds the word index of the word in the vector representation for each of the 2 input terms, if it doens't exist, it gives a default value and hence manages to deal with OOVs\n",
        "    It then uses that index to find the word vector representation in the tfidfmatrix, transposes it and then uses the cosine simialrity function in order to find the similarity between the 2 terms'''\n",
        "    word1_ind = tfidf.vocabulary_.get(word1,-1)\n",
        "    word2_ind = tfidf.vocabulary_.get(word2,-1)\n",
        "    cosine_sim = cosine_similarity(tfidf_matrix[:,word1_ind].T,tfidf_matrix[:,word2_ind].T)\n",
        "    return cosine_sim[0][0]\n",
        "\n",
        "  def output_results_to_file(self,out_path,input_data_path,tf_idf,tf_idf_matrix):\n",
        "    '''Helper function in order to read in the testing data, process it similar to the training corpus data and then calculates the simialrity between the words in the testing data.\n",
        "    It then writes to a new csv file the id and the similarity score for each row in the testing data file'''\n",
        "    with open(out_path,'w',newline='') as output_csv:\n",
        "      csv_writer = csv.writer(output_csv)\n",
        "      with open(input_data_path) as f:\n",
        "        read = csv.reader(f)\n",
        "        for col in read:\n",
        "          id = col[0]\n",
        "          term1 = LEMMATIZER.lemmatize(col[1]).lower()\n",
        "          term2 = LEMMATIZER.lemmatize(col[2]).lower()\n",
        "          similarity = self.calcualte_similarity(term1,term2,tf_idf,tf_idf_matrix)\n",
        "          csv_writer.writerow([id,similarity])\n",
        "\n",
        "#Create class for Word2Vec implementation using skip gram\n",
        "class Word2VecSG:\n",
        "  def __init__(self,training_corpus):\n",
        "    '''Initialise Word2vecsg with the training corpus such that a pre processing function is automatically called on the input value in order to clean the corpus and\n",
        "     this processed corpus is then stored as a field of the class'''\n",
        "    self.processed_tokens = self.__tokenisation_and_pre_processing__(training_corpus)\n",
        "\n",
        "  def clean_document(self,document):\n",
        "    '''Cleans the text in the input document.\n",
        "    https://towardsdatascience.com/multi-label-text-classification-with-scikit-learn-30714b7819c5'''\n",
        "    #Normalise document to lower case\n",
        "    document = document.lower()\n",
        "    #Deal with common apostrophe phrases\n",
        "    document = re.sub(r\"what's\", \"what is \", document)\n",
        "    document = re.sub(r\"\\'s\", \" \", document)\n",
        "    document = re.sub(r\"\\'ve\", \" have \", document)\n",
        "    document = re.sub(r\"can't\", \"can not \", document)\n",
        "    document = re.sub(r\"n't\", \" not \", document)\n",
        "    document = re.sub(r\"i'm\", \"i am \", document)\n",
        "    document = re.sub(r\"\\'re\", \" are \", document)\n",
        "    document = re.sub(r\"\\'d\", \" would \", document)\n",
        "    document = re.sub(r\"\\'ll\", \" will \", document)\n",
        "    document = re.sub(r\"\\'scuse\", \" excuse \", document)\n",
        "    #Replace text with space for any regular expression pattern that matches any non-alphanumeric character (equivalent to [^a-zA-Z0-9_])\n",
        "    document = re.sub('\\W', ' ', document)\n",
        "    #Replace sequences of a more than single space with a single space\n",
        "    document = re.sub('\\s+', ' ', document)\n",
        "    #Remove leading or trailing spaces from the document text\n",
        "    document = document.strip(' ')\n",
        "    return document\n",
        "\n",
        "  def __tokenisation_and_pre_processing__(self,training_corpus):\n",
        "    '''Cleans the documents in the training corpus, tokenises each document and then removes punctuation and stopwords, it also adds in bigrams to the tokens and then returns this list of list of tokens'''\n",
        "    training_corpus['plot_synopsis'] = training_corpus['plot_synopsis'].map(lambda doc: self.clean_document(doc))\n",
        "    #Tokenize the each document,remove stopwords and lemmatize\n",
        "    tokens = [[LEMMATIZER.lemmatize(token) for token in word_tokenize(doc) if token not in STOP_WORDS] for doc in training_corpus['plot_synopsis']]\n",
        "    #Create bigrams from tokens\n",
        "    potential_bigrams = Phrases(tokens)\n",
        "    tokens_with_bigrams = list(potential_bigrams[tokens])\n",
        "    return tokens_with_bigrams\n",
        "\n",
        "  def train(self,model,vector_size,window_size,min_count,workers=4,epochs=2,sg=1):\n",
        "    '''Trains the word2vec model'''\n",
        "    #By default 2 epochs as it yields the best results.\n",
        "    model = Word2Vec(sentences=self.processed_tokens,vector_size=vector_size,window=window_size,sg=sg,min_count=min_count,epochs=epochs,workers=workers)\n",
        "    return model\n",
        "\n",
        "  def calcualte_similarity(self,word2vec_model,word1, word2):\n",
        "    '''Finds the vector representation of the terms from the model, if not found then gives a default representation,\n",
        "    Uses the vector representaiton of the 2 terms in order to find the cosine similairty between the 2 input terms'''\n",
        "    # Deal with OOVs and find term similarity\n",
        "    term1_vector = word2vec_model.wv[word1] if word1 in word2vec_model.wv else [0] * 300\n",
        "    term2_vector = word2vec_model.wv[word2] if word2 in word2vec_model.wv else [0] * 300\n",
        "    return cosine_similarity([term1_vector],[term2_vector])[0][0]\n",
        "\n",
        "  def process_test_terms(self,term1,term2):\n",
        "    '''Processes the test terms so that they are in the same format as the data that the model has been trained on. if a multi word term is foudn then it combine it using an underscore\n",
        "    as this is how bigram terms are stored in the tokens given by the tokenisation and pre processing function'''\n",
        "    term1 = self.clean_document(term1)\n",
        "    term2 = self.clean_document(term2)\n",
        "    word1_tokens = [LEMMATIZER.lemmatize(token) for token in word_tokenize(term1)]\n",
        "    word2_tokens = [LEMMATIZER.lemmatize(token) for token in word_tokenize(term2)]\n",
        "    # Join tokens with underscore if there's more than one word\n",
        "    word1_lemmatized = '_'.join(word1_tokens) if len(word1_tokens) > 1 else word1_tokens[0]\n",
        "    word2_lemmatized = '_'.join(word2_tokens) if len(word2_tokens) > 1 else word2_tokens[0]\n",
        "    return word1_lemmatized,word2_lemmatized\n",
        "\n",
        "  def output_results_to_file(self,word2vec_model,out_path,input_data_path):\n",
        "    '''Outputs results to new file after finding similarity between the 2 input terms from the file'''\n",
        "    with open(out_path,'w',newline='') as output_csv:\n",
        "      csv_writer = csv.writer(output_csv)\n",
        "      with open(input_data_path) as f:\n",
        "        read = csv.reader(f)\n",
        "        for col in read:\n",
        "          id = col[0]\n",
        "          #Process terms\n",
        "          processed_term1,processed_term2 = self.process_test_terms(col[1],col[2])\n",
        "          similarity =self.calcualte_similarity(word2vec_model,processed_term1,processed_term2)\n",
        "          csv_writer.writerow([id,similarity])\n"
      ],
      "metadata": {
        "id": "FM6tIiE7U2yH"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def BoW_experiment():\n",
        "  start_time = time.time()\n",
        "  #Extract data for relevant for BoW implementation\n",
        "  bow_training_corpus = pd.read_csv(TRAINING_PATH)['plot_synopsis']\n",
        "\n",
        "  #Instantiate BoW with TFIDF class. Pass in the training corpus which will be automatically be processed\n",
        "  BoW = BoWTFIDF(bow_training_corpus)\n",
        "\n",
        "  tfidf = TfidfVectorizer(ngram_range=(1,2))#introduce bigrams as well\n",
        "\n",
        "  # Get the matrix representation for the words in the corpus\n",
        "  tfidf_matrix = tfidf.fit_transform(BoW.processed_corpus)\n",
        "\n",
        "  # Validate on new words. Calculate cosine similarity and output results\n",
        "  BoW.output_results_to_file(TASK_A_VALID_OUTPUT_PATH,VALID_DATA_PATH,tfidf,tfidf_matrix)\n",
        "\n",
        "  # Test on new words. Calculate cosine similarity and output results\n",
        "  BoW.output_results_to_file(TASK_A_TEST_OUTPUT_PATH,TEST_DATA_PATH,tfidf,tfidf_matrix)\n",
        "\n",
        "  #Determine and print out time taken for BoW experiment from start to finish\n",
        "  end_time = time.time()\n",
        "  elapsed_time = end_time - start_time\n",
        "  time_taken_formatting(elapsed_time,\"BoW\")\n",
        "\n",
        "def Word2Vec_experiment():\n",
        "  start_time = time.time()\n",
        "  # Read in the training corpus\n",
        "  word2vec_training_corpus = pd.read_csv(TRAINING_PATH)\n",
        "  #Instantiate word2vec model with the relevant training corpus, This will automtically clean/pre_process the training data, it will also tokenise each document and this will be saved as a field(processed_tokens) of the object\n",
        "  word2vec = Word2VecSG(word2vec_training_corpus)\n",
        "\n",
        "  #Train the model by default will train for 2 epochs(default of Word2Vec from gensim is 5)\n",
        "  word2vec_sg_model = word2vec.train(word2vec,vector_size=300,window_size=5,min_count=1)\n",
        "\n",
        "  # Validate by finding cosine similarity on new words from validation data and ouput the results to a new file\n",
        "  word2vec.output_results_to_file(word2vec_sg_model,TASK_B_VALID_OUTPUT_PATH,VALID_DATA_PATH)\n",
        "\n",
        "  # Test by finding cosine similarity on new words from testing data and output the results into a new file\n",
        "  word2vec.output_results_to_file(word2vec_sg_model,TASK_B_TEST_OUTPUT_PATH,TEST_DATA_PATH)\n",
        "\n",
        "  #Determine time taken for Word2Vec experiment from start to finish\n",
        "  end_time = time.time()\n",
        "  elapsed_time = end_time - start_time\n",
        "  time_taken_formatting(elapsed_time,\"Word2Vec\")\n",
        "\n",
        "def time_taken_formatting(elapsed_time,exp_name):\n",
        "  '''Helper function in order to format the time taken into hours minutes seconds and milliseconds'''\n",
        "  hrs, remainder= divmod(elapsed_time,3600)\n",
        "  mins,remainder = divmod(remainder,60)\n",
        "  secs, ms = divmod(remainder,1)\n",
        "  formatted_time_printing = \"{:02} hrs {:02} mins {:02} secs {:3} ms\".format(int(hrs), int(mins), int(secs), round(ms * 1000))\n",
        "  print(f\"Time taken for {exp_name} experiment - {formatted_time_printing}\")"
      ],
      "metadata": {
        "id": "ur_0cZLztFUV"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "  BoW_experiment()\n",
        "  Word2Vec_experiment()"
      ],
      "metadata": {
        "id": "ZBhf5ezafL0M"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "id": "SS58lrckpPuS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}