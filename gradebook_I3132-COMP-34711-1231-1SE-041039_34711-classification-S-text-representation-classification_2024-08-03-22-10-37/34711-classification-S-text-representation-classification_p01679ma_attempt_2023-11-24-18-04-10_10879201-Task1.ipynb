{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Y9SKsDhJqgj"
      },
      "source": [
        "# The validation data and the test data for method a (PPMI)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whtKtGOqgP81",
        "outputId": "848e7513-78da-45ff-c2f5-7b4e1454a0e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 79144\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "import math\n",
        "import collections\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# preprocess text data\n",
        "def clean_text(input_text):\n",
        "    lowercased_text = input_text.lower()\n",
        "    tokens = word_tokenize(lowercased_text)\n",
        "\n",
        "    alphabetic_tokens = []\n",
        "    for token in tokens:\n",
        "        if token.isalpha():\n",
        "            alphabetic_tokens.append(token)\n",
        "\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = []\n",
        "    for token in alphabetic_tokens:\n",
        "        if token not in stop_words:\n",
        "            filtered_tokens.append(token)\n",
        "\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_tokens = []\n",
        "    for token in filtered_tokens:\n",
        "        lemmatized_token = lemmatizer.lemmatize(token)\n",
        "        lemmatized_tokens.append(lemmatized_token)\n",
        "\n",
        "    return lemmatized_tokens\n",
        "\n",
        "\n",
        "training_data_path = './data/Training-dataset.csv'\n",
        "training_data = pd.read_csv(training_data_path)\n",
        "training_data['processed_tokens'] = training_data['plot_synopsis'].apply(clean_text)\n",
        "\n",
        "window_size = 1\n",
        "co_occurrence = collections.defaultdict(lambda: collections.Counter())\n",
        "for plot in training_data['processed_tokens']:\n",
        "    for i in range(len(plot)):\n",
        "        token = plot[i]\n",
        "        for j in range(max(0, i - window_size), min(len(plot), i + window_size + 1)):\n",
        "            if i != j:\n",
        "                co_occurrence[token][plot[j]] += 1\n",
        "\n",
        "total_co_occurrences = sum(sum(counter.values()) for counter in co_occurrence.values())\n",
        "vocab_size = len(co_occurrence)\n",
        "ppmi_matrix = np.zeros((vocab_size, vocab_size))\n",
        "word_to_index = {word: i for i, word in enumerate(co_occurrence.keys())}\n",
        "print(\"Vocabulary size:\", vocab_size)\n",
        "for word, contexts in co_occurrence.items():\n",
        "    for context, count in contexts.items():\n",
        "        pmi = math.log2((count * total_co_occurrences) / (sum(co_occurrence[word].values()) * sum(co_occurrence[context].values())))\n",
        "        ppmi_matrix[word_to_index[word]][word_to_index[context]] = max(pmi, 0)\n",
        "\n",
        "#data_path = './data/Task-1-validation-dataset.csv'\n",
        "#data = pd.read_csv(data_path, header=None)\n",
        "#data.columns = ['id', 'term1', 'term2', 'similarity']\n",
        "data_path = './data/Task-1-test-dataset1.csv'\n",
        "data = pd.read_csv(data_path, header=None)\n",
        "data.columns = ['id', 'term1', 'term2']\n",
        "\n",
        "# cosine similarities\n",
        "ppmi_similarities = []\n",
        "for _, row in data.iterrows():\n",
        "    term1, term2 = row['term1'], row['term2']\n",
        "    similarity = 0\n",
        "    if term1 in word_to_index and term2 in word_to_index:\n",
        "        vec1 = ppmi_matrix[word_to_index[term1]]\n",
        "        vec2 = ppmi_matrix[word_to_index[term2]]\n",
        "        similarity = cosine_similarity([vec1], [vec2])[0][0]\n",
        "    ppmi_similarities.append(similarity)\n",
        "\n",
        "ppmi_results_df = pd.DataFrame({'term_pair_id': data['id'],'similarity': ppmi_similarities})\n",
        "ppmi_results_df.to_csv('10879201-Task1-method-a.csv', index=False, header=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpGm_XKqJ8dh"
      },
      "source": [
        "# The validation data and the test data method for b (word2voc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "wzVNL7VdKAay",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "077691cf-5517-41fb-e46f-3607a7fcc111"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# preprocess text data\n",
        "def clean_text(input_text):\n",
        "    lowercased_text = input_text.lower()\n",
        "    tokens = word_tokenize(lowercased_text)\n",
        "\n",
        "    alphabetic_tokens = []\n",
        "    for token in tokens:\n",
        "        if token.isalpha():\n",
        "            alphabetic_tokens.append(token)\n",
        "\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = []\n",
        "    for token in alphabetic_tokens:\n",
        "        if token not in stop_words:\n",
        "            filtered_tokens.append(token)\n",
        "\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_tokens = []\n",
        "    for token in filtered_tokens:\n",
        "        lemmatized_token = lemmatizer.lemmatize(token)\n",
        "        lemmatized_tokens.append(lemmatized_token)\n",
        "\n",
        "    return lemmatized_tokens\n",
        "\n",
        "training_data_path = './data/Training-dataset.csv'\n",
        "training_data = pd.read_csv(training_data_path)\n",
        "training_data['cleaned_plot'] = training_data['plot_synopsis'].apply(clean_text)\n",
        "tokenized_plots = training_data['cleaned_plot'].tolist()\n",
        "word2vec_model = Word2Vec(sentences=tokenized_plots, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "#data_path = './data/Task-1-validation-dataset.csv'\n",
        "#data = pd.read_csv(data_path, header=None)\n",
        "#data.columns = ['id', 'term1', 'term2', 'similarity']\n",
        "data_path = './data/Task-1-test-dataset1.csv'\n",
        "data = pd.read_csv(data_path, header=None)\n",
        "data.columns = ['id', 'term1', 'term2']\n",
        "\n",
        "word2vec_similarities = []\n",
        "for _, row in data.iterrows():\n",
        "    term1, term2 = row['term1'], row['term2']\n",
        "    similarity = 0\n",
        "\n",
        "    if term1 in word2vec_model.wv.key_to_index:\n",
        "        term1_emb = word2vec_model.wv[term1]\n",
        "    else:\n",
        "        term1_emb = np.zeros(word2vec_model.vector_size)\n",
        "\n",
        "    if term2 in word2vec_model.wv.key_to_index:\n",
        "        term2_emb = word2vec_model.wv[term2]\n",
        "    else:\n",
        "        term2_emb = np.zeros(word2vec_model.vector_size)\n",
        "\n",
        "    similarity = cosine_similarity([term1_emb], [term2_emb])[0][0]\n",
        "    word2vec_similarities.append(similarity)\n",
        "\n",
        "# Save the results to a CSV file\n",
        "word2vec_results_df = pd.DataFrame({'term_pair_id': data['id'], 'similarity': word2vec_similarities})\n",
        "word2vec_results_df.to_csv('10879201-Task1-method-b.csv', index=False, header=False)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}