{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Task 2: Text classification"
      ],
      "metadata": {
        "id": "v99IEYC1C2wx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing data for both methods"
      ],
      "metadata": {
        "id": "_Tl6_hEF9Tyt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "ynDq9epYfVHC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5e27be3-f6b4-41d8-e359-f1658d66e704"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.svm import LinearSVC\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "import string\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "\n",
        "# Download NLTK resources required for preproccessing\n",
        "nltk.download('punkt') # Tokenizer\n",
        "nltk.download('stopwords') # Stopwords list\n",
        "nltk.download('wordnet') # Lemmatizer\n",
        "\n",
        "# Initialize lemmatizer and stop words set\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Function to preprocess text\n",
        "def preprocess_text(text):\n",
        "    tokens = word_tokenize(text) # Tokenize text\n",
        "    lower = [w.lower() for w in tokens] # Convert to lower case\n",
        "    alphab = [word for word in lower if word.isalpha()] # Remove punctuation\n",
        "    stop = [w for w in alphab if not w in stop_words] # Filter out stop words\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(w) for w in stop] # Lemmatize the words\n",
        "    return ' '.join(lemmatized_tokens)\n",
        "\n",
        "# Load datasets\n",
        "train_data = pd.read_csv('./data/Training-dataset.csv')\n",
        "\n",
        "# Comment/ uncomment for validating code with development/test dataset#\n",
        "\n",
        "# validation_data = pd.read_csv('./data/Task-2-validation-dataset.csv')\n",
        "validation_data = pd.read_csv('./data/Task-2-test-dataset1.csv')\n",
        "\n",
        "# Preprocess text data in the 'plot_synopsis'\n",
        "train_data['processed_plot'] = train_data['plot_synopsis'].apply(preprocess_text)\n",
        "validation_data['processed_plot'] = validation_data['plot_synopsis'].apply(preprocess_text)\n",
        "\n",
        "# At this point the data is ready for creating representations for classifiers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1st method(option A): Developing traditional classification method with SVM"
      ],
      "metadata": {
        "id": "7_UFHNnd9haa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize TfidfVectorizer with maximum of 10,000 features\n",
        "# Consider both unigrams and bigrams when creating feature set which provide richer representation of text data by capturing context in which words appear together\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=10000, ngram_range=(1, 2))\n",
        "\n",
        "# Fit the vectorizer on processed text and transform data into TF-IDF vectors\n",
        "train_text = tfidf_vectorizer.fit_transform(train_data['processed_plot'])\n",
        "\n",
        "# Extract the target labels (classifications) from the training dataset\n",
        "train_labels = train_data.iloc[:, 3:12] # Assuming the labels are in columns 3 to 12\n",
        "\n",
        "# Initialize and train the MultiOutputClassifier with a LinearSVC, applying SVM with a linear kernel\n",
        "svm_multi_output_classifier = MultiOutputClassifier(\n",
        "    LinearSVC(\n",
        "        tol=1e-3,  # optimization process will consider itself at convergence\n",
        "                                     # if the change is less than 0.001 between iterations\n",
        "        max_iter=10000         # maximum number of passes over the data the optimization algorithm will\n",
        "                                     # take if it doesn't converge before reaching this number\n",
        "\n",
        "    )\n",
        ").fit(train_text, train_labels)  # Fit classifier to the TF-IDF transformed training data and labels"
      ],
      "metadata": {
        "id": "XvtqYBH83FT0"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Predicting on validation/test data for SVM"
      ],
      "metadata": {
        "id": "TXNAbmOP-YeY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform validation plot text into TF-IDF vectors\n",
        "validation_text = tfidf_vectorizer.transform(validation_data['processed_plot'])\n",
        "\n",
        "# Predict the classifications for the validation dataset\n",
        "validation_pred_labels = svm_multi_output_classifier.predict(validation_text)\n",
        "\n",
        "# Saving predictions to a CSV file\n",
        "\n",
        "# Creating DataFrame with the IDs and predicted labels\n",
        "svm_predicted_labels = pd.DataFrame(validation_pred_labels, columns=['comedy', 'cult', 'flashback', 'historical', 'murder', 'revenge', 'romantic', 'scifi', 'violence'])\n",
        "svm_predicted_labels.insert(0, 'ID', validation_data['ID'])\n",
        "svm_predicted_labels.to_csv('./data/10928627-Task2-method-a.csv', index=False, header=False)"
      ],
      "metadata": {
        "id": "jvZZ4Pb2u_My"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 2nd method(option B): Developing traditional deep learning method with LSTM"
      ],
      "metadata": {
        "id": "gzTz1eXK-i1m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train_data and validation_data are already loaded in the first cell and contain 'processed_plot' column which is preprocessed in first cell too\n",
        "# Define the tokenizer with a given vocabulary size\n",
        "# Initialize tokenizer to convert text into sequences of integers to prepare if for lstm model\n",
        "tokenizer = Tokenizer(num_words=10000) # num_words is max number of words to keep, which can be tuned\n",
        "tokenizer.fit_on_texts(train_data['processed_plot'])\n",
        "\n",
        "# Convert the training data into sequences\n",
        "train_seq = tokenizer.texts_to_sequences(train_data['processed_plot'])\n",
        "\n",
        "# Pad sequences to ensure consistent input size\n",
        "maxlen = 500  # parameter can be tuned based on length distribution of text data\n",
        "pad_train_data = pad_sequences(train_seq, maxlen=maxlen)\n",
        "\n",
        "train_label = train_data.iloc[:, 3:12].to_numpy()\n",
        "\n",
        "# Defining LSTM model architecture\n",
        "lstm_model = Sequential()\n",
        "lstm_model.add(Embedding(input_dim=10000, output_dim=128, input_length=maxlen)) # Embedding layer to convert integer sequences to dense vectors\n",
        "lstm_model.add(LSTM(64)) # LSTM layer with 64 units\n",
        "lstm_model.add(Dense(9, activation='sigmoid'))  # Output layer for 9 classes with sigmoid activation\n",
        "\n",
        "# Compile model with binary crossentropy loss function and the Adam optimizer\n",
        "lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train model on the padded training sequences and binary labels\n",
        "train_history = lstm_model.fit(\n",
        "    pad_train_data,\n",
        "    train_label,\n",
        "    batch_size=32, # Size of mini-batch for gradient descent, tuned\n",
        "    epochs=10 # Num of epochs to train the model, tuned\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u6l4T-2DxkA4",
        "outputId": "78fcb212-8858-4cb0-8621-9ba51fbbabab"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "259/259 [==============================] - 104s 386ms/step - loss: 0.4724 - accuracy: 0.2574\n",
            "Epoch 2/10\n",
            "259/259 [==============================] - 100s 387ms/step - loss: 0.4427 - accuracy: 0.2767\n",
            "Epoch 3/10\n",
            "259/259 [==============================] - 100s 386ms/step - loss: 0.3912 - accuracy: 0.3358\n",
            "Epoch 4/10\n",
            "259/259 [==============================] - 100s 387ms/step - loss: 0.3334 - accuracy: 0.4259\n",
            "Epoch 5/10\n",
            "259/259 [==============================] - 101s 388ms/step - loss: 0.2792 - accuracy: 0.4956\n",
            "Epoch 6/10\n",
            "259/259 [==============================] - 100s 386ms/step - loss: 0.2287 - accuracy: 0.5400\n",
            "Epoch 7/10\n",
            "259/259 [==============================] - 100s 386ms/step - loss: 0.1833 - accuracy: 0.5761\n",
            "Epoch 8/10\n",
            "259/259 [==============================] - 100s 387ms/step - loss: 0.1485 - accuracy: 0.6109\n",
            "Epoch 9/10\n",
            "259/259 [==============================] - 100s 385ms/step - loss: 0.1281 - accuracy: 0.6237\n",
            "Epoch 10/10\n",
            "259/259 [==============================] - 100s 385ms/step - loss: 0.1001 - accuracy: 0.6293\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Predicting on validation/test data for LSTM"
      ],
      "metadata": {
        "id": "iot_qrMe-67b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "validation_seq = tokenizer.texts_to_sequences(validation_data['processed_plot'])\n",
        "pad_validation_data = pad_sequences(validation_seq, maxlen=maxlen)\n",
        "\n",
        "# Predict on validation data\n",
        "validation_pred_label = lstm_model.predict(pad_validation_data)\n",
        "\n",
        "# Convert probabilities to binary labels on a threshold\n",
        "threshold = 0.5 # tuned\n",
        "binary_label = (validation_pred_label >= threshold).astype(int)\n",
        "\n",
        "# Saving predictions to a CSV file\n",
        "\n",
        "# Creating DataFrame with the IDs and predicted labels\n",
        "lstm_pred_labels = pd.DataFrame(binary_label, columns=['comedy', 'cult', 'flashback', 'historical', 'murder', 'revenge', 'romantic', 'scifi', 'violence'])\n",
        "lstm_pred_labels.insert(0, 'ID', validation_data['ID'])\n",
        "lstm_pred_labels.to_csv('./data/10928627-Task2-method-b.csv', index=False, header=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3FwsrtzxIT9",
        "outputId": "d573aaff-0220-4a78-c82d-9048fdc500fd"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "38/38 [==============================] - 3s 88ms/step\n"
          ]
        }
      ]
    }
  ]
}