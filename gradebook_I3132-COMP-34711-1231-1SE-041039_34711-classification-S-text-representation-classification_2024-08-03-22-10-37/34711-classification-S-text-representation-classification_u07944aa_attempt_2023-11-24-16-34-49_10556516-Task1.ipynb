{"cells":[{"cell_type":"markdown","metadata":{"id":"JcMjLOfOUkrb"},"source":["# Imports"]},{"cell_type":"markdown","source":["### Sources\n","https://scikit-learn.org/ <br>\n","https://www.nltk.org/ <br>\n","https://docs.python.org/3/library/re.html <br>\n","https://pandas.pydata.org/ <br>"],"metadata":{"id":"b3K89gQRIqXy"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":262,"status":"ok","timestamp":1700770436922,"user":{"displayName":"Aditya Agarwal","userId":"14756602227534470489"},"user_tz":0},"id":"d3dm7H-h8xsI","outputId":"9b5f7b7e-fb5a-4876-9834-24434c2e7178"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":59}],"source":["import pandas as pd\n","import nltk\n","from nltk.stem import PorterStemmer\n","from nltk.corpus import stopwords\n","import re\n","nltk.download(\"punkt\") # Download 'punkt' package which is a tokenizer model used to divide text into a list of sentences or words\n","nltk.download(\"wordnet\") # Download 'wordnet' which is a large lexical database of English used by the WordNetLemmatizer\n","nltk.download(\"stopwords\") # Download 'stopwords' which contains lists of stopwords for various languages\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2tASQQZj-BSh"},"outputs":[],"source":["file_path = './data/Training-dataset.csv'\n","data = pd.read_csv(file_path)\n","# Combining title and plot synopsis\n","data['combined_text'] = data['title'] + \" \" + data['plot_synopsis']"]},{"cell_type":"markdown","metadata":{"id":"crZmdJlVUhR_"},"source":["# Data Cleaning and Pre-processing"]},{"cell_type":"markdown","metadata":{"id":"qWF05DqFUsVR"},"source":["### Stemming (not used in this code)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Sq0QAkiT7zoh"},"outputs":[],"source":["# stemmer = PorterStemmer()\n","# english_stopwords = set(stopwords.words('english'))\n","# for i in range(len(data)):\n","#   document_words = nltk.word_tokenize(data.loc[i, 'combined_text'])\n","#   document_words = [stemmer.stem(word) for word in document_words if word.lower() not in english_stopwords]\n","#   data.loc[i, 'combined_text'] = ' '.join(document_words)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JPksJQg0CkIX"},"outputs":[],"source":["from nltk.stem import WordNetLemmatizer\n","english_stopwords = set(stopwords.words('english'))\n","lemmatizer = WordNetLemmatizer()\n","\n","def preprocess_and_lemmatize(word):\n","    document_words = re.sub('[^a-zA-Z]', ' ', word)                     # Remove non-alphabetical characters\n","    document_words = document_words.lower()                             # Case-folding\n","    document_words = nltk.word_tokenize(document_words)                 # Splits string text into words and punctuation\n","    document_words = [lemmatizer.lemmatize(w) for w in document_words if w not in english_stopwords] # Removes stopwords and lemmatizes the words\n","    return document_words, ' '.join(document_words)                     # Combine back to string and return both list of tokens and the combined string\n","\n","\n","for i in range(len(data)):\n","  # Process the text\n","  list_of_words, combined_text = preprocess_and_lemmatize(data.loc[i, 'combined_text'])\n","  data.at[i, 'normalized_list_of_words']  = \"\"\n","  # Assign the processed values to the DataFrame\n","  data.at[i, 'normalized_list_of_words'] = list_of_words\n","  data.at[i, 'normalized_combined_text'] = combined_text"]},{"cell_type":"markdown","source":["# **Task 1**"],"metadata":{"id":"KsChBergE_Cr"}},{"cell_type":"markdown","source":["### Sources\n","https://youtu.be/fM4qTMfCoak?si=WdDeyiYYDcpmQZ8j <br>\n","https://youtu.be/6ZVf1jnEKGI?si=i8MXzWCCzat6Jkpn <br>\n","https://youtu.be/JpxCt3kvbLk?si=-__OTDBXHf8PSSdl <br>\n","https://youtu.be/1OMmbtVmmbg?si=c5NG8VMk54xEw4TU <br>\n","https://youtu.be/cqcUk6hC5hk?si=6LbjPE0nyRsJf62s <br>\n","https://youtu.be/IKgBLTeQQL8?si=auBb3TbPjh8o1npr <br>\n","https://youtu.be/iu2-G_5YkEo?si=qdGAoshr00dXgmI6 <br>\n","https://youtu.be/D2V1okCEsiE?si=1ifhbqkMQvnL6noQ <br>\n","https://youtu.be/z9myrLOF_1M?si=l8kTT6l9uoBLtsGn <br>\n"],"metadata":{"id":"HmVKbcw0ttFy"}},{"cell_type":"markdown","metadata":{"id":"Y2Z4AGWFYcFP"},"source":["# Approach 1: TF-IDF"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eSpRrpvhcGqV"},"outputs":[],"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","tfidf_vectorizer_normalized = TfidfVectorizer()\n","tfidf_vectorizer_normalized_matrix = tfidf_vectorizer_normalized.fit_transform(data['normalized_combined_text'])  # Fit and Transform the vectorizer on the normalized training data\n"]},{"cell_type":"code","source":["features_names = tfidf_vectorizer_normalized.get_feature_names_out()\n","tfidf_vectorizer_normalized_matrix_array= tfidf_vectorizer_normalized_matrix.toarray()"],"metadata":{"id":"n4qr34neeMyj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f2PDyvUvctEp"},"source":["# Cosine Similarity"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gJC3QdPj_Gge"},"outputs":[],"source":["from sklearn.metrics.pairwise import cosine_similarity\n","import numpy as np\n","\n","def average_embedding(words):\n","    embeddings = []\n","    for word in words:\n","        try:\n","            if word in features_names:\n","                embedding = tfidf_vectorizer_normalized_matrix_array[:, np.where(word == features_names)]\n","                embeddings.append(embedding)\n","            else:\n","                continue\n","        except:\n","            continue\n","    if len(embeddings) == 0:\n","        return None\n","    return np.mean(embeddings, axis=0)\n","\n","\n","\n","def calculate_similarity(word1, word2, verbose = False):\n","      # Preprocess the words\n","      word1_processed = preprocess_and_lemmatize(word1)[0]\n","      word2_processed = preprocess_and_lemmatize(word2)[0]\n","\n","      # Get average embeddings for each word group\n","      tfidf_vec1 = average_embedding(word1_processed)\n","      tfidf_vec2 = average_embedding(word2_processed)\n","\n","      # Check if embeddings are found\n","      if tfidf_vec1 is None or tfidf_vec2 is None:\n","          return 0\n","\n","      # Calculate and print similarity\n","      similarity = cosine_similarity(tfidf_vec1.reshape(1,-1), tfidf_vec2.reshape(1,-1))[0][0]\n","      if verbose:\n","        print(\"Cosine similarity with normalized data vectorizer: \" + str(similarity))\n","      return similarity\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":297,"status":"ok","timestamp":1700769707179,"user":{"displayName":"Aditya Agarwal","userId":"14756602227534470489"},"user_tz":0},"id":"aXf9SS9wUWYI","outputId":"22d7f25a-976c-4f60-a3c1-42a05582dd33"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cosine similarity with normalized data vectorizer: 0.12112313741514513\n"]},{"output_type":"execute_result","data":{"text/plain":["0.12112313741514513"]},"metadata":{},"execution_count":56}],"source":["# Example usage\n","calculate_similarity(\"area\", \"region\", verbose=True)"]},{"cell_type":"markdown","source":["### Development Dataset"],"metadata":{"id":"NUTowmfcnlPH"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"4SZUSIajA0AT"},"outputs":[],"source":["# Initialize an empty list to store the results\n","results = []\n","\n","# Read the validation dataset into a DataFrame without headers\n","validation_set = pd.read_csv(\"./data/Task-1-validation-dataset.csv\", header=None)\n","\n","# Iterate over each row in the validation set\n","for index, row in validation_set.iterrows():\n","    # Append a dictionary with term_pair_id and calculated similarity to the results list\n","    results.append({'term_pair_id': row[0], 'similarity': calculate_similarity(row[1], row[2])})\n","\n","# Convert the results list into a DataFrame and Save the DataFrame to a CSV\n","results_df = pd.DataFrame(results)\n","results_df.to_csv('10556516-Task1-method-a.csv', index=False, header=False)\n"]},{"cell_type":"markdown","source":["### Test Dataset"],"metadata":{"id":"E9JMmHcFnjv9"}},{"cell_type":"code","source":["# Initialize an empty list to store the results\n","results = []\n","\n","# Read the validation dataset into a DataFrame without headers\n","validation_set = pd.read_csv(\"Task-1-test-dataset1.csv\", header=None)\n","\n","# Iterate over each row in the validation set\n","for index, row in validation_set.iterrows():\n","    # Append a dictionary with term_pair_id and calculated similarity to the results list\n","    results.append({'term_pair_id': row[0], 'similarity': calculate_similarity(row[1], row[2])})\n","\n","# Convert the results list into a DataFrame and Save the DataFrame to a CSV\n","results_df = pd.DataFrame(results)\n","results_df.to_csv('10556516-Task1-method-a-test.csv', index=False, header=False)\n"],"metadata":{"id":"lLSDdR-pfkHN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PR7aE9tyYzMd"},"source":["# Approach 2: Word2Vec"]},{"cell_type":"markdown","source":["### Sources\n","https://github.com/krishnaik06/Stock-Sentiment-Analysis <br>\n","https://youtu.be/Otde6VGvhWM?si=mxnIu4mtGM4BZx79 <br>\n","https://youtu.be/h-LGjJ_oANs?si=GARBUzl_P6N2GOYV <br>\n"],"metadata":{"id":"sy83lMwhuK3G"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"T9USLMBDY0fq"},"outputs":[],"source":["from gensim.models import Word2Vec"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-Bj4VXJnkHO9"},"outputs":[],"source":["# Training the Word2Vec model\n","word2vecmodel = Word2Vec(data['normalized_list_of_words'], min_count=1, vector_size=100, window=5, workers=4)"]},{"cell_type":"markdown","source":["# Cosine Similarity"],"metadata":{"id":"Nr0qlNzpFPZs"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"TmE6oqSWkxI2"},"outputs":[],"source":["def get_vector(word):\n","    # Check if the word is in the vocabulary\n","    if word in word2vecmodel.wv:\n","        # If the word exists in the Word2Vec model's vocabulary, return its vector\n","        return word2vecmodel.wv[word]\n","    else:\n","        # If the word does not exist in the vocabulary, return None\n","        return None\n","\n","def calculate_average_vector(words):\n","    # Split the input string into individual words and retrieve their vectors\n","    vectors = [get_vector(word) for word in words.split()]\n","    # Filter out None values in case some words are not in the vocabulary\n","    vectors = [vec for vec in vectors if vec is not None]\n","    # If there are valid vectors, calculate and return the average vector\n","    if len(vectors) > 0:\n","        return np.mean(vectors, axis=0)\n","    else:\n","        return None\n","\n","def calculate_word2vec_similarity(term1, term2):\n","    # Calculate the average vectors for each term\n","    vec1 = calculate_average_vector(term1)\n","    vec2 = calculate_average_vector(term2)\n","\n","    # If vectors for both terms are found, calculate cosine similarity\n","    if vec1 is not None and vec2 is not None:\n","        return cosine_similarity([vec1], [vec2])[0][0]\n","    else:\n","        return 0  # One or both terms do not have vectors"]},{"cell_type":"code","source":["print(calculate_word2vec_similarity(\"area\", \"region area\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IU7qrUqy4zei","executionInfo":{"status":"ok","timestamp":1700771877167,"user_tz":0,"elapsed":5,"user":{"displayName":"Aditya Agarwal","userId":"14756602227534470489"}},"outputId":"37efeea2-bc28-4826-c27a-666d72ea2a5e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0.95567155\n"]}]},{"cell_type":"markdown","source":["### Development Dataset"],"metadata":{"id":"VlCyg98Tneb5"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"UVldWuaBpdUa"},"outputs":[],"source":["# Initialize an empty list to store the results\n","results = []\n","\n","# Read the validation dataset into a DataFrame without headers\n","validation_set = pd.read_csv(\"./data/Task-1-validation-dataset.csv\", header=None)\n","\n","# Iterate over each row in the validation set\n","for index, row in validation_set.iterrows():\n","    results.append({'term_pair_id': row[0], 'similarity': calculate_word2vec_similarity(row[1], row[2])})\n","\n","# Convert the results list into a DataFrame\n","results_df = pd.DataFrame(results)\n","\n","# Save the DataFrame to a CSV file without a header\n","results_df.to_csv('10556516-Task1-method-b.csv', index=False, header=False)"]},{"cell_type":"markdown","source":["### Test Dataset"],"metadata":{"id":"FcYVHetYnh2a"}},{"cell_type":"code","source":["# Initialize an empty list to store the results\n","results = []\n","\n","# Read the validation dataset into a DataFrame without headers\n","validation_set = pd.read_csv(\"./data/Task-1-test-dataset1.csv\", header=None)\n","\n","# Iterate over each row in the validation set\n","for index, row in validation_set.iterrows():\n","    results.append({'term_pair_id': row[0], 'similarity': calculate_word2vec_similarity(row[1], row[2])})\n","\n","# Convert the results list into a DataFrame\n","results_df = pd.DataFrame(results)\n","\n","# Save the DataFrame to a CSV file without a header\n","results_df.to_csv('10556516-Task1-method-b-test.csv', index=False, header=False)"],"metadata":{"id":"HSqzG3VRe0W0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"LsVAy-vFf6td"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOwHy1rbngfYdthxG/+/Pcd"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}