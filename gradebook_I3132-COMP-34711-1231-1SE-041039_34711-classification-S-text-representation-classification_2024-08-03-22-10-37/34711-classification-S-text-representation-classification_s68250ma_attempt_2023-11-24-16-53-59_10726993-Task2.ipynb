{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **NOTEBOOK WAS RUN USING THE T4 GPU**, so all timings in the questionnaire is from this GPU"
      ],
      "metadata": {
        "id": "S4IHBDPGVepK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Necessary Libraries"
      ],
      "metadata": {
        "id": "-dPqJcKzAxA7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFCVuRe54DIt",
        "outputId": "fda35e9b-bb53-416d-bdb9-e4d85ea7f415"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Imported all necessary libraries successfully\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import time\n",
        "import tqdm\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
        "import string\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "from tensorflow import keras\n",
        "from keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional, GlobalMaxPool1D, BatchNormalization\n",
        "from keras.models import Sequential\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.optimizers import Adam\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "# Load nltk resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "print(\"Imported all necessary libraries successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_WYV3VGD8px"
      },
      "source": [
        "# Data Processing (Cleaning & Preprocessing)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2UmagItGD9he"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text, remove_stopwords=True, lemmatize=True, stem=False):\n",
        "\n",
        "   # Set up stop words, lemmatizer, and stemmer\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    stemmer = SnowballStemmer(\"english\")\n",
        "\n",
        "    # Tokenize text\n",
        "    tokens = word_tokenize(text)\n",
        "    # Convert to lowercase and remove non-alphanumeric tokens i.e. punctuation\n",
        "    tokens = [token.lower() for token in tokens if token.isalnum()]\n",
        "    # Remove stopwords\n",
        "    if remove_stopwords:\n",
        "        tokens = [token for token in tokens if token not in stop_words]\n",
        "    # Lemmatize\n",
        "    if lemmatize:\n",
        "        # Since the lemmatizer as its default pos is noun, then the lemma of media is medium so it gets removed from the corpus\n",
        "        tokens = [lemmatizer.lemmatize(token) if token != 'media' else token for token in tokens]\n",
        "    # Stem\n",
        "    if stem:\n",
        "        tokens = [stemmer.stem(token) for token in tokens]\n",
        "\n",
        "    # Join tokens back into a single string\n",
        "    processed_text = \" \".join(tokens)\n",
        "\n",
        "    return processed_text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preparation"
      ],
      "metadata": {
        "id": "hEZC9nftBYOX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Data"
      ],
      "metadata": {
        "id": "KDzFIXTNBaOU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "AJRKuoXVD-md"
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "training_dataset_path = './data/Training-dataset.csv'\n",
        "training_data = pd.read_csv(training_dataset_path)\n",
        "\n",
        "# Apply preprocessing to each plot synopsis\n",
        "training_data['processed_plot_synopsis'] = training_data['plot_synopsis'].apply(preprocess_text, stem=False)\n",
        "\n",
        "genres_columns = ['comedy', 'cult', 'flashback', 'historical', 'murder', 'revenge', 'romantic', 'scifi', 'violence']\n",
        "\n",
        "train_synopses = training_data['processed_plot_synopsis']\n",
        "train_labels = training_data[genres_columns].values"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Validation Data"
      ],
      "metadata": {
        "id": "5qS66JW3Binl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "validation_dataset_path = './data/Task-2-validation-dataset.csv'\n",
        "validation_data = pd.read_csv(validation_dataset_path)\n",
        "# Apply preprocessing to each plot synopsis\n",
        "validation_data['processed_plot_synopsis'] = validation_data['plot_synopsis'].apply(preprocess_text, stem=False)\n",
        "\n",
        "val_synopses = validation_data['processed_plot_synopsis']\n",
        "val_labels = validation_data[genres_columns].values"
      ],
      "metadata": {
        "id": "90ssgxCi5vxd"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing Data"
      ],
      "metadata": {
        "id": "Q745YHPlBrUm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "testing_dataset_path = './data/Task-2-test-dataset1.csv'\n",
        "testing_data = pd.read_csv(testing_dataset_path)\n",
        "# Apply preprocessing to each plot synopsis\n",
        "testing_data['processed_plot_synopsis'] = testing_data['plot_synopsis'].apply(preprocess_text, stem=False)\n",
        "\n",
        "test_synopses = testing_data['processed_plot_synopsis']"
      ],
      "metadata": {
        "id": "Y9Z3eHnK6RUA"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_prediction_csv(predicted_labels, method, dataset):\n",
        "  if dataset == 'validation':\n",
        "    prediction_data = validation_data.copy()\n",
        "    prediction_data.drop(prediction_data.columns[1:], axis=1, inplace=True)\n",
        "    prediction_data[genres_columns] = predicted_labels\n",
        "    # Save the new dataset to a CSV file\n",
        "    prediction_path = f'./data/10726993-Task2-method-{method}-validation.csv'\n",
        "    prediction_data.to_csv(prediction_path, index=False, header=False)\n",
        "  if dataset == 'testing':\n",
        "    prediction_data = testing_data.copy()\n",
        "    prediction_data.drop(prediction_data.columns[1:], axis=1, inplace=True)\n",
        "    prediction_data[genres_columns] = predicted_labels\n",
        "    # Save the new dataset to a CSV file\n",
        "    prediction_path = f'./data/10726993-Task2-method-{method}.csv'\n",
        "    prediction_data.to_csv(prediction_path, index=False, header=False)"
      ],
      "metadata": {
        "id": "UXJf60HD6CA7"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qkywOtB13td"
      },
      "source": [
        "# Method A - SVM"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "train_synopses_tfidf = tfidf_vectorizer.fit_transform(train_synopses)\n",
        "val_synopses_tfidf = tfidf_vectorizer.transform(val_synopses)\n",
        "test_synopses_tfidf = tfidf_vectorizer.transform(test_synopses)"
      ],
      "metadata": {
        "id": "LoPyaYGa6427"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_synopses_tfidf.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_V4uEwhKuTfv",
        "outputId": "97968eb6-737d-45ee-b4d4-3d1b84f94ed3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1200, 80387)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "kwcOmoTx2mhZ"
      },
      "outputs": [],
      "source": [
        "# Initialize SVM with OneVsRestClassifier\n",
        "svm_classifier = OneVsRestClassifier(SVC(kernel='linear', probability=True))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train\n",
        "svm_classifier.fit(train_synopses_tfidf, train_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "id": "CMu64WyAanPc",
        "outputId": "175fcdb4-d55c-4bcd-8e73-1cf96f4b0658"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OneVsRestClassifier(estimator=SVC(kernel='linear', probability=True))"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>OneVsRestClassifier(estimator=SVC(kernel=&#x27;linear&#x27;, probability=True))</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OneVsRestClassifier</label><div class=\"sk-toggleable__content\"><pre>OneVsRestClassifier(estimator=SVC(kernel=&#x27;linear&#x27;, probability=True))</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(kernel=&#x27;linear&#x27;, probability=True)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(kernel=&#x27;linear&#x27;, probability=True)</pre></div></div></div></div></div></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict validation dataset\n",
        "pred_prob_val = svm_classifier.predict_proba(val_synopses_tfidf)"
      ],
      "metadata": {
        "id": "yielTuPDyZhn"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_curve, f1_score\n",
        "\n",
        "n_labels = val_labels.shape[1]\n",
        "precision_dict = {}\n",
        "recall_dict = {}\n",
        "f1_scores = []\n",
        "optimal_thresholds = []\n",
        "for i in range(n_labels):\n",
        "    # Calculate precision and recall for each label\n",
        "    precision_dict[i], recall_dict[i], thresholds = precision_recall_curve(val_labels[:, i], pred_prob_val[:, i])\n",
        "\n",
        "\n",
        "    # Find optimal threshold for F1 score for each label\n",
        "    f1_score_values = 2 * (precision_dict[i] * recall_dict[i]) / (precision_dict[i] + recall_dict[i])\n",
        "    optimal_idx = np.nanargmax(f1_score_values)  # nanargmax handles NaN values\n",
        "    optimal_threshold = thresholds[optimal_idx] if optimal_idx < len(thresholds) else 1.0\n",
        "    optimal_thresholds.append(optimal_threshold)\n",
        "    f1_scores.append(f1_score_values[optimal_idx])\n",
        "\n",
        "    print(f\"Label {i}: Optimal Threshold: {optimal_threshold}, F1 Score: {f1_score_values[optimal_idx]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Rdbuik8zBau",
        "outputId": "143350e7-c8dd-4b2e-d487-86956f09696d"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label 0: Optimal Threshold: 0.1519856575255218, F1 Score: 0.39441535776614306\n",
            "Label 1: Optimal Threshold: 0.23881594070644435, F1 Score: 0.46504065040650405\n",
            "Label 2: Optimal Threshold: 0.22301939363474063, F1 Score: 0.45223700120918986\n",
            "Label 3: Optimal Threshold: 0.24684111920397453, F1 Score: 0.2777777777777778\n",
            "Label 4: Optimal Threshold: 0.28829850282193564, F1 Score: 0.7407932011331446\n",
            "Label 5: Optimal Threshold: 0.19466419720471428, F1 Score: 0.43543543543543545\n",
            "Label 6: Optimal Threshold: 0.3892415826721485, F1 Score: 0.6198198198198198\n",
            "Label 7: Optimal Threshold: 0.15691063313922315, F1 Score: 0.3859649122807018\n",
            "Label 8: Optimal Threshold: 0.302798735529674, F1 Score: 0.6647173489278753\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimal_thresholds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GmqJ_8N84_gv",
        "outputId": "be73b60f-324e-407c-e919-4651777fcf01"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.1519856575255218,\n",
              " 0.23881594070644435,\n",
              " 0.22301939363474063,\n",
              " 0.24684111920397453,\n",
              " 0.28829850282193564,\n",
              " 0.19466419720471428,\n",
              " 0.3892415826721485,\n",
              " 0.15691063313922315,\n",
              " 0.302798735529674]"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_thresholds(probabilities, thresholds):\n",
        "  # Ensure probabilities and thresholds are numpy arrays\n",
        "    probabilities = np.array(probabilities)\n",
        "    thresholds = np.array(thresholds)\n",
        "\n",
        "    # Initialize an array to store the predicted labels\n",
        "    labels = np.zeros_like(probabilities, dtype=int)\n",
        "\n",
        "    # Apply each threshold to the corresponding column in the probabilities array\n",
        "    for i, threshold in enumerate(thresholds):\n",
        "        labels[:, i] = (probabilities[:, i] >= threshold).astype(int)\n",
        "\n",
        "    return labels"
      ],
      "metadata": {
        "id": "zSRDIcEd1tPT"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict validation dataset\n",
        "pred_labels_val = apply_thresholds(pred_prob_val, optimal_thresholds)"
      ],
      "metadata": {
        "id": "_rrkAGcRZx6B"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict testing dataset\n",
        "pred_prob_test = svm_classifier.predict_proba(test_synopses_tfidf)\n",
        "pred_labels_test = apply_thresholds(pred_prob_test, optimal_thresholds)"
      ],
      "metadata": {
        "id": "hZtceFWOZ0K5"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "P3Bzh5WiICRs"
      },
      "outputs": [],
      "source": [
        "create_prediction_csv(pred_labels_val, \"a\", \"validation\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "create_prediction_csv(pred_labels_test, \"a\", \"testing\")"
      ],
      "metadata": {
        "id": "rN5nuvQowjLg"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Results SVM with linear kernel"
      ],
      "metadata": {
        "id": "kqfM7fwMnWk1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(val_labels, pred_labels_val, target_names=genres_columns))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJXuHgEt2Tho",
        "outputId": "bf5534b5-d4d4-42e6-affc-d863cd4fb0d0"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "      comedy       0.28      0.65      0.39       175\n",
            "        cult       0.39      0.58      0.47       247\n",
            "   flashback       0.35      0.64      0.45       294\n",
            "  historical       0.42      0.21      0.28        24\n",
            "      murder       0.63      0.90      0.74       581\n",
            "     revenge       0.34      0.61      0.44       237\n",
            "    romantic       0.65      0.59      0.62       290\n",
            "       scifi       0.42      0.35      0.39        31\n",
            "    violence       0.56      0.81      0.66       420\n",
            "\n",
            "   micro avg       0.47      0.71      0.57      2299\n",
            "   macro avg       0.45      0.59      0.49      2299\n",
            "weighted avg       0.50      0.71      0.58      2299\n",
            " samples avg       0.50      0.72      0.54      2299\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%run task2_eval_script_student_version.py ./data/10726993-Task2-method-a-validation.csv ./data/Task-2-validation-dataset.csv\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8B1_vNlf2DqL",
        "outputId": "29f888fd-a91e-424c-b115-3ac0aba591f2"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class level: \n",
            "Class  1 precision: 0.2839 recall: 0.6457 f1:  0.3944\n",
            "Class  2 precision: 0.3886 recall: 0.5789 f1:  0.4650\n",
            "Class  3 precision: 0.3508 recall: 0.6361 f1:  0.4522\n",
            "Class  4 precision: 0.4167 recall: 0.2083 f1:  0.2778\n",
            "Class  5 precision: 0.6294 recall: 0.9002 f1:  0.7408\n",
            "Class  6 precision: 0.3380 recall: 0.6118 f1:  0.4354\n",
            "Class  7 precision: 0.6491 recall: 0.5931 f1:  0.6198\n",
            "Class  8 precision: 0.4231 recall: 0.3548 f1:  0.3860\n",
            "Class  9 precision: 0.5627 recall: 0.8119 f1:  0.6647\n",
            "----------------------------\n",
            "Movie (document) level: \n",
            "Precision: 0.4984\n",
            "Recall: 0.7203\n",
            "F1: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/task2_eval_script_student_version.py:60: RuntimeWarning: invalid value encountered in divide\n",
            "  movie_level_f1 = (2*movie_level_precision*movie_level_recall)/(movie_level_precision*movie_level_recall)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILQ3GVhpAtXb"
      },
      "source": [
        "# Method B - Bi-direcitonal LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "2v8D9dSpK0Kl"
      },
      "outputs": [],
      "source": [
        "# Initialize CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Fit the vectorizer to the text data\n",
        "vectorizer.fit(train_synopses)\n",
        "\n",
        "# Retrieve the vocabulary and determine its size\n",
        "vocabulary = vectorizer.vocabulary_\n",
        "vocabulary_size = len(vocabulary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ISmdD_O_L94x"
      },
      "outputs": [],
      "source": [
        "# Tokenize the text\n",
        "tokenizer = Tokenizer(num_words=vocabulary_size)\n",
        "tokenizer.fit_on_texts(train_synopses)\n",
        "train_synopses_seq = tokenizer.texts_to_sequences(train_synopses)\n",
        "max_len = max([len(seq) for seq in train_synopses_seq])\n",
        "train_synopses_pad = pad_sequences(train_synopses_seq, maxlen=max_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "RraEFkKDAyvZ"
      },
      "outputs": [],
      "source": [
        "model = Sequential([\n",
        "        Embedding(input_dim=vocabulary_size, output_dim=128),\n",
        "        Bidirectional(LSTM(128, return_sequences=True)),\n",
        "        GlobalMaxPool1D(),\n",
        "        Dense(train_labels.shape[1],activation='sigmoid')\n",
        "    ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "-mcTpSNNOJF2"
      },
      "outputs": [],
      "source": [
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "UQbdh6ogRM4n"
      },
      "outputs": [],
      "source": [
        "val_synopses_seq = tokenizer.texts_to_sequences(val_synopses)\n",
        "max_len = max([len(seq) for seq in val_synopses_seq])\n",
        "val_synopses_pad = pad_sequences(val_synopses_seq, maxlen=max_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2ml_WVbOh4K",
        "outputId": "d1a7dc62-2f88-4433-ccda-c60bf0fa0a3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/4\n",
            "259/259 [==============================] - 70s 257ms/step - loss: 0.4578 - accuracy: 0.2700 - val_loss: 0.4220 - val_accuracy: 0.3291\n",
            "Epoch 2/4\n",
            "259/259 [==============================] - 62s 240ms/step - loss: 0.4020 - accuracy: 0.3254 - val_loss: 0.3968 - val_accuracy: 0.3561\n",
            "Epoch 3/4\n",
            "259/259 [==============================] - 60s 231ms/step - loss: 0.3507 - accuracy: 0.3926 - val_loss: 0.4018 - val_accuracy: 0.3182\n",
            "Epoch 4/4\n",
            "259/259 [==============================] - 56s 218ms/step - loss: 0.2877 - accuracy: 0.4757 - val_loss: 0.4299 - val_accuracy: 0.2795\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x781e1cd22bc0>"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ],
      "source": [
        "# Train the model\n",
        "model.fit(train_synopses_pad, train_labels, batch_size=32, epochs=4, validation_data=(val_synopses_pad, val_labels) )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Validation Predict"
      ],
      "metadata": {
        "id": "Ymt4Oe3uOGdU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4UFeVWKLP5tJ",
        "outputId": "ddaebf31-9f9d-4120-f038-c04397f300fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "38/38 [==============================] - 3s 59ms/step\n"
          ]
        }
      ],
      "source": [
        "labels_prob_val = model.predict(val_synopses_pad)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_curve, f1_score\n",
        "\n",
        "n_labels = val_labels.shape[1]\n",
        "precision_dict = {}\n",
        "recall_dict = {}\n",
        "f1_scores = []\n",
        "optimal_thresholds = []\n",
        "for i in range(n_labels):\n",
        "    # Calculate precision and recall for each label\n",
        "    precision_dict[i], recall_dict[i], thresholds = precision_recall_curve(val_labels[:, i], labels_prob_val[:, i])\n",
        "\n",
        "\n",
        "    # Find optimal threshold for F1 score for each label\n",
        "    f1_score_values = 2 * (precision_dict[i] * recall_dict[i]) / (precision_dict[i] + recall_dict[i])\n",
        "    optimal_idx = np.nanargmax(f1_score_values)  # nanargmax handles NaN values\n",
        "    optimal_threshold = thresholds[optimal_idx] if optimal_idx < len(thresholds) else 1.0\n",
        "    optimal_thresholds.append(optimal_threshold)\n",
        "    f1_scores.append(f1_score_values[optimal_idx])\n",
        "\n",
        "    print(f\"Label {i}: Optimal Threshold: {optimal_threshold}, F1 Score: {f1_score_values[optimal_idx]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xwPZSiIG6Ygs",
        "outputId": "7d6a0fdc-6b5b-46cb-e361-20a4e2852aff"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label 0: Optimal Threshold: 0.18948140740394592, F1 Score: 0.3696369636963696\n",
            "Label 1: Optimal Threshold: 0.2561385929584503, F1 Score: 0.45569620253164556\n",
            "Label 2: Optimal Threshold: 0.1855451911687851, F1 Score: 0.4705882352941176\n",
            "Label 3: Optimal Threshold: 0.012305769138038158, F1 Score: 0.07865168539325842\n",
            "Label 4: Optimal Threshold: 0.06390566378831863, F1 Score: 0.7363957597173145\n",
            "Label 5: Optimal Threshold: 0.3031166195869446, F1 Score: 0.458259325044405\n",
            "Label 6: Optimal Threshold: 0.3389400541782379, F1 Score: 0.5949820788530467\n",
            "Label 7: Optimal Threshold: 0.0513652078807354, F1 Score: 0.12345679012345678\n",
            "Label 8: Optimal Threshold: 0.7265446782112122, F1 Score: 0.6453089244851259\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-105-a86b22ce96ce>:14: RuntimeWarning: invalid value encountered in divide\n",
            "  f1_score_values = 2 * (precision_dict[i] * recall_dict[i]) / (precision_dict[i] + recall_dict[i])\n",
            "<ipython-input-105-a86b22ce96ce>:14: RuntimeWarning: invalid value encountered in divide\n",
            "  f1_score_values = 2 * (precision_dict[i] * recall_dict[i]) / (precision_dict[i] + recall_dict[i])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred_labels_val = apply_thresholds(labels_prob_val, optimal_thresholds)"
      ],
      "metadata": {
        "id": "WnwNsAfD65Fe"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "id": "Ie8koRFPYDti"
      },
      "outputs": [],
      "source": [
        "create_prediction_csv(pred_labels_val, \"b\", \"validation\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kIETaUM8YPBY",
        "outputId": "90b312dd-0e21-4278-d0f7-3a0ce5740ece"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class level: \n",
            "Class  1 precision: 0.2599 recall: 0.6400 f1:  0.3696\n",
            "Class  2 precision: 0.3315 recall: 0.7287 f1:  0.4557\n",
            "Class  3 precision: 0.4069 recall: 0.5578 f1:  0.4706\n",
            "Class  4 precision: 0.0422 recall: 0.5833 f1:  0.0787\n",
            "Class  5 precision: 0.6247 recall: 0.8967 f1:  0.7364\n",
            "Class  6 precision: 0.3957 recall: 0.5443 f1:  0.4583\n",
            "Class  7 precision: 0.6194 recall: 0.5724 f1:  0.5950\n",
            "Class  8 precision: 0.0763 recall: 0.3226 f1:  0.1235\n",
            "Class  9 precision: 0.6211 recall: 0.6714 f1:  0.6453\n",
            "----------------------------\n",
            "Movie (document) level: \n",
            "Precision: 0.4402\n",
            "Recall: 0.6992\n",
            "F1: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/task2_eval_script_student_version.py:60: RuntimeWarning: invalid value encountered in divide\n",
            "  movie_level_f1 = (2*movie_level_precision*movie_level_recall)/(movie_level_precision*movie_level_recall)\n"
          ]
        }
      ],
      "source": [
        "%run task2_eval_script_student_version.py ./data/10726993-Task2-method-b-validation.csv ./data/Task-2-validation-dataset.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing predict\n"
      ],
      "metadata": {
        "id": "DrLGyxJ3ODgA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_synopses_seq = tokenizer.texts_to_sequences(test_synopses)\n",
        "max_len = max([len(seq) for seq in test_synopses_seq])\n",
        "test_synopses_pad = pad_sequences(test_synopses_seq, maxlen=max_len)"
      ],
      "metadata": {
        "id": "ZACeJuGpNVgS"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels_prob_test = model.predict(test_synopses_pad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9JXRgFUXNaTt",
        "outputId": "1958a1cd-60b9-4cfe-a063-de4dcff58b47"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "38/38 [==============================] - 5s 85ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred_labels_test = apply_thresholds(labels_prob_test, optimal_thresholds)"
      ],
      "metadata": {
        "id": "8gHzaVZ2NlVN"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "create_prediction_csv(pred_labels_test, \"b\", \"testing\")"
      ],
      "metadata": {
        "id": "sYxS0WFTNivK"
      },
      "execution_count": 118,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "-dPqJcKzAxA7",
        "b_WYV3VGD8px",
        "hEZC9nftBYOX",
        "aVG361JW8JcY"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}