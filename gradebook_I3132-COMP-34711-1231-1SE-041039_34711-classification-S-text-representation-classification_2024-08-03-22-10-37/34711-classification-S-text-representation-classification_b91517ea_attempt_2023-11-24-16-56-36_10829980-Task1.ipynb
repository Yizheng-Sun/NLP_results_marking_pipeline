{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Import Libraries"
      ],
      "metadata": {
        "id": "sBjC70cbU5BW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "JCAmjDgXfPwT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc583ee2-4e7d-444f-8a6e-3bef370985a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models.phrases import Phrases\n",
        "import string\n",
        "import numpy as np\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Create Class & Functions"
      ],
      "metadata": {
        "id": "j29QMweXekfw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DistributionalSemantics:\n",
        "\n",
        "  def __init__(self) -> None:\n",
        "    self.method = 0\n",
        "\n",
        "  def read_data(self, path: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Read csv files for use as the data\n",
        "    :param path: path where data is located\n",
        "    :return: list of all the data\n",
        "    \"\"\"\n",
        "    files = os.listdir(path)\n",
        "    corpus = []\n",
        "\n",
        "    for f in files:\n",
        "      file_name, file_extension = os.path.splitext(f)\n",
        "      if file_extension == '.csv':\n",
        "        df = pd.read_csv(f\"{path}/{f}\")\n",
        "      corpus.append(df)\n",
        "\n",
        "    return pd.concat(corpus)\n",
        "\n",
        "  def preprocess_data(self, document: list) -> list:\n",
        "    \"\"\"\n",
        "    Applies the following to the data: tokenization, stop words, stemming, punctuation & lowercase\n",
        "    :param document: list of data to be cleaned\n",
        "    :return: list of cleaned data\n",
        "    \"\"\"\n",
        "\n",
        "    # Tokenization\n",
        "    tokenized = nltk.tokenize.word_tokenize(document)\n",
        "\n",
        "    # Stop words\n",
        "    stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n",
        "    cleaned_words = [word for word in tokenized if word not in stop_words]\n",
        "\n",
        "    # Stemming\n",
        "    porter_stemmer = nltk.stem.PorterStemmer()\n",
        "    cleaned_words = [porter_stemmer.stem(word) for word in cleaned_words]\n",
        "\n",
        "    # Punctuation\n",
        "    punctuations = set(string.punctuation)\n",
        "    cleaned_words = [word for word in cleaned_words if word not in punctuations]\n",
        "\n",
        "    # Lowercase\n",
        "    cleaned_words = [term.lower() for term in cleaned_words]\n",
        "\n",
        "    return cleaned_words\n",
        "\n",
        "  def process_tfidf(self, document: list) -> list:\n",
        "    \"\"\"\n",
        "    Process data to produce a vectorizer and matrix\n",
        "    :param document: list of data\n",
        "    :return: a list of the vectorizer and matrix of the document\n",
        "    \"\"\"\n",
        "\n",
        "    # Setup tfidf\n",
        "    tfidf = TfidfVectorizer(tokenizer=self.preprocess_data, ngram_range=(1,2), stop_words='english')\n",
        "    tfidf_matrix = tfidf.fit_transform(document['plot_synopsis'])\n",
        "    print(f'Vocab Size: {len(tfidf.vocabulary_)}')\n",
        "\n",
        "    return [tfidf, tfidf_matrix]\n",
        "\n",
        "  def process_word2vec(self, document: list) -> list:\n",
        "    \"\"\"\n",
        "    Process data to produce a vectorizer and matrix\n",
        "    :param document: list of data\n",
        "    :return: a list of the vectorizer and matrix of the document\n",
        "    \"\"\"\n",
        "\n",
        "    # Setup word2vec\n",
        "    tokenized = [self.preprocess_data(doc) for doc in document['plot_synopsis']]\n",
        "    bigrams = Phrases(tokenized)\n",
        "    model = Word2Vec(sentences=bigrams[tokenized], vector_size=200, window=5, min_count=1, workers=4)\n",
        "\n",
        "    # Save and load model if needed\n",
        "    # model.save(\"word2vec.model\")\n",
        "    # model = Word2Vec.load('word2vec.model')\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "  def calc_similarity(self, vectorizer, matrix, term1: str, term2: str) -> float:\n",
        "    \"\"\"\n",
        "    Process data to produce a matrix\n",
        "    :param vectorizer: the vectorizer object being used (eg. tfidf)\n",
        "    :param matrix: the matrix transformed from the vectorizer\n",
        "    :param term1, term2: terms we are checking the similarity of\n",
        "    :return: cosine similarity value of both terms\n",
        "    \"\"\"\n",
        "\n",
        "    # Get index of terms\n",
        "    term1_index = vectorizer.vocabulary_.get(term1, -1)\n",
        "    term2_index = vectorizer.vocabulary_.get(term2, -1)\n",
        "\n",
        "    # If both are in vocab, then cosine the two terms\n",
        "    if term1_index != -1 and term2_index != -1:\n",
        "      term1_vector = matrix[:, term1_index].transpose()\n",
        "      term2_vector = matrix[:, term2_index].transpose()\n",
        "\n",
        "      cos_sim = cosine_similarity(term1_vector, term2_vector)[0][0]\n",
        "      return cos_sim\n",
        "\n",
        "    # If not in vocab, no similarity\n",
        "    return 0\n",
        "\n",
        "  def results_to_csv(self, processed_data: list, test_data: list) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Output results into a csv, with term_pair_id and similarity\n",
        "    :param processed_data: a list of the vectorizer and matrix for tfidf\n",
        "    :param test_data: document that has the terms to compare\n",
        "    \"\"\"\n",
        "\n",
        "    stemmer = nltk.stem.PorterStemmer()\n",
        "    indices = []\n",
        "    similarities = []\n",
        "\n",
        "    # Loop through evaluation data\n",
        "    if self.method == 0:\n",
        "      for index, row in test_data.iterrows():\n",
        "        term1 = ' '.join([stemmer.stem(word) for word in row[1].split()])\n",
        "        term2 = ' '.join([stemmer.stem(word) for word in row[2].split()])\n",
        "        cos_sim = self.calc_similarity(processed_data[0], processed_data[1], term1, term2)\n",
        "        indices.append(row[0])\n",
        "        similarities.append(cos_sim)\n",
        "\n",
        "    elif self.method == 1:\n",
        "      for index, row in test_data.iterrows():\n",
        "        term1 = ' '.join([stemmer.stem(word) for word in row[1].split()])\n",
        "        term2 = ' '.join([stemmer.stem(word) for word in row[2].split()])\n",
        "        if term1 in processed_data.wv and term2 in processed_data.wv:\n",
        "          cos_sim = processed_data.wv.similarity(term1, term2)\n",
        "        else:\n",
        "          cos_sim = 0\n",
        "        indices.append(row[0])\n",
        "        similarities.append(cos_sim)\n",
        "\n",
        "    # Write values to csv (id, sim_val)\n",
        "    results = pd.DataFrame({\n",
        "        'term_pair_id': indices,\n",
        "        'similarity': similarities\n",
        "    })\n",
        "\n",
        "    results.to_csv(f'results_{self.method}.csv', header=False, index=False)\n",
        "\n",
        "    return results\n",
        "\n",
        "  def train_and_test(self, path: str, train_data_name: str, test_data_name: str, to_print=True) -> None:\n",
        "    \"\"\"\n",
        "    Trains and tests data accordingly depending on the method set\n",
        "    :param path: path to directory with data\n",
        "    :param train_data_name: name of the csv for training\n",
        "    :param test_data_name: name of the csv for testing\n",
        "    :param to_print: choose whether to print results\n",
        "    :return: none\n",
        "    \"\"\"\n",
        "\n",
        "    # Read in data\n",
        "    train_data = pd.read_csv(f'{path}/{train_data_name}')\n",
        "    test_data = pd.read_csv(f'{path}/{test_data_name}', header=None)\n",
        "\n",
        "    # 0: tfidf\n",
        "    if self.method == 0:\n",
        "      train_data = self.process_tfidf(train_data)\n",
        "      results = self.results_to_csv(train_data, test_data)\n",
        "\n",
        "    # 1: Word2Vec\n",
        "    elif self.method == 1:\n",
        "      train_data = self.process_word2vec(train_data)\n",
        "      results = self.results_to_csv(train_data, test_data)\n",
        "\n",
        "    # Print results\n",
        "    if to_print:\n",
        "      print(results)"
      ],
      "metadata": {
        "id": "tLsv1z_xWLKn"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. TF*IDF"
      ],
      "metadata": {
        "id": "_HTleiif-Hdi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main() -> None:\n",
        "  dist = DistributionalSemantics()\n",
        "\n",
        "  dist.method = 0\n",
        "\n",
        "  files_path = './data/'\n",
        "  train_data_name = 'Training-dataset.csv'\n",
        "  test_data_name = 'Task-1-test-dataset1.csv'\n",
        "\n",
        "  dist.train_and_test(files_path, train_data_name, test_data_name)\n",
        "\n",
        "test = main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "juFAG8KX9_gr",
        "outputId": "00da761b-3651-40c6-b484-29bb816a2dd8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['afterward', 'alon', 'alreadi', 'alway', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becom', 'besid', 'cri', 'describ', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'henc', 'hereaft', 'herebi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'otherwis', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'thenc', 'thereaft', 'therebi', 'therefor', 'thu', 'togeth', 'twelv', 'twenti', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev'] not in stop_words.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab Size: 2280231\n",
            "     term_pair_id  similarity\n",
            "0             816    0.090296\n",
            "1             957    0.094483\n",
            "2             809    0.155944\n",
            "3             911    0.100518\n",
            "4             242    0.023100\n",
            "..            ...         ...\n",
            "97            160    0.000000\n",
            "98             14    0.099998\n",
            "99             16    0.085868\n",
            "100          4012    0.001862\n",
            "101          4013    0.003492\n",
            "\n",
            "[102 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Word2Vec"
      ],
      "metadata": {
        "id": "UTJVisUZ-OHw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main() -> None:\n",
        "  dist = DistributionalSemantics()\n",
        "\n",
        "  dist.method = 1\n",
        "\n",
        "  files_path = './data/'\n",
        "  train_data_name = 'Training-dataset.csv'\n",
        "  test_data_name = 'Task-1-test-dataset1.csv'\n",
        "\n",
        "  dist.train_and_test(files_path, train_data_name, test_data_name)\n",
        "\n",
        "test = main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_UNauuCS-B4w",
        "outputId": "133fd614-b2c5-4ce2-d101-27f795cbcf53"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     term_pair_id  similarity\n",
            "0             816    0.707323\n",
            "1             957    0.633769\n",
            "2             809    0.680188\n",
            "3             911    0.552187\n",
            "4             242    0.599650\n",
            "..            ...         ...\n",
            "97            160    0.287538\n",
            "98             14    0.682318\n",
            "99             16    0.475540\n",
            "100          4012    0.000000\n",
            "101          4013    0.000000\n",
            "\n",
            "[102 rows x 2 columns]\n"
          ]
        }
      ]
    }
  ]
}