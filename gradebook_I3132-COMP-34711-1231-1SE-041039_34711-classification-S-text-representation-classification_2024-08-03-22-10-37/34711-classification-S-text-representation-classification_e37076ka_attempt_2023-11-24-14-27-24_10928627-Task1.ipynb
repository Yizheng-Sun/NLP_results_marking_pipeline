{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1: Distributional semantics"
      ],
      "metadata": {
        "id": "v2qfdg_BClEG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing data for both Representations"
      ],
      "metadata": {
        "id": "gVxe4YYg_ZTN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "import math\n",
        "import collections\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "# Download NLTK resources required for preproccessing\n",
        "nltk.download('punkt') # Tokenizer\n",
        "nltk.download('stopwords') # Stopwords list\n",
        "nltk.download('wordnet') # Lemmatizer\n",
        "\n",
        "# Initialize lemmatizer and stop words set\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Function to preprocess text\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()  # Lowercase\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation using string translation\n",
        "    tokens = word_tokenize(text)  # Tokenize the text\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]  # Filter out stopwords and lemmatize the tokens\n",
        "    return lemmatized_tokens\n",
        "\n",
        "\n",
        "# Load dataset\n",
        "train_data = pd.read_csv('./data/Training-dataset.csv')\n",
        "\n",
        "# Comment/uncomment for validating code with development/test dataset #\n",
        "\n",
        "# validation data:\n",
        "# validation_data = pd.read_csv('./data/Task-1-validation-dataset.csv', header=None)\n",
        "# validation_data.columns = ['term_pair_id', 'term1', 'term2', 'gold_standard_similarity'] # Defining manually the column names for the validation dataset\n",
        "\n",
        "#test data:\n",
        "validation_data = pd.read_csv('./data/Task-1-test-dataset1.csv', header=None)\n",
        "validation_data.columns = ['term_pair_id', 'term1', 'term2']\n",
        "\n",
        "# Preprocess text data in the 'plot_synopsis'\n",
        "train_data['processed_plot_synopsis'] = train_data['plot_synopsis'].apply(preprocess_text)\n",
        "\n",
        "\n",
        "# At this point the training data is ready for creating representations\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q-XXg43ZfRMd",
        "outputId": "029baa2a-118a-49b2-f98b-0af6ef156f21"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.3)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.4.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1st representation(option A): A sparse representation with PPMI\n"
      ],
      "metadata": {
        "id": "DBAtXzgw_h4l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A function to build a co-occurrence matrix\n",
        "def build_matrix(token_lists, window=1):\n",
        "    cooccurrence_dict = defaultdict(Counter)\n",
        "    for token_list in token_lists:\n",
        "        token_count = len(token_list)\n",
        "\n",
        "        for index, token in enumerate(token_list):\n",
        "            # Determine the context window and update counts\n",
        "            context_indices = list(range(max(0, index - window), min(token_count, index + window + 1)))\n",
        "            context_indices.remove(index)\n",
        "            context_tokens = [token_list[i] for i in context_indices]\n",
        "            cooccurrence_dict[token].update(context_tokens)\n",
        "\n",
        "    return cooccurrence_dict\n",
        "\n",
        "# A function to calculate the PPMI matrix\n",
        "def ppmi_matrix_calc(cooccurrence_dict):\n",
        "    all_cooccurrences = sum(sum(counter.values()) for counter in cooccurrence_dict.values())\n",
        "    freq_word = {word: sum(counter.values()) for word, counter in cooccurrence_dict.items()}\n",
        "    # Create PPMI matrix\n",
        "    ppmi_mat = np.zeros((len(cooccurrence_dict), len(cooccurrence_dict)))\n",
        "    word_to_idx = {word: index for index, word in enumerate(cooccurrence_dict.keys())}\n",
        "    # Calculate PPMI values\n",
        "    for word, context_counts  in cooccurrence_dict.items():\n",
        "        for context_word, cooccurrence_count  in context_counts.items():\n",
        "            pointwise_info = math.log2((cooccurrence_count  * all_cooccurrences) / (freq_word[word] * freq_word[context_word])) # Pointwise mutual information\n",
        "            ppmi_mat[word_to_idx[word], word_to_idx[context_word]] = max(pointwise_info, 0) # Truncate negative values to zero for PPMI\n",
        "\n",
        "    return ppmi_mat, word_to_idx\n",
        "\n",
        "# A function to generate a PPMI vector for a term (could be single or multi-word)\n",
        "def get_ppmi_vector(term, ppmi_mat, word_to_idx):\n",
        "  # Retrieve vectors for each word in the term\n",
        "  vectors = [ppmi_mat[word_to_idx[word]] for word in term.split() if word in word_to_idx]\n",
        "  # Calculate the mean vector for the term\n",
        "  return np.mean(vectors, axis=0) if vectors else np.zeros((ppmi_mat.shape[1],))\n",
        "\n",
        "# a function to calculate cosine similarity between two terms using PPMI to represent context\n",
        "def cosine_similarity_ppmi_multi(term1, term2, ppmi_mat, word_to_idx):\n",
        "  # Get PPMI vectors for both terms\n",
        "  vector1 = get_ppmi_vector(term1, ppmi_mat, word_to_idx)\n",
        "  vector2 = get_ppmi_vector(term2, ppmi_mat, word_to_idx)\n",
        "  return cosine_similarity([vector1], [vector2])[0][0]\n",
        "\n",
        "\n",
        "# Building PPMI matrix from processed training data\n",
        "cooccurrence_dict = build_matrix(train_data['processed_plot_synopsis'].tolist())\n",
        "ppmi_mat, word_to_idx = ppmi_matrix_calc(cooccurrence_dict)\n",
        "\n",
        "\n",
        "# Calculating cosine similarity for validation dataset\n",
        "cosine_similarity_ppmi = [\n",
        "    cosine_similarity_ppmi_multi(row['term1'], row['term2'], ppmi_mat, word_to_idx)\n",
        "    for _, row in validation_data.iterrows()\n",
        "]\n",
        "\n",
        "# Put cosine similarity results to a file with specified format\n",
        "cosine_similarity_ppmi_result = pd.DataFrame({\n",
        "    'term_pair_id': validation_data['term_pair_id'],\n",
        "    'similarity': cosine_similarity_ppmi\n",
        "})\n",
        "cosine_similarity_ppmi_result.to_csv('./data/10928627-Task1-method-a.csv', index=False, header=False)"
      ],
      "metadata": {
        "id": "G5h4JyvDoqLX"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"vocabulary size:\")\n",
        "print(len(word_to_idx))"
      ],
      "metadata": {
        "id": "ZV8bkvBCkNTL",
        "outputId": "69ca277e-c360-443d-8655-4aac1ca24f91",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocabulary size:\n",
            "127527\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2nd representation(option B): A dense static representation with Word2vec"
      ],
      "metadata": {
        "id": "NfzHYid-_q8N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize data to make it ready for training Word2vec model\n",
        "tokenize = train_data['processed_plot_synopsis'].tolist()\n",
        "\n",
        "# Train model, word2vec CBoW\n",
        "model_trained = Word2Vec(sentences=tokenize,\n",
        "                         vector_size=100, # Determines dimensionality of word vectors\n",
        "                         window=5,        # Maximum distance between current and predicted word within sentence\n",
        "                         min_count=1,     # Ignores all words with total frequency lower than this\n",
        "                         workers=4)       # Number of worker threads used to train the model (faster training with multicore machines)\n",
        "\n",
        "# Vector representation for multi-word terms using Word2Vec\n",
        "def vector_for_multiword_word2vec(term, representation):\n",
        "    vectors= [representation.wv[word] for word in term.split() if word in representation.wv.key_to_index]\n",
        "    return np.mean(vectors, axis=0) if vectors else np.zeros((representation.vector_size,))\n",
        "\n",
        "# A function calculating cosine similarity between two terms using Word2Vec representations\n",
        "def cosine_similarity_word2vec_multi(term1, term2, representation):\n",
        "    vector1 = vector_for_multiword_word2vec(term1, representation)\n",
        "    vector2 = vector_for_multiword_word2vec(term2, representation)\n",
        "    return cosine_similarity([vector1], [vector2])[0][0]\n",
        "\n",
        "\n",
        "cosine_similarity_word2vec = [\n",
        "    cosine_similarity_word2vec_multi(row['term1'], row['term2'], model_trained)\n",
        "    for _, row in validation_data.iterrows()\n",
        "]\n",
        "\n",
        "cosine_similarity_word2vec_result = pd.DataFrame({\n",
        "    'term_pair_id': validation_data['term_pair_id'],\n",
        "    'similarity': cosine_similarity_word2vec\n",
        "})\n",
        "cosine_similarity_word2vec_result.to_csv('./data/10928627-Task1-method-b.csv', index=False, header=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "G8M_mIsOo3dj"
      },
      "execution_count": 4,
      "outputs": []
    }
  ]
}