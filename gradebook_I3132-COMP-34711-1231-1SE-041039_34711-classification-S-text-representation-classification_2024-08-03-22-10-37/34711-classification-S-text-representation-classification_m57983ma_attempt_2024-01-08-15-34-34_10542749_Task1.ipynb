{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oDikN-tJVZFr",
        "outputId": "581e1056-ca32-4832-cacf-5646b4980b14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('averaged_perceptron_tagger')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "MTACu7b9Znnr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7332c7e-1215-492c-92bb-72c632698910"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   0        1           2     3\n",
            "0  1   absorb       learn  5.48\n",
            "1  2   absorb    withdraw  2.97\n",
            "2  3  achieve  accomplish  8.57\n",
            "3  4  achieve         try  4.42\n",
            "4  6  acquire         get  8.82\n"
          ]
        }
      ],
      "source": [
        "#Load the training dataset\n",
        "training_dataset_path = '/content/drive/MyDrive/Colab Notebooks/Training-dataset.csv'\n",
        "training_df = pd.read_csv(training_dataset_path)\n",
        "\n",
        "#Load the validation dataset\n",
        "validation_dataset_path = './data/Task-1-validation-dataset.csv'\n",
        "validation_df = pd.read_csv(validation_dataset_path, header=None)\n",
        "\n",
        "\n",
        "test_dataset_path = \"./data/Task-1-test-dataset2.csv\"\n",
        "test_df = pd.read_csv(test_dataset_path)\n",
        "\n",
        "# print(training_df.head())\n",
        "print(validation_df.head())\n",
        "\n",
        "\n",
        "#load evaluation script\n",
        "#%load '/content/drive/MyDrive/Colab Notebooks/task1_eval_script_student_version.py'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing the data"
      ],
      "metadata": {
        "id": "rL3Qi6F7a80Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Tokenize\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Case folding\n",
        "    tokens = [token.lower() for token in tokens]\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    return tokens  # Make sure to return the tokens\n",
        "\n",
        "# Apply the preprocessing function to your dataframe\n",
        "training_df[\"processed_text\"] = training_df['plot_synopsis'].apply(preprocess_text)\n",
        "\n",
        "\n",
        "# Flatten all token lists in the 'processed_text' column to create a single list of words\n",
        "all_words = [word for tokens in training_df['processed_text'] for word in tokens]\n",
        "\n",
        "# Create a set of all unique words\n",
        "unique_words = set(all_words)\n",
        "\n",
        "# The vocabulary size is the number of unique words\n",
        "vocabulary_size = len(unique_words)\n",
        "\n",
        "print(f\"The vocabulary size is: {vocabulary_size}\")"
      ],
      "metadata": {
        "id": "ogZmXDnnbEvX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b109d88d-ff08-44c3-ff20-65c6c111a2ae"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The vocabulary size is: 129767\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#tf*idf Implementation"
      ],
      "metadata": {
        "id": "fo0fCebmlLVd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "#Initialize the tf*idf vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "#Then combine the preprocced texts\n",
        "combined_text = [\" \".join(doc) for doc in training_df['processed_text']]\n",
        "\n",
        "#transform combined text to tf*idf reprsentation\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(combined_text)\n"
      ],
      "metadata": {
        "id": "S7GKSsBXlUP8"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#tfidf test\n"
      ],
      "metadata": {
        "id": "zub-dvVhbBQH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# Assuming tfidf_vectorizer and tfidf_matrix are already defined\n",
        "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "similarities = []\n",
        "\n",
        "# Iterate over the term pairs\n",
        "for _, row in test_df.iterrows():\n",
        "\n",
        "    term_pair_id = row[0]  # First column is the term_pair_id\n",
        "    term1 = row[1]  # Second column contains the first term\n",
        "    term2 = row[2]  # Third column contains the second term\n",
        "\n",
        "    # Check if the terms are in the feature_names\n",
        "    if term1 in feature_names and term2 in feature_names:\n",
        "        # Find the index of the terms\n",
        "        term1_index = np.where(feature_names == term1)[0][0]\n",
        "        term2_index = np.where(feature_names == term2)[0][0]\n",
        "\n",
        "        # Extract the TF-IDF vectors for each term\n",
        "        term1_vector = tfidf_matrix[:, term1_index].reshape(1, -1)\n",
        "        term2_vector = tfidf_matrix[:, term2_index].reshape(1, -1)\n",
        "\n",
        "        # Calculate cosine similarity\n",
        "        similarity_score = cosine_similarity(term1_vector, term2_vector)[0][0]\n",
        "    else:\n",
        "        # Assign a default similarity score for out-of-vocabulary terms\n",
        "        similarity_score = 0.55\n",
        "\n",
        "    # Append the term_pair_id and similarity score to the list\n",
        "    similarities.append((term_pair_id, similarity_score))\n",
        "\n",
        "# Create a DataFrame with the similarities list\n",
        "formatted_similarities_df = pd.DataFrame(similarities)\n",
        "\n",
        "# Save to CSV without the header and index\n",
        "csv_file_path = \"./data/tfidf_results.csv\"\n",
        "formatted_similarities_df.to_csv(csv_file_path, header=False, index=False, float_format='%.6f')\n"
      ],
      "metadata": {
        "id": "JL4CZA0SOnUr"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Validation Dataset"
      ],
      "metadata": {
        "id": "lDnNoiMMbKI3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# Assuming tfidf_vectorizer and tfidf_matrix are already defined\n",
        "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "similarities = []\n",
        "\n",
        "# Iterate over the term pairs\n",
        "for _, row in validation_df.iterrows():\n",
        "    term_pair_id = row[0]  # First column is the term_pair_id\n",
        "    term1 = row[1]  # Second column contains the first term\n",
        "    term2 = row[2]  # Third column contains the second term\n",
        "\n",
        "\n",
        "\n",
        "    # Check if the terms are in the feature_names\n",
        "    if term1 in feature_names and term2 in feature_names:\n",
        "        # Find the index of the terms\n",
        "        term1_index = np.where(feature_names == term1)[0][0]\n",
        "        term2_index = np.where(feature_names == term2)[0][0]\n",
        "\n",
        "        # Extract the TF-IDF vectors for each term\n",
        "        term1_vector = tfidf_matrix[:, term1_index].reshape(1, -1)\n",
        "        term2_vector = tfidf_matrix[:, term2_index].reshape(1, -1)\n",
        "\n",
        "        # Calculate cosine similarity\n",
        "        similarity_score = cosine_similarity(term1_vector, term2_vector)[0][0]\n",
        "    else:\n",
        "        # Assign a default similarity score for out-of-vocabulary terms\n",
        "        similarity_score = 0.55\n",
        "\n",
        "    # Append the term_pair_id and similarity score to the list\n",
        "    similarities.append((term_pair_id, similarity_score))\n",
        "\n",
        "# Create a DataFrame with the similarities list\n",
        "formatted_similarities_df = pd.DataFrame(similarities)\n",
        "\n",
        "# Save to CSV without the header and index\n",
        "csv_file_path = \"./data/tfidf_validation_results.csv\"\n",
        "formatted_similarities_df.to_csv(csv_file_path, header=False, index=False, float_format='%.6f')\n"
      ],
      "metadata": {
        "id": "Af6Bcl6GbOwB"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Running the evaluation script in a Colab cell\n",
        "!python './data/task1_eval_script_student_version.py' \"./data/tfidf_validation_results.csv\" \"./data/Task-1-validation-dataset.csv\"\n"
      ],
      "metadata": {
        "id": "hvsvaTB5WxNQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pre-process data again.\n"
      ],
      "metadata": {
        "id": "ihi1Ru7UV816"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Tokenize\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Case folding\n",
        "    tokens = [token.lower() for token in tokens]\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    # Initialize lemmatizer\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    # Lemmatization\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(w) for w in tokens]\n",
        "\n",
        "    return lemmatized_tokens\n",
        "\n",
        "# Apply the preprocessing function to your dataframe\n",
        "training_df[\"processed_text\"] = training_df['plot_synopsis'].apply(preprocess_text)\n",
        "\n",
        "# Continue with calculating the vocabulary size as before\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tKwds2K-WBZK",
        "outputId": "0b36a8fa-7e05-4e53-b8b1-53601bbd4719"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word2Vec Test\n",
        "\n"
      ],
      "metadata": {
        "id": "Re_Yc-GGDPtB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from gensim.models import Word2Vec\n",
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "\n",
        "\n",
        "# Ensure sentences is a list of lists (each inner list is a tokenized and preprocessed synopsis)\n",
        "sentences = training_df['processed_text'].tolist()\n",
        "\n",
        "# Train a Word2Vec model\n",
        "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "similarities = []\n",
        "\n",
        "# Iterate over the term pairs\n",
        "for _, row in test_df.iterrows():\n",
        "    term_pair_id = row[0]  # Use the term pair ID from the first column of test_df\n",
        "    term1 = row[1]  # Second column contains the first term\n",
        "    term2 = row[2]  # Third column contains the second term\n",
        "\n",
        "    # Check if the terms are in the model\n",
        "    if term1 in model.wv.key_to_index and term2 in model.wv.key_to_index:\n",
        "        # Calculate cosine similarity\n",
        "        similarity_score = 1 - cosine(model.wv[term1], model.wv[term2])\n",
        "    else:\n",
        "        # Assign a default similarity score for out-of-vocabulary terms\n",
        "        similarity_score = 0.55  # Adjust as needed\n",
        "\n",
        "    # Store the term pair ID and similarity score\n",
        "    similarities.append((term_pair_id, float(similarity_score)))\n",
        "\n",
        "# Create and save the formatted DataFrame with only term_pair_id and similarity score\n",
        "formatted_similarities_df = pd.DataFrame(similarities, columns=None)\n",
        "\n",
        "# Save to CSV with float_format to ensure numerical values are formatted correctly\n",
        "csv_file_path = \"./data/Word2Vec_results.csv\"\n",
        "formatted_similarities_df.to_csv(csv_file_path, header=False, index=False, float_format='%.6f')\n"
      ],
      "metadata": {
        "id": "jeCkbDQRvObU"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Validation Word2Vec"
      ],
      "metadata": {
        "id": "rPGy33OacYXe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from gensim.models import Word2Vec\n",
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "# Apply the preprocessing function to your dataframe\n",
        "training_df[\"processed_text\"] = training_df['plot_synopsis'].apply(preprocess_text)\n",
        "\n",
        "# Ensure sentences is a list of lists (each inner list is a tokenized and preprocessed synopsis)\n",
        "sentences = training_df['processed_text'].tolist()\n",
        "\n",
        "\n",
        "# Train a Word2Vec model\n",
        "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "\n",
        "# Reset the index if necessary\n",
        "#validation_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Process for cosine similarity calculation\n",
        "similarities = []\n",
        "\n",
        "# Iterate over the term pairs\n",
        "for _, row in validation_df.iterrows():\n",
        "    term_pair_id = row[0]  # First column is the term_pair_id\n",
        "    term1 = row[1]  # Second column contains the first term\n",
        "    term2 = row[2]  # Third column contains the second term\n",
        "\n",
        "    # Check if the terms are in the model\n",
        "    if term1 in model.wv.key_to_index and term2 in model.wv.key_to_index:\n",
        "        # Calculate cosine similarity\n",
        "        similarity_score = 1 - cosine(model.wv[term1], model.wv[term2])\n",
        "    else:\n",
        "        # Assign a default similarity score for out-of-vocabulary terms\n",
        "        similarity_score = 0.55\n",
        "\n",
        "    # Store the term pair ID and similarity score\n",
        "    similarities.append((term_pair_id, float(similarity_score)))\n",
        "\n",
        "# Create and save the formatted DataFrame with only term_pair_id and similarity score\n",
        "formatted_similarities_df = pd.DataFrame(similarities, columns=None)\n",
        "\n",
        "# Save to CSV with float_format to ensure numerical values are formatted correctly\n",
        "csv_file_path = \"./data/Word2Vec_validation_results.csv\"\n",
        "formatted_similarities_df.to_csv(csv_file_path, header=False, index=False, float_format='%.6f')\n"
      ],
      "metadata": {
        "id": "0hiR7_pwcdkk"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Running the evaluation script in a Colab cell\n",
        "!python './data/task1_eval_script_student_version.py' \"./data/Word2Vec_validation_results.csv\" \"./data/Task-1-validation-dataset.csv\"\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kj4hcrS1cYag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LvC4yavyS17L"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}